
<!DOCTYPE HTML>

<html lang="en">

<head>
  <meta charset="utf-8">
  <title>Setup Cluster Resources (Swarm) - Docker 1.10 - W3cubDocs</title>
  
  <meta name="description" content="Now that your underlying network infrastructure is built, you can deploye and configure the Swarm cluster. A host in a Swarm cluster is called a &hellip;">
  <meta name="keywords" content="setup, cluster, resources, swarm, -, docker, docker~1.10">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="http://docs.w3cub.com/docker~1.10/swarm/swarm_at_scale/03-create-cluster/">
  <link href="/favicon.png" rel="icon">
  <link type="text/css" rel="stylesheet" href="/assets/application-50364fff564ce3b6327021805f3f00e2957b441cf27f576a7dd4ff63bbc47047.css">
  <script type="text/javascript" src="/assets/application-db64bfd54ceb42be11af7995804cf4902548419ceb79d509b0b7d62c22d98e6f.js"></script>
  <script src="/json/docker~1.10.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body>
	<div class="_app">
	<header class="_header">
  
  <form class="_search">
    <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
    <a class="_search-clear"></a>
    <div class="_search-tag"></div>
  </form>
  
  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/docker~1.10/" class="_nav-link" title="" style="margin-left:0;">Docker 1.10</a></span>
  
  <nav class="_nav">
    <a href="/app/" class="_nav-link ">App</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		<div class="_list">
			
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<div class="_page _docker">
				
<h1 id="setup-cluster-resources">Setup cluster resources</h1> <p>Now that <a href="../02-deploy-infra/">your underlying network infrastructure is built</a>, you can deploye and configure the Swarm cluster. A host in a Swarm cluster is called a <em>node</em>. So, these instructions refer to each AWS EC2 instances as a node and refers to each node by the <strong>Name</strong> it appears as in your EC2 Dashboard**.</p> <p><img src="https://docs.docker.com/v1.10/swarm/images/vpc-create-7.png" alt=""></p> <p>The steps on this page construct a Swarm cluster by:</p> <ul> <li>using Consul as the discovery backend</li> <li>join the <code>frontend</code>, <code>worker</code>
</li> <li>
<code>store</code> EC2 instances to the cluster</li> <li>use the <code>spread</code> scheduling strategy</li> </ul> <p>You’ll perform all the configuration steps from the Swarm manager node. The manager node has access to all the instances in the cluster.</p> <h2 id="step-1-construct-the-cluster">Step 1: Construct the cluster</h2> <p>In this step, you create a Consul container for use as the Swarm discovery service. The Consul backend is also used as the K/V store for the container network that you overlay on the Swarm cluster in a later step. After you launch the Consul container, you launch a Swarm manager container.</p> <ol> <li>
<p>Select the <code>manager</code> node and click <strong>Connect</strong> to display the <code>ssh</code> command you’ll need.</p> <p><img src="https://docs.docker.com/v1.10/swarm/images/ssh-dialog.gif" alt=""></p>
</li> <li><p>Open a terminal on your <code>manager</code> node with the <code>ssh</code> command.</p></li> <li>
<p>Start a Consul container that listens on TCP port <code>8500</code>.</p> <pre>$ docker run --restart=unless-stopped -d -p 8500:8500 -h consul progrium/consul -server -bootstrap
Unable to find image 'progrium/consul:latest' locally
latest: Pulling from progrium/consul
3b4d28ce80e4: Pull complete
...&lt;output snip&gt;...
d9125e9e799b: Pull complete
Digest: sha256:8cc8023462905929df9a79ff67ee435a36848ce7a10f18d6d0faba9306b97274
Status: Downloaded newer image for progrium/consul:latest
6136de6829916430ee911cc14e3f067624fcf6d3041d404930f6dbcbf5399f7d
</pre>
</li> <li>
<p>Confirm the container is running.</p> <pre>$ docker ps
IMAGE               COMMAND                  CREATED              STATUS              PORTS                                                                            NAMES
6136de682991        progrium/consul     "/bin/start -server -"   About a minute ago   Up About a minute   53/tcp, 53/udp, 8300-8302/tcp, 8400/tcp, 0.0.0.0:8500-&gt;8500/tcp, 8301-8302/udp   goofy_jepsen
</pre>
</li> <li>
<p>Start a Swarm manager container.</p> <p>This command maps port <code>3375</code> on the <code>manager</code> node to port <code>2375</code> in the Swarm manager container.</p> <pre>$ docker run --restart=unless-stopped -d -p 3375:2375 swarm manage consul://192.168.33.11:8500/
Unable to find image 'swarm:latest' locally
latest: Pulling from library/swarm
887115b43fc0: Pull complete
...&lt;output snip&gt;...
f3f134eb6413: Pull complete
Digest: sha256:51a8eba9502f1f89eef83e10b9f457cfc67193efc3edf88b45b1e910dc48c906
Status: Downloaded newer image for swarm:latest
f5f093aa9679410bee74b7f3833fb01a3610b184a530da7585c990274bd65e7e
</pre> <p>The Swarm manager container is the heart of your Swarm cluster. It is responsible for receiving all Docker commands sent to the cluster, and for scheduling resources against the cluster. In a real-world production deployment you would configure additional replica Swarm managers as secondaries for high availability (HA).</p>
</li> <li>
<p>Get information about your Docker installation.</p> <pre>  $ docker info
  Containers: 2
  Images: 26
  Server Version: 1.9.1
  Storage Driver: aufs
   Root Dir: /var/lib/docker/aufs
   Backing Filesystem: extfs
   Dirs: 30
   Dirperm1 Supported: true
  Execution Driver: native-0.2
  Logging Driver: json-file
  Kernel Version: 3.16.0-57-generic
  Operating System: Ubuntu 14.04.3 LTS
  CPUs: 1
  Total Memory: 992.2 MiB
  Name: manager
  ID: IISM:V4KJ:VXCT:ONN3:MFIJ:2ZLD:VI6I:UYB3:FJZ4:3O7J:FHKA:P3XS
  WARNING: No swap limit support
  Cluster store: consul://192.168.33.11:8500
  Cluster advertise: 192.168.33.11:2375
</pre> <p>The command information returns the information about the Engine and its daemon.</p>
</li> <li>
<p>Confirm that you have the <code>consul</code> and <code>swarm manage</code> containers running.</p> <pre>  $ docker ps
  CONTAINER ID        IMAGE               COMMAND                  CREATED              STATUS              PORTS                                                                            NAMES
  f5f093aa9679        swarm               "/swarm manage consul"   About a minute ago   Up About a minute   0.0.0.0:3375-&gt;2375/tcp                                                           sad_goldstine
  6136de682991        progrium/consul     "/bin/start -server -"   9 minutes ago        Up 9 minutes        53/tcp, 53/udp, 8300-8302/tcp, 8301-8302/udp, 8400/tcp, 0.0.0.0:8500-&gt;8500/tcp   goofy_jepsen
</pre>
</li> <li>
<p>Set the <code>DOCKER_HOST</code> environment variable.</p> <p>This ensures that the default endpoint for Docker Engine CLI commands is the Engine daemon running on the <code>manager</code> node.</p> <pre>$ export DOCKER_HOST="tcp://192.168.33.11:3375"
</pre>
</li> <li>
<p>Now that your terminal environment is set to the Swarm port, rerun the <code>docker info</code> command.</p> <pre>$ docker info
Containers: 0
Images: 0
Role: primary
Strategy: spread
Filters: health, port, dependency, affinity, constraint
Nodes: 0
Kernel Version: 3.16.0-57-generic
Operating System: linux
CPUs: 0
Total Memory: 0 B
Name: f5f093aa9679
</pre> <p>The command is acting on the Swarm port, so it returns information about the entire cluster. You have a manager and no nodes.</p>
</li> <li>
<p>While still on the <code>master</code> node, join each node one-by-one to the cluster.</p> <p>You can run these commands to join each node from the <code>manager</code> node command line. The <code>-H</code> flag with the <code>docker</code> command specifies a node IP address and the Engine port. Each command goes over the cluster to the node’s Docker daemon. The <code>join</code> command joins a node to the cluster and registers it with the Consul discovery service.</p> <p><strong>frontend01</strong>:</p> <pre>  docker -H=tcp://192.168.33.20:2375 run -d swarm join --advertise=192.168.33.20:2375 consul://192.168.33.11:8500/
</pre> <p><strong>frontend02</strong>:</p> <pre>  docker -H=tcp://192.168.33.21:2375 run -d swarm join --advertise=192.168.33.21:2375 consul://192.168.33.11:8500/
</pre> <p><strong>worker01</strong>:</p> <pre>  docker -H=tcp://192.168.33.200:2375 run -d swarm join --advertise=192.168.33.200:2375 consul://192.168.33.11:8500/
</pre> <p><strong>store</strong>:</p> <pre>  docker -H=tcp://192.168.33.250:2375 run -d swarm join --advertise=192.168.33.250:2375 consul://192.168.33.11:8500/
</pre>
</li> <li>
<p>Run the <code>docker info</code> command again to view your cluster with all its nodes.</p> <pre>  $ docker info
  Containers: 4
  Images: 4
  Role: primary
  Strategy: spread
  Filters: health, port, dependency, affinity, constraint
  Nodes: 4
   frontend01: 192.168.33.20:2375
    └ Status: Healthy
    └ Containers: 1
    └ Reserved CPUs: 0 / 1
    └ Reserved Memory: 0 B / 1.017 GiB
    └ Labels: executiondriver=native-0.2, kernelversion=3.16.0-57-generic, operatingsystem=Ubuntu 14.04.3 LTS, storagedriver=aufs
    └ Error: (none)
    └ UpdatedAt: 2016-02-14T19:02:53Z
   frontend02: 192.168.33.21:2375
    └ Status: Healthy
    └ Containers: 1
    └ Reserved CPUs: 0 / 1
    └ Reserved Memory: 0 B / 1.017 GiB
    └ Labels: executiondriver=native-0.2, kernelversion=3.16.0-57-generic, operatingsystem=Ubuntu 14.04.3 LTS, storagedriver=aufs
    └ Error: (none)
    └ UpdatedAt: 2016-02-14T19:02:58Z
   store: 192.168.33.250:2375
    └ Status: Healthy
    └ Containers: 1
    └ Reserved CPUs: 0 / 1
    └ Reserved Memory: 0 B / 3.86 GiB
    └ Labels: executiondriver=native-0.2, kernelversion=3.16.0-57-generic, operatingsystem=Ubuntu 14.04.3 LTS, storagedriver=aufs
    └ Error: (none)
    └ UpdatedAt: 2016-02-14T19:02:58Z
   worker01: 192.168.33.200:2375
    └ Status: Healthy
    └ Containers: 1
    └ Reserved CPUs: 0 / 1
    └ Reserved Memory: 0 B / 1.017 GiB
    └ Labels: executiondriver=native-0.2, kernelversion=3.16.0-57-generic, operatingsystem=Ubuntu 14.04.3 LTS, storagedriver=aufs
    └ Error: (none)
    └ UpdatedAt: 2016-02-14T19:03:21Z
  Kernel Version: 3.16.0-57-generic
  Operating System: linux
  CPUs: 4
  Total Memory: 6.912 GiB
  Name: f5f093aa9679
</pre>
</li> </ol> <h3 id="step-2-review-your-work">Step 2: Review your work</h3> <p>The diagram below shows the Swarm cluster that you created.</p> <p><img src="https://docs.docker.com/v1.10/swarm/images/review-work.jpg" alt=""></p> <p>The <code>manager</code> node is running two containers: <code>consul</code> and <code>swarm</code>. The <code>consul</code> container is providing the Swarm discovery service. This is where nodes and services register themselves and discover each other. The <code>swarm</code> container is running the <code>swarm manage</code> process which makes it act as the cluster manager. The manager is responsible for accepting Docker commands issued against the cluster and scheduling resources on the cluster.</p> <p>You mapped port 3375 on the <code>manager</code> node to port 2375 inside the <code>swarm</code> container. As a result, Docker clients (for example the CLI) wishing to issue commands against the cluster must send them to the <code>manager</code> node on port 3375. The <code>swarm</code> container then executes those commands against the relevant node(s) in the cluster over port 2375.</p> <p>Now that you have your Swarm cluster configured, you’ll overlay the container network that the application containers will be part of.</p> <h3 id="step-2-overlay-a-container-network">Step 2: Overlay a container network</h3> <p>All containers that are part of the voting application should belong to a container network called <code>mynet</code>. Container networking is a Docker Engine feature. The<code>mynet</code> network is a overlay network type that runs on top of the communication network.</p> <p>A container network can span multiple hosts on multiple networks. As a result, the <code>mynet</code> container network allows all the voting application containers to easily communicate irrespective of the underlying communication network that each node is on.</p> <p>You can create the network and join the containers from any node in your VPC that is running Docker Engine. However, best practice when using Docker Swarm is to execute commands from the <code>manager</code> node, as this is where all management tasks happen.</p> <ol> <li><p>If you haven’t already, <code>ssh</code> into a terminal on your <code>manager</code> node.</p></li> <li>
<p>Get information about the network running on just the <code>manager</code> node.</p> <p>You do this by passing the <code>-H</code> flag to restrict the Engine CLI to just the manager node.</p> <pre>$ docker -H=tcp://192.168.33.11:2375 network ls
NETWORK ID          NAME                DRIVER
d01e8f0303a9        none                null                
bab7a9c42a23        host                host                
12fd2a115d85        bridge              bridge   
</pre>
</li> <li>
<p>Now, run the same command to get cluster information.</p> <p>Provided you set <code>export DOCKER_HOST="tcp://192.168.33.11:3375"</code>`, the command directs to the Swarm port and returns information from each node in the cluster.</p> <pre>  $ docker network ls
  NETWORK ID          NAME                DRIVER
  82ce2adce6a7        store/bridge        bridge              
  c3ca43d2622d        store/host          host                
  d126c4b1e092        frontend01/host     host                
  6ea89a1a5b6a        frontend01/bridge   bridge              
  d3ddb830d7f5        frontend02/host     host                
  44f238353c14        frontend02/bridge   bridge              
  c500baec447e        frontend02/none     null                
  77897352f138        store/none          null                
  8060bd575770        frontend01/none     null                
  429e4b8c2c8d        worker01/bridge     bridge              
  57c5001aca83        worker01/none       null                
  2101103b8494        worker01/host       host        
</pre>
</li> <li>
<p>Create the overlay network with the <code>docker network</code> command</p> <pre>$ docker network create --driver overlay mynet
</pre>
</li> <li>
<p>Repeat the two network commands again to see how the network list has changed.</p> <pre>docker network ls
docker -H=tcp://192.168.33.11:2375 network ls
</pre> <p>As all Swarm nodes in your environment are configured to use the Consul discovery service at <code>consul://192.168.33.11:8500</code>, they all should see the new overlay network. Verify this with the next step.</p>
</li> <li>
<p>Try running a network command on an individual node, for example to run it on the <code>frontend01</code> node:</p> <pre>  docker -H=tcp://192.168.33.20:2375 network ls
</pre> <p>You should see an entry for the <code>mynet</code> network using the <code>overlay</code> driver as shown above. You would get the same output if your ran the command from node’s command line.</p>
</li> </ol> <h2 id="step-4-review-your-work">Step 4: Review your work</h2> <p>The diagram below shows the complete cluster configuration including the overlay container network, <code>mynet</code>. The <code>mynet</code> is shown as red and is available to all Docker hosts using the Consul discovery backend. Later in the procedure you will connect containers to this network.</p> <p><img src="https://docs.docker.com/v1.10/swarm/images/overlay-review.jpg" alt=""></p> <p>The <code>swarm</code> and <code>consul</code> containers on the <code>manager</code> node are not attached to the <code>mynet</code> overlay network. These containers are running on <code>manager</code> nodes default bridge network. To verify this try using these two commands:</p> <pre>  docker -H=tcp://192.168.33.11:2375 network inspect bridge
  docker -H=tcp://192.168.33.11:2375 network inspect mynet
</pre> <p>You should find two containers running on the <code>manager</code> node’s <code>bridge</code> network but nothing yet on the <code>mynet</code> network.</p> <h2 id="next-step">Next Step</h2> <p>Your Swarm cluster is now built and you are ready to <a href="../04-deploy-app/">build and run the voting application</a> on it.</p>
<div class="_attribution">
  <p class="_attribution-p">
    © 2013–2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>
    <a href="https://docs.docker.com/v1.10/swarm/swarm_at_scale/03-create-cluster/" class="_attribution-link" target="_blank">https://docs.docker.com/v1.10/swarm/swarm_at_scale/03-create-cluster/</a>
  </p>
</div>

			</div>
		</div>
	</section>

	</div>
</body>
</html>
