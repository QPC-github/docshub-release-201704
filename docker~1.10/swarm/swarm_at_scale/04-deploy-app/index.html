
<!DOCTYPE HTML>

<html lang="en">

<head>
  <meta charset="utf-8">
  <title>Deploy the Application (Swarm) - Docker 1.10 - W3cubDocs</title>
  
  <meta name="description" content=" You’ve built a Swarm cluster so now you are ready to build and deploy the voting application itself. ">
  <meta name="keywords" content="deploy, application, swarm, -, docker, docker~1.10">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="http://docs.w3cub.com/docker~1.10/swarm/swarm_at_scale/04-deploy-app/">
  <link href="/favicon.png" rel="icon">
  <link type="text/css" rel="stylesheet" href="/assets/application-50364fff564ce3b6327021805f3f00e2957b441cf27f576a7dd4ff63bbc47047.css">
  <script type="text/javascript" src="/assets/application-db64bfd54ceb42be11af7995804cf4902548419ceb79d509b0b7d62c22d98e6f.js"></script>
  <script src="/json/docker~1.10.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body>
	<div class="_app">
	<header class="_header">
  
  <form class="_search">
    <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
    <a class="_search-clear"></a>
    <div class="_search-tag"></div>
  </form>
  
  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/docker~1.10/" class="_nav-link" title="" style="margin-left:0;">Docker 1.10</a></span>
  
  <nav class="_nav">
    <a href="/app/" class="_nav-link ">App</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		<div class="_list">
			
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<div class="_page _docker">
				
<h1 id="deploy-the-application">Deploy the application</h1> <p>You’ve <a href="../03-create-cluster/">built a Swarm cluster</a> so now you are ready to build and deploy the voting application itself.</p> <h2 id="step-1-learn-about-the-images">Step 1: Learn about the images</h2> <p>Some of the application’s containers are launched form existing images pulled directly from Docker Hub. Other containers are launched from custom images you must build. The list below shows which containers use custom images and which do not:</p> <ul> <li>Load balancer container: stock image (<code>ehazlett/interlock</code>)</li> <li>Redis containers: stock image (official <code>redis</code> image)</li> <li>Postgres (PostgreSQL) containers: stock image (official <code>postgres</code> image)</li> <li>Web containers: custom built image</li> <li>Worker containers: custom built image</li> <li>Results containers: custom built image</li> </ul> <p>All custom built images are built using Dockerfile’s pulled from the <a href="https://github.com/docker/swarm-microservice-demo-v1" target="_blank">example application’s public GitHub repository</a>.</p> <ol> <li><p>If you haven’t already, <code>ssh</code> into the Swarm <code>manager</code> node.</p></li> <li>
<p>Clone the <a href="https://github.com/docker/swarm-microservice-demo-v1" target="_blank">application’s GitHub repo</a></p> <pre>$ git clone https://github.com/docker/swarm-microservice-demo-v1
sudo: unable to resolve host master
Cloning into 'swarm-microservice-demo-v1'...
remote: Counting objects: 304, done.
remote: Compressing objects: 100% (17/17), done.
remote: Total 304 (delta 5), reused 0 (delta 0), pack-reused 287
Receiving objects: 100% (304/304), 2.24 MiB | 2.88 MiB/s, done.
Resolving deltas: 100% (132/132), done.
Checking connectivity... done.
</pre> <p>This command creates a new directory structure inside of your working directory. The new directory contains all of the files and folders required to build the voting application images.</p> <p>The <code>AWS</code> directory contains the <code>cloudformation.json</code> file used to deploy the EC2 instances. The <code>Vagrant</code> directory contains files and instructions required to deploy the application using Vagrant. The <code>results-app</code>, <code>vote-worker</code>, and <code>web-vote-app</code> directories contain the Dockerfiles and other files required to build the custom images for those particular components of the application.</p>
</li> <li>
<p>Change directory into the <code>swarm-microservice-demo-v1/web-vote-app</code> directory.</p> <pre>$ cd swarm-microservice-demo-v1/web-vote-app/
</pre>
</li> <li>
<p>View the Dockerfile contents.</p> <pre>$ cat Dockerfile  
# Using official python runtime base image
FROM python:2.7
# Set the application directory
WORKDIR /app
# Install our requirements.txt
ADD requirements.txt /app/requirements.txt
RUN pip install -r requirements.txt
# Copy our code from the current folder to /app inside the container
ADD . /app
# Make port 80 available for links and/or publish
EXPOSE 80
# Define our command to be run when launching the container
CMD ["python", "app.py"]
</pre> <p>As you can see, the image is based on the official <code>Python:2.7</code> tagged image, adds a requirements file into the <code>/app</code> directory, installs requirements, copies files from the build context into the container, exposes port <code>80</code> and tells the container which command to run.</p>
</li> <li><p>Spend time investigating the other parts of the application by viewing the <code>results-app/Dockefile</code> and the <code>vote-worker/Dockerfile</code> in the application.</p></li> </ol> <h2 id="step-2-build-custom-images">Step 2. Build custom images</h2> <ol> <li><p>If you haven’t already, <code>ssh</code> into the Swarm <code>manager</code> node.</p></li> <li>
<p>Make sure you have DOCKER_HOST set</p> <pre>$ export DOCKER_HOST="tcp://192.168.33.11:3375"
</pre>
</li> <li><p>Change to the root of your <code>swarm-microservice-demo-v1</code> clone.</p></li> <li>
<p>Build the <code>web-votes-app</code> image both the front end nodes.</p> <p><strong>frontend01</strong>:</p> <pre>$ docker -H tcp://192.168.33.20:2375 build -t web-vote-app ./web-vote-app
</pre> <p><strong>frontend02</strong>:</p> <pre>$ docker -H tcp://192.168.33.21:2375 build -t web-vote-app ./web-vote-app
</pre> <p>These commands build the <code>web-vote-app</code> image on the <code>frontend01</code> and <code>frontend02</code> nodes. To accomplish the operation, each command copies the contents of the <code>swarm-microservice-demo-v1/web-vote-app</code> sub-directory from the <code>manager</code> node to each frontend node. The command then instructs the Docker daemon on each frontend node to build the image and store it locally.</p> <p>You’ll notice this example uses a <code>-H</code> flag to pull an image to specific host. This is to help you conceptualize the architecture for this sample. In a production deployment, you’d omit this option and rely on the Swarm manager to distribute the image. The manager would pull the image to every node; so that any node can step in to run the image as needed.</p> <p>It may take a minute or so for each image to build. Wait for the builds to finish.</p>
</li> <li>
<p>Build <code>vote-worker</code> image on the <code>worker01</code> node</p> <pre>$ docker -H tcp://192.168.33.200:2375 build -t vote-worker ./vote-worker
</pre> <p>It may take a minute or so for the image to build. Wait for the build to finish.</p>
</li> <li>
<p>Build the <code>results-app</code> on the <code>store</code> node</p> <pre>$ docker -H tcp://192.168.33.250:2375 build -t results-app ./results-app
</pre>
</li> </ol> <p>Each of the <em>custom images</em> required by the application is now built and stored locally on the nodes that will use them.</p> <h2 id="step-3-pull-images-from-docker-hub">Step 3. Pull images from Docker Hub</h2> <p>For performance reasons, it is always better to pull any required Docker Hub images locally on each instance that needs them. This ensures that containers based on those images can start quickly.</p> <ol> <li><p>Log into the Swarm <code>manager</code> node.</p></li> <li>
<p>Pull the <code>redis</code> image to your frontend nodes.</p> <p><strong>frontend01</strong>:</p> <pre>$ docker -H tcp://192.168.33.20:2375 pull redis
</pre> <p><strong>frontend02</strong>:</p> <pre>$ docker -H tcp://192.168.33.21:2375 pull redis
</pre>
</li> <li>
<p>Pull the <code>postgres</code> image to the <code>store</code> node</p> <pre>$ docker -H tcp://192.168.33.250:2375 pull postgres
</pre>
</li> <li>
<p>Pull the <code>ehazlett/interlock</code> image to the <code>interlock</code> node</p> <pre>$ docker -H tcp://192.168.33.12:2375 pull ehazlett/interlock
</pre>
</li> </ol> <p>Each node in the cluster, as well as the <code>interlock</code> node, now has the required images stored locally as shown below.</p> <p><img src="https://docs.docker.com/v1.10/swarm/images/interlock.jpg" alt=""></p> <p>Now that all images are built, pulled, and stored locally, the next step is to start the application.</p> <h2 id="step-4-start-the-voting-application">Step 4. Start the voting application</h2> <p>In the following steps, your launch several containers to the voting application.</p> <ol> <li><p>If you haven’t already, <code>ssh</code> into the Swarm <code>manager</code> node.</p></li> <li>
<p>Start the <code>interlock</code> container on the <code>interlock</code> node</p> <pre>$ docker -H tcp://192.168.33.12:2375 run --restart=unless-stopped -p 80:80 --name interlock -d ehazlett/interlock --swarm-url tcp://192.168.33.11:3375 --plugin haproxy start
</pre> <p>This command is issued against the <code>interlock</code> instance and maps port 80 on the instance to port 80 inside the container. This allows the container to load balance connections coming in over port 80 (HTTP). The command also applies the <code>--restart=unless-stopped</code> policy to the container, telling Docker to restart the container if it exits unexpectedly.</p>
</li> <li>
<p>Verify the container is running.</p> <pre>$ docker -H tcp://192.168.33.12:2375 ps
</pre>
</li> <li>
<p>Start a <code>redis</code> container on your front end nodes.</p> <p><strong>frontend01</strong>:</p> <pre>$ docker run --restart=unless-stopped --env="constraint:node==frontend01" -p 6379:6379 --name redis01 --net mynet -d redis
$ docker -H tcp://192.168.33.20:2375 ps
</pre> <p><strong>frontend02</strong>:</p> <pre>$ docker run --restart=unless-stopped --env="constraint:node==frontend02" -p 6379:6379 --name redis02 --net mynet -d redis
$ docker -H tcp://192.168.33.21:2375 ps
</pre> <p>These two commands are issued against the Swarm cluster. The commands specify <em>node constraints</em>, forcing Swarm to start the contaienrs on <code>frontend01</code> and <code>frontend02</code>. Port 6379 on each instance is mapped to port 6379 inside of each container for debugging purposes. The command also applies the <code>--restart=unless-stopped</code> policy to the containers and attaches them to the <code>mynet</code> overlay network.</p>
</li> <li>
<p>Start a <code>web-vote-app</code> container the frontend nodes.</p> <p><strong>frontend01</strong>:</p> <pre>$ docker run --restart=unless-stopped --env="constraint:node==frontend01" -d -p 5000:80 -e WEB_VOTE_NUMBER='01' --name frontend01 --net mynet --hostname votingapp.local web-vote-app
</pre> <p><strong>frontend02</strong>:</p> <pre>$ docker run --restart=unless-stopped --env="constraint:node==frontend02" -d -p 5000:80 -e WEB_VOTE_NUMBER='02' --name frontend02 --net mynet --hostname votingapp.local web-vote-app
</pre> <p>These two commands are issued against the Swarm cluster. The commands specify <em>node constraints</em>, forcing Swarm to start the contaienrs on <code>frontend01</code> and <code>frontend02</code>. Port <code>5000</code> on each node is mapped to port <code>80</code> inside of each container. This allows connections to come in to each node on port <code>5000</code> and be forwarded to port <code>80</code> inside of each container.</p> <p>Both containers are attached to the <code>mynet</code> overlay network and both containers are given the <code>votingapp-local</code> hostname. The <code>--restart=unless-stopped</code> policy is also applied to these containers.</p>
</li> <li>
<p>Start the <code>postgres</code> container on the <code>store</code> node</p> <pre>$ docker run --restart=unless-stopped --env="constraint:node==store" --name pg -e POSTGRES_PASSWORD=pg8675309 --net mynet -p 5432:5432 -d postgres
</pre> <p>This command is issued against the Swarm cluster and starts the container on <code>store</code>. It maps port 5432 on the <code>store</code> node to port 5432 inside the container and attaches the container to the <code>mynet</code> overlay network. The command also inserts the database password into the container via the <code>POSTGRES_PASSWORD</code> environment variable and applies the <code>--restart=unless-stopped</code> policy to the container.</p> <p>Sharing passwords like this is not recommended for production use cases.</p>
</li> <li>
<p>Start the <code>worker01</code> container on the <code>worker01</code> node</p> <pre>$ docker run --restart=unless-stopped --env="constraint:node==worker01" -d -e WORKER_NUMBER='01' -e FROM_REDIS_HOST=1 -e TO_REDIS_HOST=2 --name worker01 --net mynet vote-worker
</pre> <p>This command is issued against the Swarm manager and uses a constraint to start the container on the <code>worker01</code> node. It passes configuration data into the container via environment variables, telling the worker container to clear the queues on <code>frontend01</code> and <code>frontend02</code>. It adds the container to the <code>mynet</code> overlay network and applies the <code>--restart=unless-stopped</code> policy to the container.</p>
</li> <li>
<p>Start the <code>results-app</code> container on the <code>store</code> node</p> <pre>$ docker run --restart=unless-stopped --env="constraint:node==store" -p 80:80 -d --name results-app --net mynet results-app
</pre> <p>This command starts the results-app container on the <code>store</code> node by means of a <em>node constraint</em>. It maps port 80 on the <code>store</code> node to port 80 inside the container. It adds the container to the <code>mynet</code> overlay network and applies the <code>--restart=unless-stopped</code> policy to the container.</p>
</li> </ol> <p>The application is now fully deployed as shown in the diagram below.</p> <p><img src="https://docs.docker.com/v1.10/swarm/images/fully-deployed.jpg" alt=""></p> <h2 id="step-5-test-the-application">Step 5. Test the application</h2> <p>Now that the application is deployed and running, it’s time to test it. To do this, you configure a DNS mapping on the machine where you are running your web browser. This maps the “votingapp.local” DNS name to the public IP address of the <code>interlock</code> node.</p> <ol> <li>
<p>Configure the DNS name resolution on your local machine for browsing.</p> <ul> <li>On Windows machines this is done by adding <code>votingapp.local &lt;interlock-public-ip&gt;</code> to the <code>C:\Windows\System32\Drivers\etc\hosts file</code>. Modifying this file requires administrator privileges. To open the file with administrator privileges, right-click <code>C:\Windows\System32\notepad.exe</code> and select <code>Run as administrator</code>. Once Notepad is open, click <code>file</code> &gt; <code>open</code> and open the file and make the edit.</li> <li>On OSX machines this is done by adding <code>votingapp.local &lt;interlock-public-ip&gt;</code> to <code>/private/etc/hosts</code>.</li> <li>On most Linux machines this is done by adding <code>votingapp.local &lt;interlock-public-ip&gt;</code> to <code>/etc/hosts</code>.</li> </ul> <p>Be sure to replace <code>&lt;interlock-public-ip&gt;</code> with the public IP address of your <code>interlock</code> node. You can find the <code>interlock</code> node’s Public IP by selecting your <code>interlock</code> EC2 Instance from within the AWS EC2 console.</p>
</li> <li>
<p>Verify the mapping worked with a <code>ping</code> command from your local machine.</p> <pre>ping votingapp.local
Pinging votingapp.local [54.183.164.230] with 32 bytes of data:
Reply from 54.183.164.230: bytes=32 time=164ms TTL=42
Reply from 54.183.164.230: bytes=32 time=163ms TTL=42
Reply from 54.183.164.230: bytes=32 time=169ms TTL=42
</pre>
</li> <li>
<p>Point your web browser to <a href="http://votingapp.local" target="_blank">http://votingapp.local</a></p> <p><img src="https://docs.docker.com/v1.10/swarm/images/vote-app-test.jpg" alt=""></p> <p>Notice the text at the bottom of the web page. This shows which web container serviced the request. In the diagram above, this is <code>frontend02</code>. If you refresh your web browser you should see this change as the Interlock load balancer shares incoming requests across both web containers.</p> <p>To see more detailed load balancer data from the Interlock service, point your web browser to <a href="http://stats:interlock@votingapp.local/haproxy?stats" target="_blank">http://stats:interlock@votingapp.local/haproxy?stats</a></p> <p><img src="https://docs.docker.com/v1.10/swarm/images/proxy-test.jpg" alt=""></p>
</li> <li><p>Cast your vote. It is recommended to choose “Dogs” ;-)</p></li> <li>
<p>To see the results of the poll, you can point your web browser at the public IP of the <code>store</code> node</p> <p><img src="https://docs.docker.com/v1.10/swarm/images/poll-results.jpg" alt=""></p>
</li> </ol> <h2 id="next-steps">Next steps</h2> <p>Congratulations. You have successfully walked through manually deploying a microservice-based application to a Swarm cluster. Of course, not every deployment goes smoothly. Now that you’ve learned how to successfully deploy an application at scale, you should learn <a href="../05-troubleshoot/">what to consider when troubleshooting large applications running on a Swarm cluster</a>.</p>
<div class="_attribution">
  <p class="_attribution-p">
    © 2013–2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>
    <a href="https://docs.docker.com/v1.10/swarm/swarm_at_scale/04-deploy-app/" class="_attribution-link" target="_blank">https://docs.docker.com/v1.10/swarm/swarm_at_scale/04-deploy-app/</a>
  </p>
</div>

			</div>
		</div>
	</section>

	</div>
</body>
</html>
