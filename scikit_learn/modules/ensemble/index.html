
<!DOCTYPE HTML>

<html lang="en">

<head>
  <meta charset="utf-8">
  <title>1.11. Ensemble Methods - Scikit-learn - W3cubDocs</title>
  
  <meta name="description" content="The goal of ensemble methods is to combine the predictions of several base estimators built with a given learning algorithm in order to improve &hellip;">
  <meta name="keywords" content="ensemble, methods, -, scikit-learn, scikit_learn">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="http://docs.w3cub.com/scikit_learn/modules/ensemble/">
  <link href="/favicon.png" rel="icon">
  <link type="text/css" rel="stylesheet" href="/assets/application-50364fff564ce3b6327021805f3f00e2957b441cf27f576a7dd4ff63bbc47047.css">
  <script type="text/javascript" src="/assets/application-db64bfd54ceb42be11af7995804cf4902548419ceb79d509b0b7d62c22d98e6f.js"></script>
  <script src="/json/scikit_learn.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body>
	<div class="_app">
	<header class="_header">
  
  <form class="_search">
    <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
    <a class="_search-clear"></a>
    <div class="_search-tag"></div>
  </form>
  
  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/scikit_learn/" class="_nav-link" title="" style="margin-left:0;">scikit-learn</a></span>
  
  <nav class="_nav">
    <a href="/app/" class="_nav-link ">App</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		<div class="_list">
			
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<div class="_page _sphinx">
				
<h1 id="ensemble">1.11. Ensemble methods</h1> <p id="ensemble-methods">The goal of <strong>ensemble methods</strong> is to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability / robustness over a single estimator.</p> <p>Two families of ensemble methods are usually distinguished:</p> <ul> <li>
<p class="first">In <strong>averaging methods</strong>, the driving principle is to build several estimators independently and then to average their predictions. On average, the combined estimator is usually better than any of the single base estimator because its variance is reduced.</p> <p><strong>Examples:</strong> <a class="reference internal" href="#bagging"><span class="std std-ref">Bagging methods</span></a>, <a class="reference internal" href="#forest"><span class="std std-ref">Forests of randomized trees</span></a>, ...</p> </li> <li>
<p class="first">By contrast, in <strong>boosting methods</strong>, base estimators are built sequentially and one tries to reduce the bias of the combined estimator. The motivation is to combine several weak models to produce a powerful ensemble.</p> <p><strong>Examples:</strong> <a class="reference internal" href="#adaboost"><span class="std std-ref">AdaBoost</span></a>, <a class="reference internal" href="#gradient-boosting"><span class="std std-ref">Gradient Tree Boosting</span></a>, ...</p> </li> </ul>  <h2 id="bagging">1.11.1. Bagging meta-estimator</h2> <p id="bagging-meta-estimator">In ensemble algorithms, bagging methods form a class of algorithms which build several instances of a black-box estimator on random subsets of the original training set and then aggregate their individual predictions to form a final prediction. These methods are used as a way to reduce the variance of a base estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. In many cases, bagging methods constitute a very simple way to improve with respect to a single model, without making it necessary to adapt the underlying base algorithm. As they provide a way to reduce overfitting, bagging methods work best with strong and complex models (e.g., fully developed decision trees), in contrast with boosting methods which usually work best with weak models (e.g., shallow decision trees).</p> <p>Bagging methods come in many flavours but mostly differ from each other by the way they draw random subsets of the training set:</p>  <ul class="simple"> <li>When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting <a class="reference internal" href="#b1999" id="id1">[B1999]</a>.</li> <li>When samples are drawn with replacement, then the method is known as Bagging <a class="reference internal" href="#b1996" id="id2">[B1996]</a>.</li> <li>When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces <a class="reference internal" href="#h1998" id="id3">[H1998]</a>.</li> <li>Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches <a class="reference internal" href="#lg2012" id="id4">[LG2012]</a>.</li> </ul>  <p>In scikit-learn, bagging methods are offered as a unified <a class="reference internal" href="../generated/sklearn.ensemble.baggingclassifier/#sklearn.ensemble.BaggingClassifier" title="sklearn.ensemble.BaggingClassifier"><code>BaggingClassifier</code></a> meta-estimator (resp. <a class="reference internal" href="../generated/sklearn.ensemble.baggingregressor/#sklearn.ensemble.BaggingRegressor" title="sklearn.ensemble.BaggingRegressor"><code>BaggingRegressor</code></a>), taking as input a user-specified base estimator along with parameters specifying the strategy to draw random subsets. In particular, <code>max_samples</code> and <code>max_features</code> control the size of the subsets (in terms of samples and features), while <code>bootstrap</code> and <code>bootstrap_features</code> control whether samples and features are drawn with or without replacement. When using a subset of the available samples the generalization accuracy can be estimated with the out-of-bag samples by setting <code>oob_score=True</code>. As an example, the snippet below illustrates how to instantiate a bagging ensemble of <code>KNeighborsClassifier</code> base estimators, each built on random subsets of 50% of the samples and 50% of the features.</p> <pre data-language="python">&gt;&gt;&gt; from sklearn.ensemble import BaggingClassifier
&gt;&gt;&gt; from sklearn.neighbors import KNeighborsClassifier
&gt;&gt;&gt; bagging = BaggingClassifier(KNeighborsClassifier(),
...                             max_samples=0.5, max_features=0.5)
</pre> <div class="topic"> <p class="topic-title first">Examples:</p> <ul class="simple"> <li><a class="reference internal" href="../../auto_examples/ensemble/plot_bias_variance/#sphx-glr-auto-examples-ensemble-plot-bias-variance-py"><span class="std std-ref">Single estimator versus bagging: bias-variance decomposition</span></a></li> </ul> </div> <div class="topic"> <p class="topic-title first">References</p> <table class="docutils citation" frame="void" id="b1999" rules="none">   <tr>
<td class="label"><a class="fn-backref" href="#id1">[B1999]</a></td>
<td>L. Breiman, “Pasting small votes for classification in large databases and on-line”, Machine Learning, 36(1), 85-103, 1999.</td>
</tr>  </table> <table class="docutils citation" frame="void" id="b1996" rules="none">   <tr>
<td class="label"><a class="fn-backref" href="#id2">[B1996]</a></td>
<td>L. Breiman, “Bagging predictors”, Machine Learning, 24(2), 123-140, 1996.</td>
</tr>  </table> <table class="docutils citation" frame="void" id="h1998" rules="none">   <tr>
<td class="label"><a class="fn-backref" href="#id3">[H1998]</a></td>
<td>T. Ho, “The random subspace method for constructing decision forests”, Pattern Analysis and Machine Intelligence, 20(8), 832-844, 1998.</td>
</tr>  </table> <table class="docutils citation" frame="void" id="lg2012" rules="none">   <tr>
<td class="label"><a class="fn-backref" href="#id4">[LG2012]</a></td>
<td>G. Louppe and P. Geurts, “Ensembles on Random Patches”, Machine Learning and Knowledge Discovery in Databases, 346-361, 2012.</td>
</tr>  </table> </div>   <h2 id="forest">1.11.2. Forests of randomized trees</h2> <p id="forests-of-randomized-trees">The <a class="reference internal" href="../classes/#module-sklearn.ensemble" title="sklearn.ensemble"><code>sklearn.ensemble</code></a> module includes two averaging algorithms based on randomized <a class="reference internal" href="../tree/#tree"><span class="std std-ref">decision trees</span></a>: the RandomForest algorithm and the Extra-Trees method. Both algorithms are perturb-and-combine techniques <a class="reference internal" href="#b1998" id="id5">[B1998]</a> specifically designed for trees. This means a diverse set of classifiers is created by introducing randomness in the classifier construction. The prediction of the ensemble is given as the averaged prediction of the individual classifiers.</p> <p>As other classifiers, forest classifiers have to be fitted with two arrays: a sparse or dense array X of size <code>[n_samples, n_features]</code> holding the training samples, and an array Y of size <code>[n_samples]</code> holding the target values (class labels) for the training samples:</p> <pre data-language="python">&gt;&gt;&gt; from sklearn.ensemble import RandomForestClassifier
&gt;&gt;&gt; X = [[0, 0], [1, 1]]
&gt;&gt;&gt; Y = [0, 1]
&gt;&gt;&gt; clf = RandomForestClassifier(n_estimators=10)
&gt;&gt;&gt; clf = clf.fit(X, Y)
</pre> <p>Like <a class="reference internal" href="../tree/#tree"><span class="std std-ref">decision trees</span></a>, forests of trees also extend to <a class="reference internal" href="../tree/#tree-multioutput"><span class="std std-ref">multi-output problems</span></a> (if Y is an array of size <code>[n_samples, n_outputs]</code>).</p>  <h3 id="random-forests">1.11.2.1. Random Forests</h3> <p>In random forests (see <a class="reference internal" href="../generated/sklearn.ensemble.randomforestclassifier/#sklearn.ensemble.RandomForestClassifier" title="sklearn.ensemble.RandomForestClassifier"><code>RandomForestClassifier</code></a> and <a class="reference internal" href="../generated/sklearn.ensemble.randomforestregressor/#sklearn.ensemble.RandomForestRegressor" title="sklearn.ensemble.RandomForestRegressor"><code>RandomForestRegressor</code></a> classes), each tree in the ensemble is built from a sample drawn with replacement (i.e., a bootstrap sample) from the training set. In addition, when splitting a node during the construction of the tree, the split that is chosen is no longer the best split among all features. Instead, the split that is picked is the best split among a random subset of the features. As a result of this randomness, the bias of the forest usually slightly increases (with respect to the bias of a single non-random tree) but, due to averaging, its variance also decreases, usually more than compensating for the increase in bias, hence yielding an overall better model.</p> <p>In contrast to the original publication <a class="reference internal" href="#b2001" id="id6">[B2001]</a>, the scikit-learn implementation combines classifiers by averaging their probabilistic prediction, instead of letting each classifier vote for a single class.</p>   <h3 id="extremely-randomized-trees">1.11.2.2. Extremely Randomized Trees</h3> <p>In extremely randomized trees (see <a class="reference internal" href="../generated/sklearn.ensemble.extratreesclassifier/#sklearn.ensemble.ExtraTreesClassifier" title="sklearn.ensemble.ExtraTreesClassifier"><code>ExtraTreesClassifier</code></a> and <a class="reference internal" href="../generated/sklearn.ensemble.extratreesregressor/#sklearn.ensemble.ExtraTreesRegressor" title="sklearn.ensemble.ExtraTreesRegressor"><code>ExtraTreesRegressor</code></a> classes), randomness goes one step further in the way splits are computed. As in random forests, a random subset of candidate features is used, but instead of looking for the most discriminative thresholds, thresholds are drawn at random for each candidate feature and the best of these randomly-generated thresholds is picked as the splitting rule. This usually allows to reduce the variance of the model a bit more, at the expense of a slightly greater increase in bias:</p> <pre data-language="python">&gt;&gt;&gt; from sklearn.model_selection import cross_val_score
&gt;&gt;&gt; from sklearn.datasets import make_blobs
&gt;&gt;&gt; from sklearn.ensemble import RandomForestClassifier
&gt;&gt;&gt; from sklearn.ensemble import ExtraTreesClassifier
&gt;&gt;&gt; from sklearn.tree import DecisionTreeClassifier

&gt;&gt;&gt; X, y = make_blobs(n_samples=10000, n_features=10, centers=100,
...     random_state=0)

&gt;&gt;&gt; clf = DecisionTreeClassifier(max_depth=None, min_samples_split=2,
...     random_state=0)
&gt;&gt;&gt; scores = cross_val_score(clf, X, y)
&gt;&gt;&gt; scores.mean()                             
0.97...

&gt;&gt;&gt; clf = RandomForestClassifier(n_estimators=10, max_depth=None,
...     min_samples_split=2, random_state=0)
&gt;&gt;&gt; scores = cross_val_score(clf, X, y)
&gt;&gt;&gt; scores.mean()                             
0.999...

&gt;&gt;&gt; clf = ExtraTreesClassifier(n_estimators=10, max_depth=None,
...     min_samples_split=2, random_state=0)
&gt;&gt;&gt; scores = cross_val_score(clf, X, y)
&gt;&gt;&gt; scores.mean() &gt; 0.999
True
</pre> <div class="figure align-center"> <a class="reference external image-reference" href="../../auto_examples/ensemble/plot_forest_iris/"><img alt="../_images/sphx_glr_plot_forest_iris_0011.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_forest_iris_0011.png" style="width: 600.0px; height: 450.0px;"></a> </div>   <h3 id="parameters">1.11.2.3. Parameters</h3> <p>The main parameters to adjust when using these methods is <code>n_estimators</code> and <code>max_features</code>. The former is the number of trees in the forest. The larger the better, but also the longer it will take to compute. In addition, note that results will stop getting significantly better beyond a critical number of trees. The latter is the size of the random subsets of features to consider when splitting a node. The lower the greater the reduction of variance, but also the greater the increase in bias. Empirical good default values are <code>max_features=n_features</code> for regression problems, and <code>max_features=sqrt(n_features)</code> for classification tasks (where <code>n_features</code> is the number of features in the data). Good results are often achieved when setting <code>max_depth=None</code> in combination with <code>min_samples_split=1</code> (i.e., when fully developing the trees). Bear in mind though that these values are usually not optimal, and might result in models that consume a lot of RAM. The best parameter values should always be cross-validated. In addition, note that in random forests, bootstrap samples are used by default (<code>bootstrap=True</code>) while the default strategy for extra-trees is to use the whole dataset (<code>bootstrap=False</code>). When using bootstrap sampling the generalization accuracy can be estimated on the left out or out-of-bag samples. This can be enabled by setting <code>oob_score=True</code>.</p>   <h3 id="parallelization">1.11.2.4. Parallelization</h3> <p>Finally, this module also features the parallel construction of the trees and the parallel computation of the predictions through the <code>n_jobs</code> parameter. If <code>n_jobs=k</code> then computations are partitioned into <code>k</code> jobs, and run on <code>k</code> cores of the machine. If <code>n_jobs=-1</code> then all cores available on the machine are used. Note that because of inter-process communication overhead, the speedup might not be linear (i.e., using <code>k</code> jobs will unfortunately not be <code>k</code> times as fast). Significant speedup can still be achieved though when building a large number of trees, or when building a single tree requires a fair amount of time (e.g., on large datasets).</p> <div class="topic"> <p class="topic-title first">Examples:</p> <ul class="simple"> <li><a class="reference internal" href="../../auto_examples/ensemble/plot_forest_iris/#sphx-glr-auto-examples-ensemble-plot-forest-iris-py"><span class="std std-ref">Plot the decision surfaces of ensembles of trees on the iris dataset</span></a></li> <li><a class="reference internal" href="../../auto_examples/ensemble/plot_forest_importances_faces/#sphx-glr-auto-examples-ensemble-plot-forest-importances-faces-py"><span class="std std-ref">Pixel importances with a parallel forest of trees</span></a></li> <li><a class="reference internal" href="../../auto_examples/plot_multioutput_face_completion/#sphx-glr-auto-examples-plot-multioutput-face-completion-py"><span class="std std-ref">Face completion with a multi-output estimators</span></a></li> </ul> </div> <div class="topic"> <p class="topic-title first">References</p> <table class="docutils citation" frame="void" id="b2001" rules="none">   <tr>
<td class="label"><a class="fn-backref" href="#id6">[B2001]</a></td>
<td>
<ol class="first last upperalpha simple" start="12"> <li>Breiman, “Random Forests”, Machine Learning, 45(1), 5-32, 2001.</li> </ol> </td>
</tr>  </table> <table class="docutils citation" frame="void" id="b1998" rules="none">   <tr>
<td class="label"><a class="fn-backref" href="#id5">[B1998]</a></td>
<td>
<ol class="first last upperalpha simple" start="12"> <li>Breiman, “Arcing Classifiers”, Annals of Statistics 1998.</li> </ol> </td>
</tr>  </table> <table class="docutils citation" frame="void" id="gew2006" rules="none">   <tr>
<td class="label">[GEW2006]</td>
<td>P. Geurts, D. Ernst., and L. Wehenkel, “Extremely randomized trees”, Machine Learning, 63(1), 3-42, 2006.</td>
</tr>  </table> </div>   <h3 id="random-forest-feature-importance">1.11.2.5. Feature importance evaluation</h3> <p id="feature-importance-evaluation">The relative rank (i.e. depth) of a feature used as a decision node in a tree can be used to assess the relative importance of that feature with respect to the predictability of the target variable. Features used at the top of the tree contribute to the final prediction decision of a larger fraction of the input samples. The <strong>expected fraction of the samples</strong> they contribute to can thus be used as an estimate of the <strong>relative importance of the features</strong>.</p> <p>By <strong>averaging</strong> those expected activity rates over several randomized trees one can <strong>reduce the variance</strong> of such an estimate and use it for feature selection.</p> <p>The following example shows a color-coded representation of the relative importances of each individual pixel for a face recognition task using a <a class="reference internal" href="../generated/sklearn.ensemble.extratreesclassifier/#sklearn.ensemble.ExtraTreesClassifier" title="sklearn.ensemble.ExtraTreesClassifier"><code>ExtraTreesClassifier</code></a> model.</p> <div class="figure align-center"> <a class="reference external image-reference" href="../../auto_examples/ensemble/plot_forest_importances_faces/"><img alt="../_images/sphx_glr_plot_forest_importances_faces_0011.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_forest_importances_faces_0011.png" style="width: 450.0px; height: 450.0px;"></a> </div> <p>In practice those estimates are stored as an attribute named <code>feature_importances_</code> on the fitted model. This is an array with shape <code>(n_features,)</code> whose values are positive and sum to 1.0. The higher the value, the more important is the contribution of the matching feature to the prediction function.</p> <div class="topic"> <p class="topic-title first">Examples:</p> <ul class="simple"> <li><a class="reference internal" href="../../auto_examples/ensemble/plot_forest_importances_faces/#sphx-glr-auto-examples-ensemble-plot-forest-importances-faces-py"><span class="std std-ref">Pixel importances with a parallel forest of trees</span></a></li> <li><a class="reference internal" href="../../auto_examples/ensemble/plot_forest_importances/#sphx-glr-auto-examples-ensemble-plot-forest-importances-py"><span class="std std-ref">Feature importances with forests of trees</span></a></li> </ul> </div>   <h3 id="random-trees-embedding">1.11.2.6. Totally Random Trees Embedding</h3> <p id="totally-random-trees-embedding"><a class="reference internal" href="../generated/sklearn.ensemble.randomtreesembedding/#sklearn.ensemble.RandomTreesEmbedding" title="sklearn.ensemble.RandomTreesEmbedding"><code>RandomTreesEmbedding</code></a> implements an unsupervised transformation of the data. Using a forest of completely random trees, <a class="reference internal" href="../generated/sklearn.ensemble.randomtreesembedding/#sklearn.ensemble.RandomTreesEmbedding" title="sklearn.ensemble.RandomTreesEmbedding"><code>RandomTreesEmbedding</code></a> encodes the data by the indices of the leaves a data point ends up in. This index is then encoded in a one-of-K manner, leading to a high dimensional, sparse binary coding. This coding can be computed very efficiently and can then be used as a basis for other learning tasks. The size and sparsity of the code can be influenced by choosing the number of trees and the maximum depth per tree. For each tree in the ensemble, the coding contains one entry of one. The size of the coding is at most <code>n_estimators * 2
** max_depth</code>, the maximum number of leaves in the forest.</p> <p>As neighboring data points are more likely to lie within the same leaf of a tree, the transformation performs an implicit, non-parametric density estimation.</p> <div class="topic"> <p class="topic-title first">Examples:</p> <ul class="simple"> <li><a class="reference internal" href="../../auto_examples/ensemble/plot_random_forest_embedding/#sphx-glr-auto-examples-ensemble-plot-random-forest-embedding-py"><span class="std std-ref">Hashing feature transformation using Totally Random Trees</span></a></li> <li>
<a class="reference internal" href="../../auto_examples/manifold/plot_lle_digits/#sphx-glr-auto-examples-manifold-plot-lle-digits-py"><span class="std std-ref">Manifold learning on handwritten digits: Locally Linear Embedding, Isomap...</span></a> compares non-linear dimensionality reduction techniques on handwritten digits.</li> <li>
<a class="reference internal" href="../../auto_examples/ensemble/plot_feature_transformation/#sphx-glr-auto-examples-ensemble-plot-feature-transformation-py"><span class="std std-ref">Feature transformations with ensembles of trees</span></a> compares supervised and unsupervised tree based feature transformations.</li> </ul> </div> <div class="admonition seealso"> <p class="first admonition-title">See also</p> <p class="last"><a class="reference internal" href="../manifold/#manifold"><span class="std std-ref">Manifold learning</span></a> techniques can also be useful to derive non-linear representations of feature space, also these approaches focus also on dimensionality reduction.</p> </div>    <h2 id="id7">1.11.3. AdaBoost</h2> <p id="adaboost">The module <a class="reference internal" href="../classes/#module-sklearn.ensemble" title="sklearn.ensemble"><code>sklearn.ensemble</code></a> includes the popular boosting algorithm AdaBoost, introduced in 1995 by Freund and Schapire <a class="reference internal" href="#fs1995" id="id8">[FS1995]</a>.</p> <p>The core principle of AdaBoost is to fit a sequence of weak learners (i.e., models that are only slightly better than random guessing, such as small decision trees) on repeatedly modified versions of the data. The predictions from all of them are then combined through a weighted majority vote (or sum) to produce the final prediction. The data modifications at each so-called boosting iteration consist of applying weights <img class="math" src="http://scikit-learn.org/stable/_images/math/992a357338333368bc3d78583418b3033b9c0857.png" alt="w_1">, <img class="math" src="http://scikit-learn.org/stable/_images/math/52f0968514f104b80866f12f688399f3203b7970.png" alt="w_2">, ..., <img class="math" src="http://scikit-learn.org/stable/_images/math/b17de5a4d8467dcb41c4473a76d24e747b859fae.png" alt="w_N"> to each of the training samples. Initially, those weights are all set to <img class="math" src="http://scikit-learn.org/stable/_images/math/5990533ea4dac587466ab3b7d7058d686cf4ddbd.png" alt="w_i = 1/N">, so that the first step simply trains a weak learner on the original data. For each successive iteration, the sample weights are individually modified and the learning algorithm is reapplied to the reweighted data. At a given step, those training examples that were incorrectly predicted by the boosted model induced at the previous step have their weights increased, whereas the weights are decreased for those that were predicted correctly. As iterations proceed, examples that are difficult to predict receive ever-increasing influence. Each subsequent weak learner is thereby forced to concentrate on the examples that are missed by the previous ones in the sequence <a class="reference internal" href="#htf" id="id9">[HTF]</a>.</p> <div class="figure align-center"> <a class="reference external image-reference" href="../../auto_examples/ensemble/plot_adaboost_hastie_10_2/"><img alt="../_images/sphx_glr_plot_adaboost_hastie_10_2_0011.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_adaboost_hastie_10_2_0011.png" style="width: 600.0px; height: 450.0px;"></a> </div> <p>AdaBoost can be used both for classification and regression problems:</p>  <ul class="simple"> <li>For multi-class classification, <a class="reference internal" href="../generated/sklearn.ensemble.adaboostclassifier/#sklearn.ensemble.AdaBoostClassifier" title="sklearn.ensemble.AdaBoostClassifier"><code>AdaBoostClassifier</code></a> implements AdaBoost-SAMME and AdaBoost-SAMME.R <a class="reference internal" href="#zzrh2009" id="id10">[ZZRH2009]</a>.</li> <li>For regression, <a class="reference internal" href="../generated/sklearn.ensemble.adaboostregressor/#sklearn.ensemble.AdaBoostRegressor" title="sklearn.ensemble.AdaBoostRegressor"><code>AdaBoostRegressor</code></a> implements AdaBoost.R2 <a class="reference internal" href="#d1997" id="id11">[D1997]</a>.</li> </ul>   <h3 id="usage">1.11.3.1. Usage</h3> <p>The following example shows how to fit an AdaBoost classifier with 100 weak learners:</p> <pre data-language="python">&gt;&gt;&gt; from sklearn.model_selection import cross_val_score
&gt;&gt;&gt; from sklearn.datasets import load_iris
&gt;&gt;&gt; from sklearn.ensemble import AdaBoostClassifier

&gt;&gt;&gt; iris = load_iris()
&gt;&gt;&gt; clf = AdaBoostClassifier(n_estimators=100)
&gt;&gt;&gt; scores = cross_val_score(clf, iris.data, iris.target)
&gt;&gt;&gt; scores.mean()                             
0.9...
</pre> <p>The number of weak learners is controlled by the parameter <code>n_estimators</code>. The <code>learning_rate</code> parameter controls the contribution of the weak learners in the final combination. By default, weak learners are decision stumps. Different weak learners can be specified through the <code>base_estimator</code> parameter. The main parameters to tune to obtain good results are <code>n_estimators</code> and the complexity of the base estimators (e.g., its depth <code>max_depth</code> or minimum required number of samples at a leaf <code>min_samples_leaf</code> in case of decision trees).</p> <div class="topic"> <p class="topic-title first">Examples:</p> <ul class="simple"> <li>
<a class="reference internal" href="../../auto_examples/ensemble/plot_adaboost_hastie_10_2/#sphx-glr-auto-examples-ensemble-plot-adaboost-hastie-10-2-py"><span class="std std-ref">Discrete versus Real AdaBoost</span></a> compares the classification error of a decision stump, decision tree, and a boosted decision stump using AdaBoost-SAMME and AdaBoost-SAMME.R.</li> <li>
<a class="reference internal" href="../../auto_examples/ensemble/plot_adaboost_multiclass/#sphx-glr-auto-examples-ensemble-plot-adaboost-multiclass-py"><span class="std std-ref">Multi-class AdaBoosted Decision Trees</span></a> shows the performance of AdaBoost-SAMME and AdaBoost-SAMME.R on a multi-class problem.</li> <li>
<a class="reference internal" href="../../auto_examples/ensemble/plot_adaboost_twoclass/#sphx-glr-auto-examples-ensemble-plot-adaboost-twoclass-py"><span class="std std-ref">Two-class AdaBoost</span></a> shows the decision boundary and decision function values for a non-linearly separable two-class problem using AdaBoost-SAMME.</li> <li>
<a class="reference internal" href="../../auto_examples/ensemble/plot_adaboost_regression/#sphx-glr-auto-examples-ensemble-plot-adaboost-regression-py"><span class="std std-ref">Decision Tree Regression with AdaBoost</span></a> demonstrates regression with the AdaBoost.R2 algorithm.</li> </ul> </div> <div class="topic"> <p class="topic-title first">References</p> <table class="docutils citation" frame="void" id="fs1995" rules="none">   <tr>
<td class="label"><a class="fn-backref" href="#id8">[FS1995]</a></td>
<td>Y. Freund, and R. Schapire, “A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting”, 1997.</td>
</tr>  </table> <table class="docutils citation" frame="void" id="zzrh2009" rules="none">   <tr>
<td class="label"><a class="fn-backref" href="#id10">[ZZRH2009]</a></td>
<td>J. Zhu, H. Zou, S. Rosset, T. Hastie. “Multi-class AdaBoost”, 2009.</td>
</tr>  </table> <table class="docutils citation" frame="void" id="d1997" rules="none">   <tr>
<td class="label"><a class="fn-backref" href="#id11">[D1997]</a></td>
<td>
<ol class="first last upperalpha simple" start="8"> <li>Drucker. “Improving Regressors using Boosting Techniques”, 1997.</li> </ol> </td>
</tr>  </table> <table class="docutils citation" frame="void" id="htf" rules="none">   <tr>
<td class="label"><a class="fn-backref" href="#id9">[HTF]</a></td>
<td>T. Hastie, R. Tibshirani and J. Friedman, “Elements of Statistical Learning Ed. 2”, Springer, 2009.</td>
</tr>  </table> </div>    <h2 id="gradient-boosting">1.11.4. Gradient Tree Boosting</h2> <p id="gradient-tree-boosting"><a class="reference external" href="https://en.wikipedia.org/wiki/Gradient_boosting" target="_blank">Gradient Tree Boosting</a> or Gradient Boosted Regression Trees (GBRT) is a generalization of boosting to arbitrary differentiable loss functions. GBRT is an accurate and effective off-the-shelf procedure that can be used for both regression and classification problems. Gradient Tree Boosting models are used in a variety of areas including Web search ranking and ecology.</p> <p>The advantages of GBRT are:</p>  <ul class="simple"> <li>Natural handling of data of mixed type (= heterogeneous features)</li> <li>Predictive power</li> <li>Robustness to outliers in output space (via robust loss functions)</li> </ul>  <p>The disadvantages of GBRT are:</p>  <ul class="simple"> <li>Scalability, due to the sequential nature of boosting it can hardly be parallelized.</li> </ul>  <p>The module <a class="reference internal" href="../classes/#module-sklearn.ensemble" title="sklearn.ensemble"><code>sklearn.ensemble</code></a> provides methods for both classification and regression via gradient boosted regression trees.</p>  <h3 id="classification">1.11.4.1. Classification</h3> <p><a class="reference internal" href="../generated/sklearn.ensemble.gradientboostingclassifier/#sklearn.ensemble.GradientBoostingClassifier" title="sklearn.ensemble.GradientBoostingClassifier"><code>GradientBoostingClassifier</code></a> supports both binary and multi-class classification. The following example shows how to fit a gradient boosting classifier with 100 decision stumps as weak learners:</p> <pre data-language="python">&gt;&gt;&gt; from sklearn.datasets import make_hastie_10_2
&gt;&gt;&gt; from sklearn.ensemble import GradientBoostingClassifier

&gt;&gt;&gt; X, y = make_hastie_10_2(random_state=0)
&gt;&gt;&gt; X_train, X_test = X[:2000], X[2000:]
&gt;&gt;&gt; y_train, y_test = y[:2000], y[2000:]

&gt;&gt;&gt; clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,
...     max_depth=1, random_state=0).fit(X_train, y_train)
&gt;&gt;&gt; clf.score(X_test, y_test)                 
0.913...
</pre> <p>The number of weak learners (i.e. regression trees) is controlled by the parameter <code>n_estimators</code>; <a class="reference internal" href="#gradient-boosting-tree-size"><span class="std std-ref">The size of each tree</span></a> can be controlled either by setting the tree depth via <code>max_depth</code> or by setting the number of leaf nodes via <code>max_leaf_nodes</code>. The <code>learning_rate</code> is a hyper-parameter in the range (0.0, 1.0] that controls overfitting via <a class="reference internal" href="#gradient-boosting-shrinkage"><span class="std std-ref">shrinkage</span></a> .</p> <div class="admonition note"> <p class="first admonition-title">Note</p> <p class="last">Classification with more than 2 classes requires the induction of <code>n_classes</code> regression trees at each iteration, thus, the total number of induced trees equals <code>n_classes * n_estimators</code>. For datasets with a large number of classes we strongly recommend to use <a class="reference internal" href="../generated/sklearn.ensemble.randomforestclassifier/#sklearn.ensemble.RandomForestClassifier" title="sklearn.ensemble.RandomForestClassifier"><code>RandomForestClassifier</code></a> as an alternative to <a class="reference internal" href="../generated/sklearn.ensemble.gradientboostingclassifier/#sklearn.ensemble.GradientBoostingClassifier" title="sklearn.ensemble.GradientBoostingClassifier"><code>GradientBoostingClassifier</code></a> .</p> </div>   <h3 id="regression">1.11.4.2. Regression</h3> <p><a class="reference internal" href="../generated/sklearn.ensemble.gradientboostingregressor/#sklearn.ensemble.GradientBoostingRegressor" title="sklearn.ensemble.GradientBoostingRegressor"><code>GradientBoostingRegressor</code></a> supports a number of <a class="reference internal" href="#gradient-boosting-loss"><span class="std std-ref">different loss functions</span></a> for regression which can be specified via the argument <code>loss</code>; the default loss function for regression is least squares (<code>'ls'</code>).</p> <pre data-language="python">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from sklearn.metrics import mean_squared_error
&gt;&gt;&gt; from sklearn.datasets import make_friedman1
&gt;&gt;&gt; from sklearn.ensemble import GradientBoostingRegressor

&gt;&gt;&gt; X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)
&gt;&gt;&gt; X_train, X_test = X[:200], X[200:]
&gt;&gt;&gt; y_train, y_test = y[:200], y[200:]
&gt;&gt;&gt; est = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,
...     max_depth=1, random_state=0, loss='ls').fit(X_train, y_train)
&gt;&gt;&gt; mean_squared_error(y_test, est.predict(X_test))    
5.00...
</pre> <p>The figure below shows the results of applying <a class="reference internal" href="../generated/sklearn.ensemble.gradientboostingregressor/#sklearn.ensemble.GradientBoostingRegressor" title="sklearn.ensemble.GradientBoostingRegressor"><code>GradientBoostingRegressor</code></a> with least squares loss and 500 base learners to the Boston house price dataset (<a class="reference internal" href="../generated/sklearn.datasets.load_boston/#sklearn.datasets.load_boston" title="sklearn.datasets.load_boston"><code>sklearn.datasets.load_boston</code></a>). The plot on the left shows the train and test error at each iteration. The train error at each iteration is stored in the <code>train_score_</code> attribute of the gradient boosting model. The test error at each iterations can be obtained via the <a class="reference internal" href="../generated/sklearn.ensemble.gradientboostingregressor/#sklearn.ensemble.GradientBoostingRegressor.staged_predict" title="sklearn.ensemble.GradientBoostingRegressor.staged_predict"><code>staged_predict</code></a> method which returns a generator that yields the predictions at each stage. Plots like these can be used to determine the optimal number of trees (i.e. <code>n_estimators</code>) by early stopping. The plot on the right shows the feature importances which can be obtained via the <code>feature_importances_</code> property.</p> <div class="figure align-center"> <a class="reference external image-reference" href="../../auto_examples/ensemble/plot_gradient_boosting_regression/"><img alt="../_images/sphx_glr_plot_gradient_boosting_regression_0011.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_gradient_boosting_regression_0011.png" style="width: 900.0px; height: 450.0px;"></a> </div> <div class="topic"> <p class="topic-title first">Examples:</p> <ul class="simple"> <li><a class="reference internal" href="../../auto_examples/ensemble/plot_gradient_boosting_regression/#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-regression-py"><span class="std std-ref">Gradient Boosting regression</span></a></li> <li><a class="reference internal" href="../../auto_examples/ensemble/plot_gradient_boosting_oob/#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-oob-py"><span class="std std-ref">Gradient Boosting Out-of-Bag estimates</span></a></li> </ul> </div>   <h3 id="gradient-boosting-warm-start">1.11.4.3. Fitting additional weak-learners</h3> <p id="fitting-additional-weak-learners">Both <a class="reference internal" href="../generated/sklearn.ensemble.gradientboostingregressor/#sklearn.ensemble.GradientBoostingRegressor" title="sklearn.ensemble.GradientBoostingRegressor"><code>GradientBoostingRegressor</code></a> and <a class="reference internal" href="../generated/sklearn.ensemble.gradientboostingclassifier/#sklearn.ensemble.GradientBoostingClassifier" title="sklearn.ensemble.GradientBoostingClassifier"><code>GradientBoostingClassifier</code></a> support <code>warm_start=True</code> which allows you to add more estimators to an already fitted model.</p> <pre data-language="python">&gt;&gt;&gt; _ = est.set_params(n_estimators=200, warm_start=True)  # set warm_start and new nr of trees
&gt;&gt;&gt; _ = est.fit(X_train, y_train) # fit additional 100 trees to est
&gt;&gt;&gt; mean_squared_error(y_test, est.predict(X_test))    
3.84...
</pre>   <h3 id="gradient-boosting-tree-size">1.11.4.4. Controlling the tree size</h3> <p id="controlling-the-tree-size">The size of the regression tree base learners defines the level of variable interactions that can be captured by the gradient boosting model. In general, a tree of depth <code>h</code> can capture interactions of order <code>h</code> . There are two ways in which the size of the individual regression trees can be controlled.</p> <p>If you specify <code>max_depth=h</code> then complete binary trees of depth <code>h</code> will be grown. Such trees will have (at most) <code>2**h</code> leaf nodes and <code>2**h - 1</code> split nodes.</p> <p>Alternatively, you can control the tree size by specifying the number of leaf nodes via the parameter <code>max_leaf_nodes</code>. In this case, trees will be grown using best-first search where nodes with the highest improvement in impurity will be expanded first. A tree with <code>max_leaf_nodes=k</code> has <code>k - 1</code> split nodes and thus can model interactions of up to order <code>max_leaf_nodes - 1</code> .</p> <p>We found that <code>max_leaf_nodes=k</code> gives comparable results to <code>max_depth=k-1</code> but is significantly faster to train at the expense of a slightly higher training error. The parameter <code>max_leaf_nodes</code> corresponds to the variable <code>J</code> in the chapter on gradient boosting in <a class="reference internal" href="#f2001" id="id13">[F2001]</a> and is related to the parameter <code>interaction.depth</code> in R’s gbm package where <code>max_leaf_nodes == interaction.depth + 1</code> .</p>   <h3 id="mathematical-formulation">1.11.4.5. Mathematical formulation</h3> <p>GBRT considers additive models of the following form:</p>  <div class="math"> <p><img src="http://scikit-learn.org/stable/_images/math/958a81a72d1492a36a82531c9820186392bde5c8.png" alt="F(x) = \sum_{m=1}^{M} \gamma_m h_m(x)"></p> </div> <p>where <img class="math" src="http://scikit-learn.org/stable/_images/math/bb052ebc51285274e7775f81c0ff94d67f7e3a6c.png" alt="h_m(x)"> are the basis functions which are usually called <em>weak learners</em> in the context of boosting. Gradient Tree Boosting uses <a class="reference internal" href="../tree/#tree"><span class="std std-ref">decision trees</span></a> of fixed size as weak learners. Decision trees have a number of abilities that make them valuable for boosting, namely the ability to handle data of mixed type and the ability to model complex functions.</p> <p>Similar to other boosting algorithms GBRT builds the additive model in a forward stagewise fashion:</p>  <div class="math"> <p><img src="http://scikit-learn.org/stable/_images/math/7b0b481e5f2c318df259c4a1021745d07f19b8d8.png" alt="F_m(x) = F_{m-1}(x) + \gamma_m h_m(x)"></p> </div> <p>At each stage the decision tree <img class="math" src="http://scikit-learn.org/stable/_images/math/bb052ebc51285274e7775f81c0ff94d67f7e3a6c.png" alt="h_m(x)"> is chosen to minimize the loss function <img class="math" src="http://scikit-learn.org/stable/_images/math/ae2b750f71e1fc0daaa3de9a85d42794d7cd1326.png" alt="L"> given the current model <img class="math" src="http://scikit-learn.org/stable/_images/math/ae9df12c10481b6e843e2b8a0e1058dc40d356e5.png" alt="F_{m-1}"> and its fit <img class="math" src="http://scikit-learn.org/stable/_images/math/aaa0095d8e9f1d72aba0a1b5871ae4f4f6d35ccc.png" alt="F_{m-1}(x_i)"></p>  <div class="math"> <p><img src="http://scikit-learn.org/stable/_images/math/9aeabf91d2d9c30ca45c2618f51e8d8c82ba4d77.png" alt="F_m(x) = F_{m-1}(x) + \arg\min_{h} \sum_{i=1}^{n} L(y_i,
F_{m-1}(x_i) - h(x))"></p> </div> <p>The initial model <img class="math" src="http://scikit-learn.org/stable/_images/math/31a04d17cd387e3b19d104a29ba2bd159f42eccb.png" alt="F_{0}"> is problem specific, for least-squares regression one usually chooses the mean of the target values.</p> <div class="admonition note"> <p class="first admonition-title">Note</p> <p class="last">The initial model can also be specified via the <code>init</code> argument. The passed object has to implement <code>fit</code> and <code>predict</code>.</p> </div> <p>Gradient Boosting attempts to solve this minimization problem numerically via steepest descent: The steepest descent direction is the negative gradient of the loss function evaluated at the current model <img class="math" src="http://scikit-learn.org/stable/_images/math/ae9df12c10481b6e843e2b8a0e1058dc40d356e5.png" alt="F_{m-1}"> which can be calculated for any differentiable loss function:</p>  <div class="math"> <p><img src="http://scikit-learn.org/stable/_images/math/eac4fdf64ca91da58bfcbda90a05067118d52429.png" alt="F_m(x) = F_{m-1}(x) + \gamma_m \sum_{i=1}^{n} \nabla_F L(y_i,
F_{m-1}(x_i))"></p> </div> <p>Where the step length <img class="math" src="http://scikit-learn.org/stable/_images/math/ab2418d0fe06e2097d0d32cece8f0ab21b9227b4.png" alt="\gamma_m"> is chosen using line search:</p>  <div class="math"> <p><img src="http://scikit-learn.org/stable/_images/math/fe9f5a6b17d269a906e91f358e1118bb4838dcf0.png" alt="\gamma_m = \arg\min_{\gamma} \sum_{i=1}^{n} L(y_i, F_{m-1}(x_i)
- \gamma \frac{\partial L(y_i, F_{m-1}(x_i))}{\partial F_{m-1}(x_i)})"></p> </div> <p>The algorithms for regression and classification only differ in the concrete loss function used.</p>  <h4 id="gradient-boosting-loss">1.11.4.5.1. Loss Functions</h4> <p id="loss-functions">The following loss functions are supported and can be specified using the parameter <code>loss</code>:</p>  <ul class="simple"> <li>Regression<ul> <li>Least squares (<code>'ls'</code>): The natural choice for regression due to its superior computational properties. The initial model is given by the mean of the target values.</li> <li>Least absolute deviation (<code>'lad'</code>): A robust loss function for regression. The initial model is given by the median of the target values.</li> <li>Huber (<code>'huber'</code>): Another robust loss function that combines least squares and least absolute deviation; use <code>alpha</code> to control the sensitivity with regards to outliers (see <a class="reference internal" href="#f2001" id="id14">[F2001]</a> for more details).</li> <li>Quantile (<code>'quantile'</code>): A loss function for quantile regression. Use <code>0 &lt; alpha &lt; 1</code> to specify the quantile. This loss function can be used to create prediction intervals (see <a class="reference internal" href="../../auto_examples/ensemble/plot_gradient_boosting_quantile/#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-quantile-py"><span class="std std-ref">Prediction Intervals for Gradient Boosting Regression</span></a>).</li> </ul> </li> <li>Classification<ul> <li>Binomial deviance (<code>'deviance'</code>): The negative binomial log-likelihood loss function for binary classification (provides probability estimates). The initial model is given by the log odds-ratio.</li> <li>Multinomial deviance (<code>'deviance'</code>): The negative multinomial log-likelihood loss function for multi-class classification with <code>n_classes</code> mutually exclusive classes. It provides probability estimates. The initial model is given by the prior probability of each class. At each iteration <code>n_classes</code> regression trees have to be constructed which makes GBRT rather inefficient for data sets with a large number of classes.</li> <li>Exponential loss (<code>'exponential'</code>): The same loss function as <a class="reference internal" href="../generated/sklearn.ensemble.adaboostclassifier/#sklearn.ensemble.AdaBoostClassifier" title="sklearn.ensemble.AdaBoostClassifier"><code>AdaBoostClassifier</code></a>. Less robust to mislabeled examples than <code>'deviance'</code>; can only be used for binary classification.</li> </ul> </li> </ul>     <h3 id="regularization">1.11.4.6. Regularization</h3>  <h4 id="gradient-boosting-shrinkage">1.11.4.6.1. Shrinkage</h4> <p id="shrinkage"><a class="reference internal" href="#f2001" id="id15">[F2001]</a> proposed a simple regularization strategy that scales the contribution of each weak learner by a factor <img class="math" src="http://scikit-learn.org/stable/_images/math/ca3b8fa4180eee2dfc3af9d13fae1da451cd2c31.png" alt="\nu">:</p> <div class="math"> <p><img src="http://scikit-learn.org/stable/_images/math/4982f3c33c9ccb4b45557d8332d3e5aa754cb229.png" alt="F_m(x) = F_{m-1}(x) + \nu \gamma_m h_m(x)"></p> </div>
<p>The parameter <img class="math" src="http://scikit-learn.org/stable/_images/math/ca3b8fa4180eee2dfc3af9d13fae1da451cd2c31.png" alt="\nu"> is also called the <strong>learning rate</strong> because it scales the step length the gradient descent procedure; it can be set via the <code>learning_rate</code> parameter.</p> <p>The parameter <code>learning_rate</code> strongly interacts with the parameter <code>n_estimators</code>, the number of weak learners to fit. Smaller values of <code>learning_rate</code> require larger numbers of weak learners to maintain a constant training error. Empirical evidence suggests that small values of <code>learning_rate</code> favor better test error. <a class="reference internal" href="#htf2009" id="id16">[HTF2009]</a> recommend to set the learning rate to a small constant (e.g. <code>learning_rate &lt;= 0.1</code>) and choose <code>n_estimators</code> by early stopping. For a more detailed discussion of the interaction between <code>learning_rate</code> and <code>n_estimators</code> see <a class="reference internal" href="#r2007" id="id17">[R2007]</a>.</p>   <h4 id="subsampling">1.11.4.6.2. Subsampling</h4> <p><a class="reference internal" href="#f1999" id="id18">[F1999]</a> proposed stochastic gradient boosting, which combines gradient boosting with bootstrap averaging (bagging). At each iteration the base classifier is trained on a fraction <code>subsample</code> of the available training data. The subsample is drawn without replacement. A typical value of <code>subsample</code> is 0.5.</p> <p>The figure below illustrates the effect of shrinkage and subsampling on the goodness-of-fit of the model. We can clearly see that shrinkage outperforms no-shrinkage. Subsampling with shrinkage can further increase the accuracy of the model. Subsampling without shrinkage, on the other hand, does poorly.</p> <div class="figure align-center"> <a class="reference external image-reference" href="../../auto_examples/ensemble/plot_gradient_boosting_regularization/"><img alt="../_images/sphx_glr_plot_gradient_boosting_regularization_0011.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_gradient_boosting_regularization_0011.png" style="width: 600.0px; height: 450.0px;"></a> </div> <p>Another strategy to reduce the variance is by subsampling the features analogous to the random splits in <a class="reference internal" href="../generated/sklearn.ensemble.randomforestclassifier/#sklearn.ensemble.RandomForestClassifier" title="sklearn.ensemble.RandomForestClassifier"><code>RandomForestClassifier</code></a> . The number of subsampled features can be controlled via the <code>max_features</code> parameter.</p> <div class="admonition note"> <p class="first admonition-title">Note</p> <p class="last">Using a small <code>max_features</code> value can significantly decrease the runtime.</p> </div> <p>Stochastic gradient boosting allows to compute out-of-bag estimates of the test deviance by computing the improvement in deviance on the examples that are not included in the bootstrap sample (i.e. the out-of-bag examples). The improvements are stored in the attribute <code>oob_improvement_</code>. <code>oob_improvement_[i]</code> holds the improvement in terms of the loss on the OOB samples if you add the i-th stage to the current predictions. Out-of-bag estimates can be used for model selection, for example to determine the optimal number of iterations. OOB estimates are usually very pessimistic thus we recommend to use cross-validation instead and only use OOB if cross-validation is too time consuming.</p> <div class="topic"> <p class="topic-title first">Examples:</p> <ul class="simple"> <li><a class="reference internal" href="../../auto_examples/ensemble/plot_gradient_boosting_regularization/#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-regularization-py"><span class="std std-ref">Gradient Boosting regularization</span></a></li> <li><a class="reference internal" href="../../auto_examples/ensemble/plot_gradient_boosting_oob/#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-oob-py"><span class="std std-ref">Gradient Boosting Out-of-Bag estimates</span></a></li> <li><a class="reference internal" href="../../auto_examples/ensemble/plot_ensemble_oob/#sphx-glr-auto-examples-ensemble-plot-ensemble-oob-py"><span class="std std-ref">OOB Errors for Random Forests</span></a></li> </ul> </div>    <h3 id="interpretation">1.11.4.7. Interpretation</h3> <p>Individual decision trees can be interpreted easily by simply visualizing the tree structure. Gradient boosting models, however, comprise hundreds of regression trees thus they cannot be easily interpreted by visual inspection of the individual trees. Fortunately, a number of techniques have been proposed to summarize and interpret gradient boosting models.</p>  <h4 id="feature-importance">1.11.4.7.1. Feature importance</h4> <p>Often features do not contribute equally to predict the target response; in many situations the majority of the features are in fact irrelevant. When interpreting a model, the first question usually is: what are those important features and how do they contributing in predicting the target response?</p> <p>Individual decision trees intrinsically perform feature selection by selecting appropriate split points. This information can be used to measure the importance of each feature; the basic idea is: the more often a feature is used in the split points of a tree the more important that feature is. This notion of importance can be extended to decision tree ensembles by simply averaging the feature importance of each tree (see <a class="reference internal" href="#random-forest-feature-importance"><span class="std std-ref">Feature importance evaluation</span></a> for more details).</p> <p>The feature importance scores of a fit gradient boosting model can be accessed via the <code>feature_importances_</code> property:</p> <pre data-language="python">&gt;&gt;&gt; from sklearn.datasets import make_hastie_10_2
&gt;&gt;&gt; from sklearn.ensemble import GradientBoostingClassifier

&gt;&gt;&gt; X, y = make_hastie_10_2(random_state=0)
&gt;&gt;&gt; clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,
...     max_depth=1, random_state=0).fit(X, y)
&gt;&gt;&gt; clf.feature_importances_  
array([ 0.11,  0.1 ,  0.11,  ...
</pre> <div class="topic"> <p class="topic-title first">Examples:</p> <ul class="simple"> <li><a class="reference internal" href="../../auto_examples/ensemble/plot_gradient_boosting_regression/#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-regression-py"><span class="std std-ref">Gradient Boosting regression</span></a></li> </ul> </div>   <h4 id="id19">1.11.4.7.2. Partial dependence</h4> <p id="partial-dependence">Partial dependence plots (PDP) show the dependence between the target response and a set of ‘target’ features, marginalizing over the values of all other features (the ‘complement’ features). Intuitively, we can interpret the partial dependence as the expected target response <a class="footnote-reference" href="#id22" id="id20">[1]</a> as a function of the ‘target’ features <a class="footnote-reference" href="#id23" id="id21">[2]</a>.</p> <p>Due to the limits of human perception the size of the target feature set must be small (usually, one or two) thus the target features are usually chosen among the most important features.</p> <p>The Figure below shows four one-way and one two-way partial dependence plots for the California housing dataset:</p> <div class="figure align-center"> <a class="reference external image-reference" href="../../auto_examples/ensemble/plot_partial_dependence/"><img alt="../_images/sphx_glr_plot_partial_dependence_0011.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_partial_dependence_0011.png" style="width: 560.0px; height: 420.0px;"></a> </div> <p>One-way PDPs tell us about the interaction between the target response and the target feature (e.g. linear, non-linear). The upper left plot in the above Figure shows the effect of the median income in a district on the median house price; we can clearly see a linear relationship among them.</p> <p>PDPs with two target features show the interactions among the two features. For example, the two-variable PDP in the above Figure shows the dependence of median house price on joint values of house age and avg. occupants per household. We can clearly see an interaction between the two features: For an avg. occupancy greater than two, the house price is nearly independent of the house age, whereas for values less than two there is a strong dependence on age.</p> <p>The module <code>partial_dependence</code> provides a convenience function <a class="reference internal" href="../generated/sklearn.ensemble.partial_dependence.plot_partial_dependence/#sklearn.ensemble.partial_dependence.plot_partial_dependence" title="sklearn.ensemble.partial_dependence.plot_partial_dependence"><code>plot_partial_dependence</code></a> to create one-way and two-way partial dependence plots. In the below example we show how to create a grid of partial dependence plots: two one-way PDPs for the features <code>0</code> and <code>1</code> and a two-way PDP between the two features:</p> <pre data-language="python">&gt;&gt;&gt; from sklearn.datasets import make_hastie_10_2
&gt;&gt;&gt; from sklearn.ensemble import GradientBoostingClassifier
&gt;&gt;&gt; from sklearn.ensemble.partial_dependence import plot_partial_dependence

&gt;&gt;&gt; X, y = make_hastie_10_2(random_state=0)
&gt;&gt;&gt; clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,
...     max_depth=1, random_state=0).fit(X, y)
&gt;&gt;&gt; features = [0, 1, (0, 1)]
&gt;&gt;&gt; fig, axs = plot_partial_dependence(clf, X, features) 
</pre> <p>For multi-class models, you need to set the class label for which the PDPs should be created via the <code>label</code> argument:</p> <pre data-language="python">&gt;&gt;&gt; from sklearn.datasets import load_iris
&gt;&gt;&gt; iris = load_iris()
&gt;&gt;&gt; mc_clf = GradientBoostingClassifier(n_estimators=10,
...     max_depth=1).fit(iris.data, iris.target)
&gt;&gt;&gt; features = [3, 2, (3, 2)]
&gt;&gt;&gt; fig, axs = plot_partial_dependence(mc_clf, X, features, label=0) 
</pre> <p>If you need the raw values of the partial dependence function rather than the plots you can use the <a class="reference internal" href="../generated/sklearn.ensemble.partial_dependence.partial_dependence/#sklearn.ensemble.partial_dependence.partial_dependence" title="sklearn.ensemble.partial_dependence.partial_dependence"><code>partial_dependence</code></a> function:</p> <pre data-language="python">&gt;&gt;&gt; from sklearn.ensemble.partial_dependence import partial_dependence

&gt;&gt;&gt; pdp, axes = partial_dependence(clf, [0], X=X)
&gt;&gt;&gt; pdp  
array([[ 2.46643157,  2.46643157, ...
&gt;&gt;&gt; axes  
[array([-1.62497054, -1.59201391, ...
</pre> <p>The function requires either the argument <code>grid</code> which specifies the values of the target features on which the partial dependence function should be evaluated or the argument <code>X</code> which is a convenience mode for automatically creating <code>grid</code> from the training data. If <code>X</code> is given, the <code>axes</code> value returned by the function gives the axis for each target feature.</p> <p>For each value of the ‘target’ features in the <code>grid</code> the partial dependence function need to marginalize the predictions of a tree over all possible values of the ‘complement’ features. In decision trees this function can be evaluated efficiently without reference to the training data. For each grid point a weighted tree traversal is performed: if a split node involves a ‘target’ feature, the corresponding left or right branch is followed, otherwise both branches are followed, each branch is weighted by the fraction of training samples that entered that branch. Finally, the partial dependence is given by a weighted average of all visited leaves. For tree ensembles the results of each individual tree are again averaged.</p> <h4 class="rubric">Footnotes</h4> <table class="docutils footnote" frame="void" id="id22" rules="none">   <tr>
<td class="label"><a class="fn-backref" href="#id20">[1]</a></td>
<td>For classification with <code>loss='deviance'</code> the target response is logit(p).</td>
</tr>  </table> <table class="docutils footnote" frame="void" id="id23" rules="none">   <tr>
<td class="label"><a class="fn-backref" href="#id21">[2]</a></td>
<td>More precisely its the expectation of the target response after accounting for the initial model; partial dependence plots do not include the <code>init</code> model.</td>
</tr>  </table> <div class="topic"> <p class="topic-title first">Examples:</p> <ul class="simple"> <li><a class="reference internal" href="../../auto_examples/ensemble/plot_partial_dependence/#sphx-glr-auto-examples-ensemble-plot-partial-dependence-py"><span class="std std-ref">Partial Dependence Plots</span></a></li> </ul> </div> <div class="topic"> <p class="topic-title first">References</p> <table class="docutils citation" frame="void" id="f2001" rules="none">   <tr>
<td class="label">[F2001]</td>
<td>
<em>(<a class="fn-backref" href="#id13">1</a>, <a class="fn-backref" href="#id14">2</a>, <a class="fn-backref" href="#id15">3</a>)</em> J. Friedman, “Greedy Function Approximation: A Gradient Boosting Machine”, The Annals of Statistics, Vol. 29, No. 5, 2001.</td>
</tr>  </table> <table class="docutils citation" frame="void" id="f1999" rules="none">   <tr>
<td class="label"><a class="fn-backref" href="#id18">[F1999]</a></td>
<td>
<ol class="first last upperalpha simple" start="10"> <li>Friedman, “Stochastic Gradient Boosting”, 1999</li> </ol> </td>
</tr>  </table> <table class="docutils citation" frame="void" id="htf2009" rules="none">   <tr>
<td class="label"><a class="fn-backref" href="#id16">[HTF2009]</a></td>
<td>
<ol class="first last upperalpha simple" start="20"> <li>Hastie, R. Tibshirani and J. Friedman, “Elements of Statistical Learning Ed. 2”, Springer, 2009.</li> </ol> </td>
</tr>  </table> <table class="docutils citation" frame="void" id="r2007" rules="none">   <tr>
<td class="label"><a class="fn-backref" href="#id17">[R2007]</a></td>
<td>
<ol class="first last upperalpha simple" start="7"> <li>Ridgeway, “Generalized Boosted Models: A guide to the gbm package”, 2007</li> </ol> </td>
</tr>  </table> </div>     <h2 id="voting-classifier">1.11.5. VotingClassifier</h2> <p id="votingclassifier">The idea behind the voting classifier implementation is to combine conceptually different machine learning classifiers and use a majority vote or the average predicted probabilities (soft vote) to predict the class labels. Such a classifier can be useful for a set of equally well performing model in order to balance out their individual weaknesses.</p>  <h3 id="majority-class-labels-majority-hard-voting">1.11.5.1. Majority Class Labels (Majority/Hard Voting)</h3> <p>In majority voting, the predicted class label for a particular sample is the class label that represents the majority (mode) of the class labels predicted by each individual classifier.</p> <p>E.g., if the prediction for a given sample is</p> <ul class="simple"> <li>classifier 1 -&gt; class 1</li> <li>classifier 2 -&gt; class 1</li> <li>classifier 3 -&gt; class 2</li> </ul> <p>the VotingClassifier (with <code>voting='hard'</code>) would classify the sample as “class 1” based on the majority class label.</p> <p>In the cases of a tie, the <code>VotingClassifier</code> will select the class based on the ascending sort order. E.g., in the following scenario</p> <ul class="simple"> <li>classifier 1 -&gt; class 2</li> <li>classifier 2 -&gt; class 1</li> </ul> <p>the class label 1 will be assigned to the sample.</p>  <h4 id="id24">1.11.5.1.1. Usage</h4> <p>The following example shows how to fit the majority rule classifier:</p> <pre data-language="python">&gt;&gt;&gt; from sklearn import datasets
&gt;&gt;&gt; from sklearn.model_selection import cross_val_score
&gt;&gt;&gt; from sklearn.linear_model import LogisticRegression
&gt;&gt;&gt; from sklearn.naive_bayes import GaussianNB
&gt;&gt;&gt; from sklearn.ensemble import RandomForestClassifier
&gt;&gt;&gt; from sklearn.ensemble import VotingClassifier

&gt;&gt;&gt; iris = datasets.load_iris()
&gt;&gt;&gt; X, y = iris.data[:, 1:3], iris.target

&gt;&gt;&gt; clf1 = LogisticRegression(random_state=1)
&gt;&gt;&gt; clf2 = RandomForestClassifier(random_state=1)
&gt;&gt;&gt; clf3 = GaussianNB()

&gt;&gt;&gt; eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')

&gt;&gt;&gt; for clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensemble']):
...     scores = cross_val_score(clf, X, y, cv=5, scoring='accuracy')
...     print("Accuracy: %0.2f (+/- %0.2f) [%s]" % (scores.mean(), scores.std(), label))
Accuracy: 0.90 (+/- 0.05) [Logistic Regression]
Accuracy: 0.93 (+/- 0.05) [Random Forest]
Accuracy: 0.91 (+/- 0.04) [naive Bayes]
Accuracy: 0.95 (+/- 0.05) [Ensemble]
</pre>    <h3 id="weighted-average-probabilities-soft-voting">1.11.5.2. Weighted Average Probabilities (Soft Voting)</h3> <p>In contrast to majority voting (hard voting), soft voting returns the class label as argmax of the sum of predicted probabilities.</p> <p>Specific weights can be assigned to each classifier via the <code>weights</code> parameter. When weights are provided, the predicted class probabilities for each classifier are collected, multiplied by the classifier weight, and averaged. The final class label is then derived from the class label with the highest average probability.</p> <p>To illustrate this with a simple example, let’s assume we have 3 classifiers and a 3-class classification problems where we assign equal weights to all classifiers: w1=1, w2=1, w3=1.</p> <p>The weighted average probabilities for a sample would then be calculated as follows:</p> <table class="docutils">  <thead valign="bottom"> <tr class="row-odd">
<th class="head">classifier</th> <th class="head">class 1</th> <th class="head">class 2</th> <th class="head">class 3</th> </tr> </thead>  <tr class="row-even">
<td>classifier 1</td> <td>w1 * 0.2</td> <td>w1 * 0.5</td> <td>w1 * 0.3</td> </tr> <tr class="row-odd">
<td>classifier 2</td> <td>w2 * 0.6</td> <td>w2 * 0.3</td> <td>w2 * 0.1</td> </tr> <tr class="row-even">
<td>classifier 3</td> <td>w3 * 0.3</td> <td>w3 * 0.4</td> <td>w3 * 0.3</td> </tr> <tr class="row-odd">
<td>weighted average</td> <td>0.37</td> <td>0.4</td> <td>0.23</td> </tr>  </table> <p>Here, the predicted class label is 2, since it has the highest average probability.</p> <p>The following example illustrates how the decision regions may change when a soft <code>VotingClassifier</code> is used based on an linear Support Vector Machine, a Decision Tree, and a K-nearest neighbor classifier:</p> <pre data-language="python">&gt;&gt;&gt; from sklearn import datasets
&gt;&gt;&gt; from sklearn.tree import DecisionTreeClassifier
&gt;&gt;&gt; from sklearn.neighbors import KNeighborsClassifier
&gt;&gt;&gt; from sklearn.svm import SVC
&gt;&gt;&gt; from itertools import product
&gt;&gt;&gt; from sklearn.ensemble import VotingClassifier

&gt;&gt;&gt; # Loading some example data
&gt;&gt;&gt; iris = datasets.load_iris()
&gt;&gt;&gt; X = iris.data[:, [0,2]]
&gt;&gt;&gt; y = iris.target

&gt;&gt;&gt; # Training classifiers
&gt;&gt;&gt; clf1 = DecisionTreeClassifier(max_depth=4)
&gt;&gt;&gt; clf2 = KNeighborsClassifier(n_neighbors=7)
&gt;&gt;&gt; clf3 = SVC(kernel='rbf', probability=True)
&gt;&gt;&gt; eclf = VotingClassifier(estimators=[('dt', clf1), ('knn', clf2), ('svc', clf3)], voting='soft', weights=[2,1,2])

&gt;&gt;&gt; clf1 = clf1.fit(X,y)
&gt;&gt;&gt; clf2 = clf2.fit(X,y)
&gt;&gt;&gt; clf3 = clf3.fit(X,y)
&gt;&gt;&gt; eclf = eclf.fit(X,y)
</pre> <div class="figure align-center"> <a class="reference external image-reference" href="../../auto_examples/ensemble/plot_voting_decision_regions/"><img alt="../_images/sphx_glr_plot_voting_decision_regions_0011.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_voting_decision_regions_0011.png" style="width: 750.0px; height: 600.0px;"></a> </div>   <h3 id="using-the-votingclassifier-with-gridsearch">1.11.5.3. Using the <code>VotingClassifier</code> with <code>GridSearch</code>
</h3> <p>The <code>VotingClassifier</code> can also be used together with <code>GridSearch</code> in order to tune the hyperparameters of the individual estimators:</p> <pre data-language="python">&gt;&gt;&gt; from sklearn.model_selection import GridSearchCV
&gt;&gt;&gt; clf1 = LogisticRegression(random_state=1)
&gt;&gt;&gt; clf2 = RandomForestClassifier(random_state=1)
&gt;&gt;&gt; clf3 = GaussianNB()
&gt;&gt;&gt; eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='soft')

&gt;&gt;&gt; params = {'lr__C': [1.0, 100.0], 'rf__n_estimators': [20, 200],}

&gt;&gt;&gt; grid = GridSearchCV(estimator=eclf, param_grid=params, cv=5)
&gt;&gt;&gt; grid = grid.fit(iris.data, iris.target)
</pre>  <h4 id="id25">1.11.5.3.1. Usage</h4> <p>In order to predict the class labels based on the predicted class-probabilities (scikit-learn estimators in the VotingClassifier must support <code>predict_proba</code> method):</p> <pre data-language="python">&gt;&gt;&gt; eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='soft')
</pre> <p>Optionally, weights can be provided for the individual classifiers:</p> <pre data-language="python">&gt;&gt;&gt; eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='soft', weights=[2,5,1])
</pre>
<div class="_attribution">
  <p class="_attribution-p">
    © 2007–2016 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="http://scikit-learn.org/stable/modules/ensemble.html" class="_attribution-link" target="_blank">http://scikit-learn.org/stable/modules/ensemble.html</a>
  </p>
</div>

			</div>
		</div>
	</section>

	</div>
</body>
</html>
