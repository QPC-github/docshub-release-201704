
<!DOCTYPE HTML>

<html lang="en">

<head>
  <meta charset="utf-8">
  <title>1.7. Gaussian Processes - Scikit-learn - W3cubDocs</title>
  
  <meta name="description" content=" Gaussian Processes (GP) are a generic supervised learning method designed to solve regression and probabilistic classification problems. ">
  <meta name="keywords" content="gaussian, processes, -, scikit-learn, scikit_learn">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="http://docs.w3cub.com/scikit_learn/modules/gaussian_process/">
  <link href="/favicon.png" rel="icon">
  <link type="text/css" rel="stylesheet" href="/assets/application-50364fff564ce3b6327021805f3f00e2957b441cf27f576a7dd4ff63bbc47047.css">
  <script type="text/javascript" src="/assets/application-db64bfd54ceb42be11af7995804cf4902548419ceb79d509b0b7d62c22d98e6f.js"></script>
  <script src="/json/scikit_learn.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body>
	<div class="_app">
	<header class="_header">
  
  <form class="_search">
    <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
    <a class="_search-clear"></a>
    <div class="_search-tag"></div>
  </form>
  
  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/scikit_learn/" class="_nav-link" title="" style="margin-left:0;">scikit-learn</a></span>
  
  <nav class="_nav">
    <a href="/app/" class="_nav-link ">App</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		<div class="_list">
			
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<div class="_page _sphinx">
				
<h1 id="gaussian-process">1.7. Gaussian Processes</h1> <p id="gaussian-processes"><strong>Gaussian Processes (GP)</strong> are a generic supervised learning method designed to solve <em>regression</em> and <em>probabilistic classification</em> problems.</p> <p>The advantages of Gaussian processes are:</p>  <ul class="simple"> <li>The prediction interpolates the observations (at least for regular kernels).</li> <li>The prediction is probabilistic (Gaussian) so that one can compute empirical confidence intervals and decide based on those if one should refit (online fitting, adaptive fitting) the prediction in some region of interest.</li> <li>Versatile: different <a class="reference internal" href="#gp-kernels"><span class="std std-ref">kernels</span></a> can be specified. Common kernels are provided, but it is also possible to specify custom kernels.</li> </ul>  <p>The disadvantages of Gaussian processes include:</p>  <ul class="simple"> <li>They are not sparse, i.e., they use the whole samples/features information to perform the prediction.</li> <li>They lose efficiency in high dimensional spaces – namely when the number of features exceeds a few dozens.</li> </ul>   <h2 id="gpr">1.7.1. Gaussian Process Regression (GPR)</h2> <p id="gaussian-process-regression-gpr">The <a class="reference internal" href="../generated/sklearn.gaussian_process.gaussianprocessregressor/#sklearn.gaussian_process.GaussianProcessRegressor" title="sklearn.gaussian_process.GaussianProcessRegressor"><code>GaussianProcessRegressor</code></a> implements Gaussian processes (GP) for regression purposes. For this, the prior of the GP needs to be specified. The prior mean is assumed to be constant and zero (for <code>normalize_y=False</code>) or the training data’s mean (for <code>normalize_y=True</code>). The prior’s covariance is specified by a passing a <a class="reference internal" href="#gp-kernels"><span class="std std-ref">kernel</span></a> object. The hyperparameters of the kernel are optimized during fitting of GaussianProcessRegressor by maximizing the log-marginal-likelihood (LML) based on the passed <code>optimizer</code>. As the LML may have multiple local optima, the optimizer can be started repeatedly by specifying <code>n_restarts_optimizer</code>. The first run is always conducted starting from the initial hyperparameter values of the kernel; subsequent runs are conducted from hyperparameter values that have been chosen randomly from the range of allowed values. If the initial hyperparameters should be kept fixed, <code>None</code> can be passed as optimizer.</p> <p>The noise level in the targets can be specified by passing it via the parameter <code>alpha</code>, either globally as a scalar or per datapoint. Note that a moderate noise level can also be helpful for dealing with numeric issues during fitting as it is effectively implemented as Tikhonov regularization, i.e., by adding it to the diagonal of the kernel matrix. An alternative to specifying the noise level explicitly is to include a WhiteKernel component into the kernel, which can estimate the global noise level from the data (see example below).</p> <p>The implementation is based on Algorithm 2.1 of [RW2006]. In addition to the API of standard scikit-learn estimators, GaussianProcessRegressor:</p> <ul class="simple"> <li>allows prediction without prior fitting (based on the GP prior)</li> <li>provides an additional method <code>sample_y(X)</code>, which evaluates samples drawn from the GPR (prior or posterior) at given inputs</li> <li>exposes a method <code>log_marginal_likelihood(theta)</code>, which can be used externally for other ways of selecting hyperparameters, e.g., via Markov chain Monte Carlo.</li> </ul>   <h2 id="gpr-examples">1.7.2. GPR examples</h2>  <h3 id="gpr-with-noise-level-estimation">1.7.2.1. GPR with noise-level estimation</h3> <p>This example illustrates that GPR with a sum-kernel including a WhiteKernel can estimate the noise level of data. An illustration of the log-marginal-likelihood (LML) landscape shows that there exist two local maxima of LML.</p> <div class="figure align-center"> <a class="reference external image-reference" href="../../auto_examples/gaussian_process/plot_gpr_noisy/"><img alt="../_images/sphx_glr_plot_gpr_noisy_0001.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_gpr_noisy_0001.png"></a> </div> <p>The first corresponds to a model with a high noise level and a large length scale, which explains all variations in the data by noise.</p> <div class="figure align-center"> <a class="reference external image-reference" href="../../auto_examples/gaussian_process/plot_gpr_noisy/"><img alt="../_images/sphx_glr_plot_gpr_noisy_0011.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_gpr_noisy_0011.png"></a> </div> <p>The second one has a smaller noise level and shorter length scale, which explains most of the variation by the noise-free functional relationship. The second model has a higher likelihood; however, depending on the initial value for the hyperparameters, the gradient-based optimization might also converge to the high-noise solution. It is thus important to repeat the optimization several times for different initializations.</p> <div class="figure align-center"> <a class="reference external image-reference" href="../../auto_examples/gaussian_process/plot_gpr_noisy/"><img alt="../_images/sphx_glr_plot_gpr_noisy_0021.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_gpr_noisy_0021.png"></a> </div>   <h3 id="comparison-of-gpr-and-kernel-ridge-regression">1.7.2.2. Comparison of GPR and Kernel Ridge Regression</h3> <p>Both kernel ridge regression (KRR) and GPR learn a target function by employing internally the “kernel trick”. KRR learns a linear function in the space induced by the respective kernel which corresponds to a non-linear function in the original space. The linear function in the kernel space is chosen based on the mean-squared error loss with ridge regularization. GPR uses the kernel to define the covariance of a prior distribution over the target functions and uses the observed training data to define a likelihood function. Based on Bayes theorem, a (Gaussian) posterior distribution over target functions is defined, whose mean is used for prediction.</p> <p>A major difference is that GPR can choose the kernel’s hyperparameters based on gradient-ascent on the marginal likelihood function while KRR needs to perform a grid search on a cross-validated loss function (mean-squared error loss). A further difference is that GPR learns a generative, probabilistic model of the target function and can thus provide meaningful confidence intervals and posterior samples along with the predictions while KRR only provides predictions.</p> <p>The following figure illustrates both methods on an artificial dataset, which consists of a sinusoidal target function and strong noise. The figure compares the learned model of KRR and GPR based on a ExpSineSquared kernel, which is suited for learning periodic functions. The kernel’s hyperparameters control the smoothness (length_scale) and periodicity of the kernel (periodicity). Moreover, the noise level of the data is learned explicitly by GPR by an additional WhiteKernel component in the kernel and by the regularization parameter alpha of KRR.</p> <div class="figure align-center"> <a class="reference external image-reference" href="../../auto_examples/gaussian_process/plot_compare_gpr_krr/"><img alt="../_images/sphx_glr_plot_compare_gpr_krr_0011.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_compare_gpr_krr_0011.png"></a> </div> <p>The figure shows that both methods learn reasonable models of the target function. GPR correctly identifies the periodicity of the function to be roughly <img class="math" src="http://scikit-learn.org/stable/_images/math/7e5891265e5a58d3348270b2a201168f1ceecd9e.png" alt="2*\pi"> (6.28), while KRR chooses the doubled periodicity <img class="math" src="http://scikit-learn.org/stable/_images/math/edb72a7bafc63046ffccc792526da09519e38d8d.png" alt="4*\pi"> . Besides that, GPR provides reasonable confidence bounds on the prediction which are not available for KRR. A major difference between the two methods is the time required for fitting and predicting: while fitting KRR is fast in principle, the grid-search for hyperparameter optimization scales exponentially with the number of hyperparameters (“curse of dimensionality”). The gradient-based optimization of the parameters in GPR does not suffer from this exponential scaling and is thus considerable faster on this example with 3-dimensional hyperparameter space. The time for predicting is similar; however, generating the variance of the predictive distribution of GPR takes considerable longer than just predicting the mean.</p>   <h3 id="gpr-on-mauna-loa-co2-data">1.7.2.3. GPR on Mauna Loa CO2 data</h3> <p>This example is based on Section 5.4.3 of [RW2006]. It illustrates an example of complex kernel engineering and hyperparameter optimization using gradient ascent on the log-marginal-likelihood. The data consists of the monthly average atmospheric CO2 concentrations (in parts per million by volume (ppmv)) collected at the Mauna Loa Observatory in Hawaii, between 1958 and 1997. The objective is to model the CO2 concentration as a function of the time t.</p> <p>The kernel is composed of several terms that are responsible for explaining different properties of the signal:</p> <ul class="simple"> <li>a long term, smooth rising trend is to be explained by an RBF kernel. The RBF kernel with a large length-scale enforces this component to be smooth; it is not enforced that the trend is rising which leaves this choice to the GP. The specific length-scale and the amplitude are free hyperparameters.</li> <li>a seasonal component, which is to be explained by the periodic ExpSineSquared kernel with a fixed periodicity of 1 year. The length-scale of this periodic component, controlling its smoothness, is a free parameter. In order to allow decaying away from exact periodicity, the product with an RBF kernel is taken. The length-scale of this RBF component controls the decay time and is a further free parameter.</li> <li>smaller, medium term irregularities are to be explained by a RationalQuadratic kernel component, whose length-scale and alpha parameter, which determines the diffuseness of the length-scales, are to be determined. According to [RW2006], these irregularities can better be explained by a RationalQuadratic than an RBF kernel component, probably because it can accommodate several length-scales.</li> <li>a “noise” term, consisting of an RBF kernel contribution, which shall explain the correlated noise components such as local weather phenomena, and a WhiteKernel contribution for the white noise. The relative amplitudes and the RBF’s length scale are further free parameters.</li> </ul> <p>Maximizing the log-marginal-likelihood after subtracting the target’s mean yields the following kernel with an LML of -83.214:</p> <pre data-language="python">34.4**2 * RBF(length_scale=41.8)
+ 3.27**2 * RBF(length_scale=180) * ExpSineSquared(length_scale=1.44,
                                                   periodicity=1)
+ 0.446**2 * RationalQuadratic(alpha=17.7, length_scale=0.957)
+ 0.197**2 * RBF(length_scale=0.138) + WhiteKernel(noise_level=0.0336)
</pre> <p>Thus, most of the target signal (34.4ppm) is explained by a long-term rising trend (length-scale 41.8 years). The periodic component has an amplitude of 3.27ppm, a decay time of 180 years and a length-scale of 1.44. The long decay time indicates that we have a locally very close to periodic seasonal component. The correlated noise has an amplitude of 0.197ppm with a length scale of 0.138 years and a white-noise contribution of 0.197ppm. Thus, the overall noise level is very small, indicating that the data can be very well explained by the model. The figure shows also that the model makes very confident predictions until around 2015</p> <div class="figure align-center"> <a class="reference external image-reference" href="../../auto_examples/gaussian_process/plot_gpr_co2/"><img alt="../_images/sphx_glr_plot_gpr_co2_0011.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_gpr_co2_0011.png"></a> </div>    <h2 id="gpc">1.7.3. Gaussian Process Classification (GPC)</h2> <p id="gaussian-process-classification-gpc">The <a class="reference internal" href="../generated/sklearn.gaussian_process.gaussianprocessclassifier/#sklearn.gaussian_process.GaussianProcessClassifier" title="sklearn.gaussian_process.GaussianProcessClassifier"><code>GaussianProcessClassifier</code></a> implements Gaussian processes (GP) for classification purposes, more specifically for probabilistic classification, where test predictions take the form of class probabilities. GaussianProcessClassifier places a GP prior on a latent function <img class="math" src="http://scikit-learn.org/stable/_images/math/875eb40014526135383caa89fd500ae40a835f56.png" alt="f">, which is then squashed through a link function to obtain the probabilistic classification. The latent function <img class="math" src="http://scikit-learn.org/stable/_images/math/875eb40014526135383caa89fd500ae40a835f56.png" alt="f"> is a so-called nuisance function, whose values are not observed and are not relevant by themselves. Its purpose is to allow a convenient formulation of the model, and <img class="math" src="http://scikit-learn.org/stable/_images/math/875eb40014526135383caa89fd500ae40a835f56.png" alt="f"> is removed (integrated out) during prediction. GaussianProcessClassifier implements the logistic link function, for which the integral cannot be computed analytically but is easily approximated in the binary case.</p> <p>In contrast to the regression setting, the posterior of the latent function <img class="math" src="http://scikit-learn.org/stable/_images/math/875eb40014526135383caa89fd500ae40a835f56.png" alt="f"> is not Gaussian even for a GP prior since a Gaussian likelihood is inappropriate for discrete class labels. Rather, a non-Gaussian likelihood corresponding to the logistic link function (logit) is used. GaussianProcessClassifier approximates the non-Gaussian posterior with a Gaussian based on the Laplace approximation. More details can be found in Chapter 3 of [RW2006].</p> <p>The GP prior mean is assumed to be zero. The prior’s covariance is specified by a passing a <a class="reference internal" href="#gp-kernels"><span class="std std-ref">kernel</span></a> object. The hyperparameters of the kernel are optimized during fitting of GaussianProcessRegressor by maximizing the log-marginal-likelihood (LML) based on the passed <code>optimizer</code>. As the LML may have multiple local optima, the optimizer can be started repeatedly by specifying <code>n_restarts_optimizer</code>. The first run is always conducted starting from the initial hyperparameter values of the kernel; subsequent runs are conducted from hyperparameter values that have been chosen randomly from the range of allowed values. If the initial hyperparameters should be kept fixed, <code>None</code> can be passed as optimizer.</p> <p><a class="reference internal" href="../generated/sklearn.gaussian_process.gaussianprocessclassifier/#sklearn.gaussian_process.GaussianProcessClassifier" title="sklearn.gaussian_process.GaussianProcessClassifier"><code>GaussianProcessClassifier</code></a> supports multi-class classification by performing either one-versus-rest or one-versus-one based training and prediction. In one-versus-rest, one binary Gaussian process classifier is fitted for each class, which is trained to separate this class from the rest. In “one_vs_one”, one binary Gaussian process classifier is fitted for each pair of classes, which is trained to separate these two classes. The predictions of these binary predictors are combined into multi-class predictions. See the section on <a class="reference internal" href="../multiclass/#multiclass"><span class="std std-ref">multi-class classification</span></a> for more details.</p> <p>In the case of Gaussian process classification, “one_vs_one” might be computationally cheaper since it has to solve many problems involving only a subset of the whole training set rather than fewer problems on the whole dataset. Since Gaussian process classification scales cubically with the size of the dataset, this might be considerably faster. However, note that “one_vs_one” does not support predicting probability estimates but only plain predictions. Moreover, note that <a class="reference internal" href="../generated/sklearn.gaussian_process.gaussianprocessclassifier/#sklearn.gaussian_process.GaussianProcessClassifier" title="sklearn.gaussian_process.GaussianProcessClassifier"><code>GaussianProcessClassifier</code></a> does not (yet) implement a true multi-class Laplace approximation internally, but as discussed aboved is based on solving several binary classification tasks internally, which are combined using one-versus-rest or one-versus-one.</p>   <h2 id="gpc-examples">1.7.4. GPC examples</h2>  <h3 id="probabilistic-predictions-with-gpc">1.7.4.1. Probabilistic predictions with GPC</h3> <p>This example illustrates the predicted probability of GPC for an RBF kernel with different choices of the hyperparameters. The first figure shows the predicted probability of GPC with arbitrarily chosen hyperparameters and with the hyperparameters corresponding to the maximum log-marginal-likelihood (LML).</p> <p>While the hyperparameters chosen by optimizing LML have a considerable larger LML, they perform slightly worse according to the log-loss on test data. The figure shows that this is because they exhibit a steep change of the class probabilities at the class boundaries (which is good) but have predicted probabilities close to 0.5 far away from the class boundaries (which is bad) This undesirable effect is caused by the Laplace approximation used internally by GPC.</p> <p>The second figure shows the log-marginal-likelihood for different choices of the kernel’s hyperparameters, highlighting the two choices of the hyperparameters used in the first figure by black dots.</p> <div class="figure align-center"> <a class="reference external image-reference" href="../../auto_examples/gaussian_process/plot_gpc/"><img alt="../_images/sphx_glr_plot_gpc_0001.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_gpc_0001.png"></a> </div> <div class="figure align-center"> <a class="reference external image-reference" href="../../auto_examples/gaussian_process/plot_gpc/"><img alt="../_images/sphx_glr_plot_gpc_0011.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_gpc_0011.png"></a> </div>   <h3 id="illustration-of-gpc-on-the-xor-dataset">1.7.4.2. Illustration of GPC on the XOR dataset</h3> <p>This example illustrates GPC on XOR data. Compared are a stationary, isotropic kernel (<a class="reference internal" href="../generated/sklearn.gaussian_process.kernels.rbf/#sklearn.gaussian_process.kernels.RBF" title="sklearn.gaussian_process.kernels.RBF"><code>RBF</code></a>) and a non-stationary kernel (<a class="reference internal" href="../generated/sklearn.gaussian_process.kernels.dotproduct/#sklearn.gaussian_process.kernels.DotProduct" title="sklearn.gaussian_process.kernels.DotProduct"><code>DotProduct</code></a>). On this particular dataset, the <code>DotProduct</code> kernel obtains considerably better results because the class-boundaries are linear and coincide with the coordinate axes. In practice, however, stationary kernels such as <a class="reference internal" href="../generated/sklearn.gaussian_process.kernels.rbf/#sklearn.gaussian_process.kernels.RBF" title="sklearn.gaussian_process.kernels.RBF"><code>RBF</code></a> often obtain better results.</p> <div class="figure align-center"> <a class="reference external image-reference" href="../../auto_examples/gaussian_process/plot_gpc_xor/"><img alt="../_images/sphx_glr_plot_gpc_xor_0011.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_gpc_xor_0011.png"></a> </div>   <h3 id="gaussian-process-classification-gpc-on-iris-dataset">1.7.4.3. Gaussian process classification (GPC) on iris dataset</h3> <p>This example illustrates the predicted probability of GPC for an isotropic and anisotropic RBF kernel on a two-dimensional version for the iris-dataset. This illustrates the applicability of GPC to non-binary classification. The anisotropic RBF kernel obtains slightly higher log-marginal-likelihood by assigning different length-scales to the two feature dimensions.</p> <div class="figure align-center"> <a class="reference external image-reference" href="../../auto_examples/gaussian_process/plot_gpc_iris/"><img alt="../_images/sphx_glr_plot_gpc_iris_0011.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_gpc_iris_0011.png"></a> </div>    <h2 id="gp-kernels">1.7.5. Kernels for Gaussian Processes</h2> <p id="kernels-for-gaussian-processes">Kernels (also called “covariance functions” in the context of GPs) are a crucial ingredient of GPs which determine the shape of prior and posterior of the GP. They encode the assumptions on the function being learned by defining the “similarity” of two datapoints combined with the assumption that similar datapoints should have similar target values. Two categories of kernels can be distinguished: stationary kernels depend only on the distance of two datapoints and not on their absolute values <img class="math" src="http://scikit-learn.org/stable/_images/math/4881aa9b46726599125f494cc1c180361ba8fffa.png" alt="k(x_i, x_j)= k(d(x_i, x_j))"> and are thus invariant to translations in the input space, while non-stationary kernels depend also on the specific values of the datapoints. Stationary kernels can further be subdivided into isotropic and anisotropic kernels, where isotropic kernels are also invariant to rotations in the input space. For more details, we refer to Chapter 4 of [RW2006].</p>  <h3 id="gaussian-process-kernel-api">1.7.5.1. Gaussian Process Kernel API</h3> <p>The main usage of a <a class="reference internal" href="../generated/sklearn.gaussian_process.kernels.kernel/#sklearn.gaussian_process.kernels.Kernel" title="sklearn.gaussian_process.kernels.Kernel"><code>Kernel</code></a> is to compute the GP’s covariance between datapoints. For this, the method <code>__call__</code> of the kernel can be called. This method can either be used to compute the “auto-covariance” of all pairs of datapoints in a 2d array X, or the “cross-covariance” of all combinations of datapoints of a 2d array X with datapoints in a 2d array Y. The following identity holds true for all kernels k (except for the <a class="reference internal" href="../generated/sklearn.gaussian_process.kernels.whitekernel/#sklearn.gaussian_process.kernels.WhiteKernel" title="sklearn.gaussian_process.kernels.WhiteKernel"><code>WhiteKernel</code></a>): <code>k(X) == K(X, Y=X)</code></p> <p>If only the diagonal of the auto-covariance is being used, the method <code>diag()</code> of a kernel can be called, which is more computationally efficient than the equivalent call to <code>__call__</code>: <code>np.diag(k(X, X)) == k.diag(X)</code></p> <p>Kernels are parameterized by a vector <img class="math" src="http://scikit-learn.org/stable/_images/math/3be04d4207434584251f6921820c24ac9fa8c6f1.png" alt="\theta"> of hyperparameters. These hyperparameters can for instance control length-scales or periodicity of a kernel (see below). All kernels support computing analytic gradients of of the kernel’s auto-covariance with respect to <img class="math" src="http://scikit-learn.org/stable/_images/math/3be04d4207434584251f6921820c24ac9fa8c6f1.png" alt="\theta"> via setting <code>eval_gradient=True</code> in the <code>__call__</code> method. This gradient is used by the Gaussian process (both regressor and classifier) in computing the gradient of the log-marginal-likelihood, which in turn is used to determine the value of <img class="math" src="http://scikit-learn.org/stable/_images/math/3be04d4207434584251f6921820c24ac9fa8c6f1.png" alt="\theta">, which maximizes the log-marginal-likelihood, via gradient ascent. For each hyperparameter, the initial value and the bounds need to be specified when creating an instance of the kernel. The current value of <img class="math" src="http://scikit-learn.org/stable/_images/math/3be04d4207434584251f6921820c24ac9fa8c6f1.png" alt="\theta"> can be get and set via the property <code>theta</code> of the kernel object. Moreover, the bounds of the hyperparameters can be accessed by the property <code>bounds</code> of the kernel. Note that both properties (theta and bounds) return log-transformed values of the internally used values since those are typically more amenable to gradient-based optimization. The specification of each hyperparameter is stored in the form of an instance of <a class="reference internal" href="../generated/sklearn.gaussian_process.kernels.hyperparameter/#sklearn.gaussian_process.kernels.Hyperparameter" title="sklearn.gaussian_process.kernels.Hyperparameter"><code>Hyperparameter</code></a> in the respective kernel. Note that a kernel using a hyperparameter with name “x” must have the attributes self.x and self.x_bounds.</p> <p>The abstract base class for all kernels is <a class="reference internal" href="../generated/sklearn.gaussian_process.kernels.kernel/#sklearn.gaussian_process.kernels.Kernel" title="sklearn.gaussian_process.kernels.Kernel"><code>Kernel</code></a>. Kernel implements a similar interface as <code>Estimator</code>, providing the methods <code>get_params()</code>, <code>set_params()</code>, and <code>clone()</code>. This allows setting kernel values also via meta-estimators such as <code>Pipeline</code> or <code>GridSearch</code>. Note that due to the nested structure of kernels (by applying kernel operators, see below), the names of kernel parameters might become relatively complicated. In general, for a binary kernel operator, parameters of the left operand are prefixed with <code>k1__</code> and parameters of the right operand with <code>k2__</code>. An additional convenience method is <code>clone_with_theta(theta)</code>, which returns a cloned version of the kernel but with the hyperparameters set to <code>theta</code>. An illustrative example:</p> <pre data-language="python">&gt;&gt;&gt; from sklearn.gaussian_process.kernels import ConstantKernel, RBF
&gt;&gt;&gt; kernel = ConstantKernel(constant_value=1.0, constant_value_bounds=(0.0, 10.0)) * RBF(length_scale=0.5, length_scale_bounds=(0.0, 10.0)) + RBF(length_scale=2.0, length_scale_bounds=(0.0, 10.0))
&gt;&gt;&gt; for hyperparameter in kernel.hyperparameters: print(hyperparameter)
Hyperparameter(name='k1__k1__constant_value', value_type='numeric', bounds=array([[  0.,  10.]]), n_elements=1, fixed=False)
Hyperparameter(name='k1__k2__length_scale', value_type='numeric', bounds=array([[  0.,  10.]]), n_elements=1, fixed=False)
Hyperparameter(name='k2__length_scale', value_type='numeric', bounds=array([[  0.,  10.]]), n_elements=1, fixed=False)
&gt;&gt;&gt; params = kernel.get_params()
&gt;&gt;&gt; for key in sorted(params): print("%s : %s" % (key, params[key]))
k1 : 1**2 * RBF(length_scale=0.5)
k1__k1 : 1**2
k1__k1__constant_value : 1.0
k1__k1__constant_value_bounds : (0.0, 10.0)
k1__k2 : RBF(length_scale=0.5)
k1__k2__length_scale : 0.5
k1__k2__length_scale_bounds : (0.0, 10.0)
k2 : RBF(length_scale=2)
k2__length_scale : 2.0
k2__length_scale_bounds : (0.0, 10.0)
&gt;&gt;&gt; print(kernel.theta)  # Note: log-transformed
[ 0.         -0.69314718  0.69314718]
&gt;&gt;&gt; print(kernel.bounds)  # Note: log-transformed
[[       -inf  2.30258509]
 [       -inf  2.30258509]
 [       -inf  2.30258509]]
</pre> <p>All Gaussian process kernels are interoperable with <a class="reference internal" href="../classes/#module-sklearn.metrics.pairwise" title="sklearn.metrics.pairwise"><code>sklearn.metrics.pairwise</code></a> and vice versa: instances of subclasses of <a class="reference internal" href="../generated/sklearn.gaussian_process.kernels.kernel/#sklearn.gaussian_process.kernels.Kernel" title="sklearn.gaussian_process.kernels.Kernel"><code>Kernel</code></a> can be passed as <code>metric</code> to pairwise_kernels`` from <a class="reference internal" href="../classes/#module-sklearn.metrics.pairwise" title="sklearn.metrics.pairwise"><code>sklearn.metrics.pairwise</code></a>. Moreover, kernel functions from pairwise can be used as GP kernels by using the wrapper class <a class="reference internal" href="../generated/sklearn.gaussian_process.kernels.pairwisekernel/#sklearn.gaussian_process.kernels.PairwiseKernel" title="sklearn.gaussian_process.kernels.PairwiseKernel"><code>PairwiseKernel</code></a>. The only caveat is that the gradient of the hyperparameters is not analytic but numeric and all those kernels support only isotropic distances. The parameter <code>gamma</code> is considered to be a hyperparameter and may be optimized. The other kernel parameters are set directly at initialization and are kept fixed.</p>   <h3 id="basic-kernels">1.7.5.2. Basic kernels</h3> <p>The <a class="reference internal" href="../generated/sklearn.gaussian_process.kernels.constantkernel/#sklearn.gaussian_process.kernels.ConstantKernel" title="sklearn.gaussian_process.kernels.ConstantKernel"><code>ConstantKernel</code></a> kernel can be used as part of a <a class="reference internal" href="../generated/sklearn.gaussian_process.kernels.product/#sklearn.gaussian_process.kernels.Product" title="sklearn.gaussian_process.kernels.Product"><code>Product</code></a> kernel where it scales the magnitude of the other factor (kernel) or as part of a <a class="reference internal" href="../generated/sklearn.gaussian_process.kernels.sum/#sklearn.gaussian_process.kernels.Sum" title="sklearn.gaussian_process.kernels.Sum"><code>Sum</code></a> kernel, where it modifies the mean of the Gaussian process. It depends on a parameter <img class="math" src="http://scikit-learn.org/stable/_images/math/dace9e57a53affd82f1dd9088f41796692a49b5f.png" alt="constant\_value">. It is defined as:</p> <div class="math"> <p><img src="http://scikit-learn.org/stable/_images/math/78db639aac615f9ade72ff5054ef9e50edad9d8b.png" alt="k(x_i, x_j) = constant\_value \;\forall\; x_1, x_2"></p> </div>
<p>The main use-case of the <a class="reference internal" href="../generated/sklearn.gaussian_process.kernels.whitekernel/#sklearn.gaussian_process.kernels.WhiteKernel" title="sklearn.gaussian_process.kernels.WhiteKernel"><code>WhiteKernel</code></a> kernel is as part of a sum-kernel where it explains the noise-component of the signal. Tuning its parameter <img class="math" src="http://scikit-learn.org/stable/_images/math/02da34bc67259d369b2af889c9ed593c8e4bad11.png" alt="noise\_level"> corresponds to estimating the noise-level. It is defined as:e</p> <div class="math"> <p><img src="http://scikit-learn.org/stable/_images/math/e6b27361bfbe081f64998807df9e3261e18777ce.png" alt="k(x_i, x_j) = noise\_level \text{ if } x_i == x_j \text{ else } 0"></p> </div>  <h3 id="kernel-operators">1.7.5.3. Kernel operators</h3> <p>Kernel operators take one or two base kernels and combine them into a new kernel. The <a class="reference internal" href="../generated/sklearn.gaussian_process.kernels.sum/#sklearn.gaussian_process.kernels.Sum" title="sklearn.gaussian_process.kernels.Sum"><code>Sum</code></a> kernel takes two kernels <img class="math" src="http://scikit-learn.org/stable/_images/math/68315c68055d1a751cf80a988f9e8efb17916057.png" alt="k1"> and <img class="math" src="http://scikit-learn.org/stable/_images/math/3e33b4a0e6e222a9867bd30547116a2e04c57685.png" alt="k2"> and combines them via <img class="math" src="http://scikit-learn.org/stable/_images/math/750b1071df551aa409aeb001874d5eeabafe597f.png" alt="k_{sum}(X, Y) = k1(X, Y) + k2(X, Y)">. The <a class="reference internal" href="../generated/sklearn.gaussian_process.kernels.product/#sklearn.gaussian_process.kernels.Product" title="sklearn.gaussian_process.kernels.Product"><code>Product</code></a> kernel takes two kernels <img class="math" src="http://scikit-learn.org/stable/_images/math/68315c68055d1a751cf80a988f9e8efb17916057.png" alt="k1"> and <img class="math" src="http://scikit-learn.org/stable/_images/math/3e33b4a0e6e222a9867bd30547116a2e04c57685.png" alt="k2"> and combines them via <img class="math" src="http://scikit-learn.org/stable/_images/math/c0fdd6204c73b45e9e702acec75fff19355e448a.png" alt="k_{product}(X, Y) = k1(X, Y) * k2(X, Y)">. The <a class="reference internal" href="../generated/sklearn.gaussian_process.kernels.exponentiation/#sklearn.gaussian_process.kernels.Exponentiation" title="sklearn.gaussian_process.kernels.Exponentiation"><code>Exponentiation</code></a> kernel takes one base kernel and a scalar parameter <img class="math" src="http://scikit-learn.org/stable/_images/math/4efad020bdbbbde1980aa28b5eef54551359f040.png" alt="exponent"> and combines them via <img class="math" src="http://scikit-learn.org/stable/_images/math/6f64569f809e5607c7f5d2cdaeb168d1ed635075.png" alt="k_{exp}(X, Y) = k(X, Y)^\text{exponent}">.</p>   <h3 id="radial-basis-function-rbf-kernel">1.7.5.4. Radial-basis function (RBF) kernel</h3> <p>The <a class="reference internal" href="../generated/sklearn.gaussian_process.kernels.rbf/#sklearn.gaussian_process.kernels.RBF" title="sklearn.gaussian_process.kernels.RBF"><code>RBF</code></a> kernel is a stationary kernel. It is also known as the “squared exponential” kernel. It is parameterized by a length-scale parameter <img class="math" src="http://scikit-learn.org/stable/_images/math/2b03f10859ffeeee9ece9f213287e3b54f5f39b8.png" alt="l&gt;0">, which can either be a scalar (isotropic variant of the kernel) or a vector with the same number of dimensions as the inputs <img class="math" src="http://scikit-learn.org/stable/_images/math/a59f68a4202623bb859a7093f0316bf466e6f75d.png" alt="x"> (anisotropic variant of the kernel). The kernel is given by:</p> <div class="math"> <p><img src="http://scikit-learn.org/stable/_images/math/2846faf35c60f5b1d7659f9086fe5d2abfbef25e.png" alt="k(x_i, x_j) = \text{exp}\left(-\frac{1}{2} d(x_i / l, x_j / l)^2\right)"></p> </div>
<p>This kernel is infinitely differentiable, which implies that GPs with this kernel as covariance function have mean square derivatives of all orders, and are thus very smooth. The prior and posterior of a GP resulting from an RBF kernel are shown in the following figure:</p> <div class="figure align-center"> <a class="reference external image-reference" href="../../auto_examples/gaussian_process/plot_gpr_prior_posterior/"><img alt="../_images/sphx_glr_plot_gpr_prior_posterior_0001.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_gpr_prior_posterior_0001.png"></a> </div>   <h3 id="matern-kernel">1.7.5.5. Matérn kernel</h3> <p>The <a class="reference internal" href="../generated/sklearn.gaussian_process.kernels.matern/#sklearn.gaussian_process.kernels.Matern" title="sklearn.gaussian_process.kernels.Matern"><code>Matern</code></a> kernel is a stationary kernel and a generalization of the <a class="reference internal" href="../generated/sklearn.gaussian_process.kernels.rbf/#sklearn.gaussian_process.kernels.RBF" title="sklearn.gaussian_process.kernels.RBF"><code>RBF</code></a> kernel. It has an additional parameter <img class="math" src="http://scikit-learn.org/stable/_images/math/ca3b8fa4180eee2dfc3af9d13fae1da451cd2c31.png" alt="\nu"> which controls the smoothness of the resulting function. It is parameterized by a length-scale parameter <img class="math" src="http://scikit-learn.org/stable/_images/math/2b03f10859ffeeee9ece9f213287e3b54f5f39b8.png" alt="l&gt;0">, which can either be a scalar (isotropic variant of the kernel) or a vector with the same number of dimensions as the inputs <img class="math" src="http://scikit-learn.org/stable/_images/math/a59f68a4202623bb859a7093f0316bf466e6f75d.png" alt="x"> (anisotropic variant of the kernel). The kernel is given by:</p> <div class="math"> <p><img src="http://scikit-learn.org/stable/_images/math/3073f503e2051eae1f6768f094d9e8d3ebe3ae3d.png" alt="k(x_i, x_j) = \sigma^2\frac{1}{\Gamma(\nu)2^{\nu-1}}\Bigg(\gamma\sqrt{2\nu} d(x_i / l, x_j / l)\Bigg)^\nu K_\nu\Bigg(\gamma\sqrt{2\nu} d(x_i / l, x_j / l)\Bigg),"></p> </div>
<p>As <img class="math" src="http://scikit-learn.org/stable/_images/math/c4a63cbc0ce26541a38674d6679f3e6734e32e3f.png" alt="\nu\rightarrow\infty">, the Matérn kernel converges to the RBF kernel. When <img class="math" src="http://scikit-learn.org/stable/_images/math/1eaefca0196440b9da604036e8a3fcbf91e2793f.png" alt="\nu = 1/2">, the Matérn kernel becomes identical to the absolute exponential kernel, i.e.,</p> <div class="math"> <p><img src="http://scikit-learn.org/stable/_images/math/ac3dac103e91f823b9cbd3b57f5ba65ea6b4c176.png" alt="k(x_i, x_j) = \sigma^2 \exp \Bigg(-\gamma d(x_i / l, x_j / l) \Bigg) \quad \quad \nu= \tfrac{1}{2}"></p> </div>
<p>In particular, <img class="math" src="http://scikit-learn.org/stable/_images/math/8611382bdaa0ae96ed10c4629e5ff6da6a92956c.png" alt="\nu = 3/2">:</p> <div class="math"> <p><img src="http://scikit-learn.org/stable/_images/math/c891c4103f23b6dc72855d38ef10900596264aef.png" alt="k(x_i, x_j) = \sigma^2 \Bigg(1 + \gamma \sqrt{3} d(x_i / l, x_j / l)\Bigg) \exp \Bigg(-\gamma \sqrt{3}d(x_i / l, x_j / l) \Bigg) \quad \quad \nu= \tfrac{3}{2}"></p> </div>
<p>and <img class="math" src="http://scikit-learn.org/stable/_images/math/1917d8043d8745bfad4f78d3527ed0d21e1b5cbc.png" alt="\nu = 5/2">:</p> <div class="math"> <p><img src="http://scikit-learn.org/stable/_images/math/5dbdc85e62777db765f4f2df0a4793e59d979663.png" alt="k(x_i, x_j) = \sigma^2 \Bigg(1 + \gamma \sqrt{5}d(x_i / l, x_j / l) +\frac{5}{3} \gamma^2d(x_i / l, x_j / l)^2 \Bigg) \exp \Bigg(-\gamma \sqrt{5}d(x_i / l, x_j / l) \Bigg) \quad \quad \nu= \tfrac{5}{2}"></p> </div>
<p>are popular choices for learning functions that are not infinitely differentiable (as assumed by the RBF kernel) but at least once (<img class="math" src="http://scikit-learn.org/stable/_images/math/0e8ccc1dc653d5e027ab2476fa3987edb564c10f.png" alt="\nu =
3/2">) or twice differentiable (<img class="math" src="http://scikit-learn.org/stable/_images/math/1917d8043d8745bfad4f78d3527ed0d21e1b5cbc.png" alt="\nu = 5/2">).</p> <p>The flexibility of controlling the smoothness of the learned function via <img class="math" src="http://scikit-learn.org/stable/_images/math/ca3b8fa4180eee2dfc3af9d13fae1da451cd2c31.png" alt="\nu"> allows adapting to the properties of the true underlying functional relation. The prior and posterior of a GP resulting from a Matérn kernel are shown in the following figure:</p> <div class="figure align-center"> <a class="reference external image-reference" href="../../auto_examples/gaussian_process/plot_gpr_prior_posterior/"><img alt="../_images/sphx_glr_plot_gpr_prior_posterior_0041.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_gpr_prior_posterior_0041.png"></a> </div> <p>See [RW2006], pp84 for further details regarding the different variants of the Matérn kernel.</p>   <h3 id="rational-quadratic-kernel">1.7.5.6. Rational quadratic kernel</h3> <p>The <a class="reference internal" href="../generated/sklearn.gaussian_process.kernels.rationalquadratic/#sklearn.gaussian_process.kernels.RationalQuadratic" title="sklearn.gaussian_process.kernels.RationalQuadratic"><code>RationalQuadratic</code></a> kernel can be seen as a scale mixture (an infinite sum) of <a class="reference internal" href="../generated/sklearn.gaussian_process.kernels.rbf/#sklearn.gaussian_process.kernels.RBF" title="sklearn.gaussian_process.kernels.RBF"><code>RBF</code></a> kernels with different characteristic length-scales. It is parameterized by a length-scale parameter <img class="math" src="http://scikit-learn.org/stable/_images/math/2b03f10859ffeeee9ece9f213287e3b54f5f39b8.png" alt="l&gt;0"> and a scale mixture parameter <img class="math" src="http://scikit-learn.org/stable/_images/math/9f344c8d0967114a3878754e90bbc1890d772791.png" alt="\alpha&gt;0"> Only the isotropic variant where <img class="math" src="http://scikit-learn.org/stable/_images/math/b359488b993294ebbc2c7b30ab8f749dcbc6826d.png" alt="l"> is a scalar is supported at the moment. The kernel is given by:</p> <div class="math"> <p><img src="http://scikit-learn.org/stable/_images/math/271a59884193345d1292f6d325e7a3a91eba9e5b.png" alt="k(x_i, x_j) = \left(1 + \frac{d(x_i, x_j)^2}{2\alpha l^2}\right)^\alpha"></p> </div>
<p>The prior and posterior of a GP resulting from an RBF kernel are shown in the following figure:</p> <div class="figure align-center"> <a class="reference external image-reference" href="../../auto_examples/gaussian_process/plot_gpr_prior_posterior/"><img alt="../_images/sphx_glr_plot_gpr_prior_posterior_0011.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_gpr_prior_posterior_0011.png"></a> </div>   <h3 id="exp-sine-squared-kernel">1.7.5.7. Exp-Sine-Squared kernel</h3> <p>The <a class="reference internal" href="../generated/sklearn.gaussian_process.kernels.expsinesquared/#sklearn.gaussian_process.kernels.ExpSineSquared" title="sklearn.gaussian_process.kernels.ExpSineSquared"><code>ExpSineSquared</code></a> kernel allows modeling periodic functions. It is parameterized by a length-scale parameter <img class="math" src="http://scikit-learn.org/stable/_images/math/2b03f10859ffeeee9ece9f213287e3b54f5f39b8.png" alt="l&gt;0"> and a periodicity parameter <img class="math" src="http://scikit-learn.org/stable/_images/math/2af41505f2ef569714a5cd573632db182b69ce23.png" alt="p&gt;0">. Only the isotropic variant where <img class="math" src="http://scikit-learn.org/stable/_images/math/b359488b993294ebbc2c7b30ab8f749dcbc6826d.png" alt="l"> is a scalar is supported at the moment. The kernel is given by:</p> <div class="math"> <p><img src="http://scikit-learn.org/stable/_images/math/e3567fa21e2c72a0569505c0c0fc553cd91943ad.png" alt="k(x_i, x_j) = \text{exp}\left(-2 \text{sin}(\pi / p * d(x_i, x_j)) / l\right)^2"></p> </div>
<p>The prior and posterior of a GP resulting from an ExpSineSquared kernel are shown in the following figure:</p> <div class="figure align-center"> <a class="reference external image-reference" href="../../auto_examples/gaussian_process/plot_gpr_prior_posterior/"><img alt="../_images/sphx_glr_plot_gpr_prior_posterior_0021.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_gpr_prior_posterior_0021.png"></a> </div>   <h3 id="dot-product-kernel">1.7.5.8. Dot-Product kernel</h3> <p>The <a class="reference internal" href="../generated/sklearn.gaussian_process.kernels.dotproduct/#sklearn.gaussian_process.kernels.DotProduct" title="sklearn.gaussian_process.kernels.DotProduct"><code>DotProduct</code></a> kernel is non-stationary and can be obtained from linear regression by putting <img class="math" src="http://scikit-learn.org/stable/_images/math/35a99affae9aad839228ce0706c16021dd353e21.png" alt="N(0, 1)"> priors on the coefficients of <img class="math" src="http://scikit-learn.org/stable/_images/math/987ab6cf10335c5861bbf97f8a4d23affe39099b.png" alt="x_d (d = 1, . . . , D)"> and a prior of <img class="math" src="http://scikit-learn.org/stable/_images/math/0a966a93ff32902df853e5cad1567afe49c5c33d.png" alt="N(0, \sigma_0^2)"> on the bias. The <a class="reference internal" href="../generated/sklearn.gaussian_process.kernels.dotproduct/#sklearn.gaussian_process.kernels.DotProduct" title="sklearn.gaussian_process.kernels.DotProduct"><code>DotProduct</code></a> kernel is invariant to a rotation of the coordinates about the origin, but not translations. It is parameterized by a parameter <img class="math" src="http://scikit-learn.org/stable/_images/math/d1bb611863c366568ca62a0445d35f29b79bfab9.png" alt="\sigma_0^2">. For <img class="math" src="http://scikit-learn.org/stable/_images/math/34484a0ed084809e99e9a3c18aec52eca87f73b9.png" alt="\sigma_0^2 = 0">, the kernel is called the homogeneous linear kernel, otherwise it is inhomogeneous. The kernel is given by</p> <div class="math"> <p><img src="http://scikit-learn.org/stable/_images/math/6512a4721e521f0bf1eeaf8e837334dac11596df.png" alt="k(x_i, x_j) = \sigma_0 ^ 2 + x_i \cdot x_j"></p> </div>
<p>The <a class="reference internal" href="../generated/sklearn.gaussian_process.kernels.dotproduct/#sklearn.gaussian_process.kernels.DotProduct" title="sklearn.gaussian_process.kernels.DotProduct"><code>DotProduct</code></a> kernel is commonly combined with exponentiation. An example with exponent 2 is shown in the following figure:</p> <div class="figure align-center"> <a class="reference external image-reference" href="../../auto_examples/gaussian_process/plot_gpr_prior_posterior/"><img alt="../_images/sphx_glr_plot_gpr_prior_posterior_0031.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_gpr_prior_posterior_0031.png"></a> </div>   <h3 id="references">1.7.5.9. References</h3>  <ul class="simple"> <li>
<a class="reference external" href="http://www.gaussianprocess.org/gpml/chapters/" target="_blank">[RW2006]</a> <strong>Gaussian Processes for Machine Learning</strong>, Carl Eduard Rasmussen and Christopher K.I. Williams, MIT Press 2006. Link to an official complete PDF version of the book <a class="reference external" href="http://www.gaussianprocess.org/gpml/chapters/RW.pdf" target="_blank">here</a> .</li> </ul>     <h2 id="legacy-gaussian-processes">1.7.6. Legacy Gaussian Processes</h2> <p>In this section, the implementation of Gaussian processes used in scikit-learn until release 0.16.1 is described. Note that this implementation is deprecated and will be removed in version 0.18.</p>  <h3 id="an-introductory-regression-example">1.7.6.1. An introductory regression example</h3> <p>Say we want to surrogate the function <img class="math" src="http://scikit-learn.org/stable/_images/math/4fb3ff02fcf59ecca945a30ea54943beec6170b2.png" alt="g(x) = x \sin(x)">. To do so, the function is evaluated onto a design of experiments. Then, we define a GaussianProcess model whose regression and correlation models might be specified using additional kwargs, and ask for the model to be fitted to the data. Depending on the number of parameters provided at instantiation, the fitting procedure may recourse to maximum likelihood estimation for the parameters or alternatively it uses the given parameters.</p> <pre data-language="python">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from sklearn import gaussian_process
&gt;&gt;&gt; def f(x):
...     return x * np.sin(x)
&gt;&gt;&gt; X = np.atleast_2d([1., 3., 5., 6., 7., 8.]).T
&gt;&gt;&gt; y = f(X).ravel()
&gt;&gt;&gt; x = np.atleast_2d(np.linspace(0, 10, 1000)).T
&gt;&gt;&gt; gp = gaussian_process.GaussianProcess(theta0=1e-2, thetaL=1e-4, thetaU=1e-1)
&gt;&gt;&gt; gp.fit(X, y)  
GaussianProcess(beta0=None, corr=&lt;function squared_exponential at 0x...&gt;,
        normalize=True, nugget=array(2.22...-15),
        optimizer='fmin_cobyla', random_start=1, random_state=...
        regr=&lt;function constant at 0x...&gt;, storage_mode='full',
        theta0=array([[ 0.01]]), thetaL=array([[ 0.0001]]),
        thetaU=array([[ 0.1]]), verbose=False)
&gt;&gt;&gt; y_pred, sigma2_pred = gp.predict(x, eval_MSE=True)
</pre>   <h3 id="fitting-noisy-data">1.7.6.2. Fitting Noisy Data</h3> <p>When the data to be fit includes noise, the Gaussian process model can be used by specifying the variance of the noise for each point. <a class="reference internal" href="../generated/sklearn.gaussian_process.gaussianprocess/#sklearn.gaussian_process.GaussianProcess" title="sklearn.gaussian_process.GaussianProcess"><code>GaussianProcess</code></a> takes a parameter <code>nugget</code> which is added to the diagonal of the correlation matrix between training points: in general this is a type of Tikhonov regularization. In the special case of a squared-exponential correlation function, this normalization is equivalent to specifying a fractional variance in the input. That is</p> <div class="math"> <p><img src="http://scikit-learn.org/stable/_images/math/bec91d7032e113797c13fb4b50295c35309bbd60.png" alt="\mathrm{nugget}_i = \left[\frac{\sigma_i}{y_i}\right]^2"></p> </div>
<p>With <code>nugget</code> and <code>corr</code> properly set, Gaussian Processes can be used to robustly recover an underlying function from noisy data.</p>   <h3 id="mathematical-formulation">1.7.6.3. Mathematical formulation</h3>  <h4 id="the-initial-assumption">1.7.6.3.1. The initial assumption</h4> <p>Suppose one wants to model the output of a computer experiment, say a mathematical function:</p> <div class="math"> <p><img src="http://scikit-learn.org/stable/_images/math/69d2415c9026b7a1b47321ff64417833632a8357.png" alt="g: &amp; \mathbb{R}^{n_{\rm features}} \rightarrow \mathbb{R} \\
   &amp; X \mapsto y = g(X)"></p> </div>
<p>GPML starts with the assumption that this function is <em>a</em> conditional sample path of <em>a</em> Gaussian process <img class="math" src="http://scikit-learn.org/stable/_images/math/dee63237674afc3154c2e33b7b68d67f85f2ee0a.png" alt="G"> which is additionally assumed to read as follows:</p> <div class="math"> <p><img src="http://scikit-learn.org/stable/_images/math/041d4c39282499c142df53da634fbca34bf0eaec.png" alt="G(X) = f(X)^T \beta + Z(X)"></p> </div>
<p>where <img class="math" src="http://scikit-learn.org/stable/_images/math/4bf3f79e592f44a0918ad6060416de45a8dea3b1.png" alt="f(X)^T \beta"> is a linear regression model and <img class="math" src="http://scikit-learn.org/stable/_images/math/42684295c2eb32524bd1d7ec928f9a968c554c9a.png" alt="Z(X)"> is a zero-mean Gaussian process with a fully stationary covariance function:</p> <div class="math"> <p><img src="http://scikit-learn.org/stable/_images/math/9d813e7d4f6a54efb73241b2847fc09b6d6b4577.png" alt="C(X, X') = \sigma^2 R(|X - X'|)"></p> </div>
<p><img class="math" src="http://scikit-learn.org/stable/_images/math/7a3d0c9264473a58cc3a769f99a662631131377c.png" alt="\sigma^2"> being its variance and <img class="math" src="http://scikit-learn.org/stable/_images/math/a00254b18ffa992f0ef19f6e6e095b83c8f85e94.png" alt="R"> being the correlation function which solely depends on the absolute relative distance between each sample, possibly featurewise (this is the stationarity assumption).</p> <p>From this basic formulation, note that GPML is nothing but an extension of a basic least squares linear regression problem:</p> <div class="math"> <p><img src="http://scikit-learn.org/stable/_images/math/c3d5bd15d3878ea8d4317646629723e693d14ab1.png" alt="g(X) \approx f(X)^T \beta"></p> </div>
<p>Except we additionally assume some spatial coherence (correlation) between the samples dictated by the correlation function. Indeed, ordinary least squares assumes the correlation model <img class="math" src="http://scikit-learn.org/stable/_images/math/7a42ce125aa465fa1885cd5405aec2aac4d360bc.png" alt="R(|X - X'|)"> is one when <img class="math" src="http://scikit-learn.org/stable/_images/math/1103e88c4e6e6e69dfca9fd3d110911904484bd7.png" alt="X = X'"> and zero otherwise : a <em>dirac</em> correlation model – sometimes referred to as a <em>nugget</em> correlation model in the kriging literature.</p>   <h4 id="the-best-linear-unbiased-prediction-blup">1.7.6.3.2. The best linear unbiased prediction (BLUP)</h4> <p>We now derive the <em>best linear unbiased prediction</em> of the sample path <img class="math" src="http://scikit-learn.org/stable/_images/math/307b3725cbb03398131f9ca542d79aff4933195f.png" alt="g"> conditioned on the observations:</p> <div class="math"> <p><img src="http://scikit-learn.org/stable/_images/math/63c9610b9dbef0e08c93d3d0f6606910749f7360.png" alt="\hat{G}(X) = G(X | y_1 = g(X_1), ...,
                            y_{n_{\rm samples}} = g(X_{n_{\rm samples}}))"></p> </div>
<p>It is derived from its <em>given properties</em>:</p> <ul class="simple"> <li>It is linear (a linear combination of the observations)</li> </ul> <div class="math"> <p><img src="http://scikit-learn.org/stable/_images/math/cee1832e9eb1d42f48aaa5751956b8ba529e23cd.png" alt="\hat{G}(X) \equiv a(X)^T y"></p> </div>
<ul class="simple"> <li>It is unbiased</li> </ul> <div class="math"> <p><img src="http://scikit-learn.org/stable/_images/math/f750e6bb18a235003a6a3bdef92bedca8c043ba9.png" alt="\mathbb{E}[G(X) - \hat{G}(X)] = 0"></p> </div>
<ul class="simple"> <li>It is the best (in the Mean Squared Error sense)</li> </ul> <div class="math"> <p><img src="http://scikit-learn.org/stable/_images/math/87701822cbf21bb28139288a2ab081911785ef58.png" alt="\hat{G}(X)^* = \arg \min\limits_{\hat{G}(X)} \;
                                        \mathbb{E}[(G(X) - \hat{G}(X))^2]"></p> </div>
<p>So that the optimal weight vector <img class="math" src="http://scikit-learn.org/stable/_images/math/d6b710f7b85b73b7fa48070ffd5a306128331a4e.png" alt="a(X)"> is solution of the following equality constrained optimization problem:</p> <div class="math"> <p><img src="http://scikit-learn.org/stable/_images/math/cdac62152599b18e381d5e60cb87cc2f31d4046e.png" alt="a(X)^* = \arg \min\limits_{a(X)} &amp; \; \mathbb{E}[(G(X) - a(X)^T y)^2] \\
                   {\rm s. t.} &amp; \; \mathbb{E}[G(X) - a(X)^T y] = 0"></p> </div>
<p>Rewriting this constrained optimization problem in the form of a Lagrangian and looking further for the first order optimality conditions to be satisfied, one ends up with a closed form expression for the sought predictor – see references for the complete proof.</p> <p>In the end, the BLUP is shown to be a Gaussian random variate with mean:</p> <div class="math"> <p><img src="http://scikit-learn.org/stable/_images/math/0de279ae0d207f7cd045fc0b2b14c24ffa0f99f9.png" alt="\mu_{\hat{Y}}(X) = f(X)^T\,\hat{\beta} + r(X)^T\,\gamma"></p> </div>
<p>and variance:</p> <div class="math"> <p><img src="http://scikit-learn.org/stable/_images/math/0f517571c98035dfcf5baf68cf7fa2b64a005657.png" alt="\sigma_{\hat{Y}}^2(X) = \sigma_{Y}^2\,
( 1
- r(X)^T\,R^{-1}\,r(X)
+ u(X)^T\,(F^T\,R^{-1}\,F)^{-1}\,u(X)
)"></p> </div>
<p>where we have introduced:</p> <ul class="simple"> <li>the correlation matrix whose terms are defined wrt the autocorrelation function and its built-in parameters <img class="math" src="http://scikit-learn.org/stable/_images/math/3be04d4207434584251f6921820c24ac9fa8c6f1.png" alt="\theta">:</li> </ul> <div class="math"> <p><img src="http://scikit-learn.org/stable/_images/math/1722557cac6e58298c0b2436e514891edf85f64e.png" alt="R_{i\,j} = R(|X_i - X_j|, \theta), \; i,\,j = 1, ..., m"></p> </div>
<ul class="simple"> <li>the vector of cross-correlations between the point where the prediction is made and the points in the DOE:</li> </ul> <div class="math"> <p><img src="http://scikit-learn.org/stable/_images/math/f3ccfb83a31f41f8829cbd986b5e589ed1b62795.png" alt="r_i = R(|X - X_i|, \theta), \; i = 1, ..., m"></p> </div>
<ul class="simple"> <li>the regression matrix (eg the Vandermonde matrix if <img class="math" src="http://scikit-learn.org/stable/_images/math/875eb40014526135383caa89fd500ae40a835f56.png" alt="f"> is a polynomial basis):</li> </ul> <div class="math"> <p><img src="http://scikit-learn.org/stable/_images/math/693dba60f07eadbda2ba980e37e419718ca37745.png" alt="F_{i\,j} = f_i(X_j), \; i = 1, ..., p, \, j = 1, ..., m"></p> </div>
<ul class="simple"> <li>the generalized least square regression weights:</li> </ul> <div class="math"> <p><img src="http://scikit-learn.org/stable/_images/math/ce6094ccf2ce46a4e9622a23e98ab4223884c84f.png" alt="\hat{\beta} =(F^T\,R^{-1}\,F)^{-1}\,F^T\,R^{-1}\,Y"></p> </div>
<ul class="simple"> <li>and the vectors:</li> </ul> <div class="math"> <p><img src="http://scikit-learn.org/stable/_images/math/cbdf7d1a061e7d00cd0186dd20f7a54724463cd1.png" alt="\gamma &amp; = R^{-1}(Y - F\,\hat{\beta}) \\
u(X) &amp; = F^T\,R^{-1}\,r(X) - f(X)"></p> </div>
<p>It is important to notice that the probabilistic response of a Gaussian Process predictor is fully analytic and mostly relies on basic linear algebra operations. More precisely the mean prediction is the sum of two simple linear combinations (dot products), and the variance requires two matrix inversions, but the correlation matrix can be decomposed only once using a Cholesky decomposition algorithm.</p>   <h4 id="the-empirical-best-linear-unbiased-predictor-eblup">1.7.6.3.3. The empirical best linear unbiased predictor (EBLUP)</h4> <p>Until now, both the autocorrelation and regression models were assumed given. In practice however they are never known in advance so that one has to make (motivated) empirical choices for these models <a class="reference internal" href="#correlation-models"><span class="std std-ref">Correlation Models</span></a>.</p> <p>Provided these choices are made, one should estimate the remaining unknown parameters involved in the BLUP. To do so, one uses the set of provided observations in conjunction with some inference technique. The present implementation, which is based on the DACE’s Matlab toolbox uses the <em>maximum likelihood estimation</em> technique – see DACE manual in references for the complete equations. This maximum likelihood estimation problem is turned into a global optimization problem onto the autocorrelation parameters. In the present implementation, this global optimization is solved by means of the fmin_cobyla optimization function from scipy.optimize. In the case of anisotropy however, we provide an implementation of Welch’s componentwise optimization algorithm – see references.</p>    <h3 id="id7">1.7.6.4. Correlation Models</h3> <p id="correlation-models">Common correlation models matches some famous SVM’s kernels because they are mostly built on equivalent assumptions. They must fulfill Mercer’s conditions and should additionally remain stationary. Note however, that the choice of the correlation model should be made in agreement with the known properties of the original experiment from which the observations come. For instance:</p> <ul class="simple"> <li>If the original experiment is known to be infinitely differentiable (smooth), then one should use the <em>squared-exponential correlation model</em>.</li> <li>If it’s not, then one should rather use the <em>exponential correlation model</em>.</li> <li>Note also that there exists a correlation model that takes the degree of derivability as input: this is the Matern correlation model, but it’s not implemented here (TODO).</li> </ul> <p>For a more detailed discussion on the selection of appropriate correlation models, see the book by Rasmussen &amp; Williams in references.</p>   <h3 id="id8">1.7.6.5. Regression Models</h3> <p id="regression-models">Common linear regression models involve zero- (constant), first- and second-order polynomials. But one may specify its own in the form of a Python function that takes the features X as input and that returns a vector containing the values of the functional set. The only constraint is that the number of functions must not exceed the number of available observations so that the underlying regression problem is not <em>underdetermined</em>.</p>   <h3 id="implementation-details">1.7.6.6. Implementation details</h3> <p>The implementation is based on a translation of the DACE Matlab toolbox.</p> <div class="topic"> <p class="topic-title first">References:</p> <ul class="simple"> <li>
<a class="reference external" href="http://imedea.uib-csic.es/master/cambioglobal/Modulo_V_cod101615/Lab/lab_maps/krigging/DACE-krigingsoft/dace/dace.pdf" target="_blank">DACE, A Matlab Kriging Toolbox</a> S Lophaven, HB Nielsen, J Sondergaard 2002,</li> <li>W.J. Welch, R.J. Buck, J. Sacks, H.P. Wynn, T.J. Mitchell, and M.D. Morris (1992). Screening, predicting, and computer experiments. Technometrics, 34(1) 15–25.</li> </ul> </div>
<div class="_attribution">
  <p class="_attribution-p">
    © 2007–2016 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="http://scikit-learn.org/stable/modules/gaussian_process.html" class="_attribution-link" target="_blank">http://scikit-learn.org/stable/modules/gaussian_process.html</a>
  </p>
</div>

			</div>
		</div>
	</section>

	</div>
</body>
</html>
