
<!DOCTYPE HTML>

<html lang="en">

<head>
  <meta charset="utf-8">
  <title>2.7. Novelty and Outlier Detection - Scikit-learn - W3cubDocs</title>
  
  <meta name="description" content="Many applications require being able to decide whether a new observation belongs to the same distribution as existing observations (it is an inlier &hellip;">
  <meta name="keywords" content="novelty, and, outlier, detection, -, scikit-learn, scikit_learn">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="http://docs.w3cub.com/scikit_learn/modules/outlier_detection/">
  <link href="/favicon.png" rel="icon">
  <link type="text/css" rel="stylesheet" href="/assets/application-50364fff564ce3b6327021805f3f00e2957b441cf27f576a7dd4ff63bbc47047.css">
  <script type="text/javascript" src="/assets/application-db64bfd54ceb42be11af7995804cf4902548419ceb79d509b0b7d62c22d98e6f.js"></script>
  <script src="/json/scikit_learn.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body>
	<div class="_app">
	<header class="_header">
  
  <form class="_search">
    <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
    <a class="_search-clear"></a>
    <div class="_search-tag"></div>
  </form>
  
  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/scikit_learn/" class="_nav-link" title="" style="margin-left:0;">scikit-learn</a></span>
  
  <nav class="_nav">
    <a href="/app/" class="_nav-link ">App</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		<div class="_list">
			
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<div class="_page _sphinx">
				
<h1 id="outlier-detection">2.7. Novelty and Outlier Detection</h1> <p id="novelty-and-outlier-detection">Many applications require being able to decide whether a new observation belongs to the same distribution as existing observations (it is an <code>inlier</code>), or should be considered as different (it is an outlier). Often, this ability is used to clean real data sets. Two important distinction must be made:</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr class="field-odd field"><th class="field-name" colspan="2">novelty detection:</th></tr> <tr class="field-odd field">
<td> </td>
<td class="field-body">The training data is not polluted by outliers, and we are interested in detecting anomalies in new observations.</td> </tr> <tr class="field-even field"><th class="field-name" colspan="2">outlier detection:</th></tr> <tr class="field-even field">
<td> </td>
<td class="field-body">The training data contains outliers, and we need to fit the central mode of the training data, ignoring the deviant observations.</td> </tr>  </table> <p>The scikit-learn project provides a set of machine learning tools that can be used both for novelty or outliers detection. This strategy is implemented with objects learning in an unsupervised way from the data:</p> <pre data-language="python">estimator.fit(X_train)
</pre> <p>new observations can then be sorted as inliers or outliers with a <code>predict</code> method:</p> <pre data-language="python">estimator.predict(X_test)
</pre> <p>Inliers are labeled 1, while outliers are labeled -1.</p>  <h2 id="novelty-detection">2.7.1. Novelty Detection</h2> <p>Consider a data set of <img class="math" src="http://scikit-learn.org/stable/_images/math/e11f2701c4a39c7fe543a6c4150b421d50f1c159.png" alt="n"> observations from the same distribution described by <img class="math" src="http://scikit-learn.org/stable/_images/math/27d463da4622be5b3ef1d4176ced7d7a323c6425.png" alt="p"> features. Consider now that we add one more observation to that data set. Is the new observation so different from the others that we can doubt it is regular? (i.e. does it come from the same distribution?) Or on the contrary, is it so similar to the other that we cannot distinguish it from the original observations? This is the question addressed by the novelty detection tools and methods.</p> <p>In general, it is about to learn a rough, close frontier delimiting the contour of the initial observations distribution, plotted in embedding <img class="math" src="http://scikit-learn.org/stable/_images/math/27d463da4622be5b3ef1d4176ced7d7a323c6425.png" alt="p">-dimensional space. Then, if further observations lay within the frontier-delimited subspace, they are considered as coming from the same population than the initial observations. Otherwise, if they lay outside the frontier, we can say that they are abnormal with a given confidence in our assessment.</p> <p>The One-Class SVM has been introduced by Schölkopf et al. for that purpose and implemented in the <a class="reference internal" href="../svm/#svm"><span class="std std-ref">Support Vector Machines</span></a> module in the <a class="reference internal" href="../generated/sklearn.svm.oneclasssvm/#sklearn.svm.OneClassSVM" title="sklearn.svm.OneClassSVM"><code>svm.OneClassSVM</code></a> object. It requires the choice of a kernel and a scalar parameter to define a frontier. The RBF kernel is usually chosen although there exists no exact formula or algorithm to set its bandwidth parameter. This is the default in the scikit-learn implementation. The <img class="math" src="http://scikit-learn.org/stable/_images/math/ca3b8fa4180eee2dfc3af9d13fae1da451cd2c31.png" alt="\nu"> parameter, also known as the margin of the One-Class SVM, corresponds to the probability of finding a new, but regular, observation outside the frontier.</p> <div class="topic"> <p class="topic-title first">References:</p> <ul class="simple"> <li>
<a class="reference external" href="http://dl.acm.org/citation.cfm?id=1119749" target="_blank">Estimating the support of a high-dimensional distribution</a> Schölkopf, Bernhard, et al. Neural computation 13.7 (2001): 1443-1471.</li> </ul> </div> <div class="topic"> <p class="topic-title first">Examples:</p> <ul class="simple"> <li>See <a class="reference internal" href="../../auto_examples/svm/plot_oneclass/#sphx-glr-auto-examples-svm-plot-oneclass-py"><span class="std std-ref">One-class SVM with non-linear kernel (RBF)</span></a> for visualizing the frontier learned around some data by a <a class="reference internal" href="../generated/sklearn.svm.oneclasssvm/#sklearn.svm.OneClassSVM" title="sklearn.svm.OneClassSVM"><code>svm.OneClassSVM</code></a> object.</li> </ul> </div> <div class="figure align-center"> <a class="reference external image-reference" href="../../auto_examples/svm/plot_oneclass/"><img alt="../_images/sphx_glr_plot_oneclass_0011.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_oneclass_0011.png" style="width: 600.0px; height: 450.0px;"></a> </div>   <h2 id="id1">2.7.2. Outlier Detection</h2> <p>Outlier detection is similar to novelty detection in the sense that the goal is to separate a core of regular observations from some polluting ones, called “outliers”. Yet, in the case of outlier detection, we don’t have a clean data set representing the population of regular observations that can be used to train any tool.</p>  <h3 id="fitting-an-elliptic-envelope">2.7.2.1. Fitting an elliptic envelope</h3> <p>One common way of performing outlier detection is to assume that the regular data come from a known distribution (e.g. data are Gaussian distributed). From this assumption, we generally try to define the “shape” of the data, and can define outlying observations as observations which stand far enough from the fit shape.</p> <p>The scikit-learn provides an object <a class="reference internal" href="../generated/sklearn.covariance.ellipticenvelope/#sklearn.covariance.EllipticEnvelope" title="sklearn.covariance.EllipticEnvelope"><code>covariance.EllipticEnvelope</code></a> that fits a robust covariance estimate to the data, and thus fits an ellipse to the central data points, ignoring points outside the central mode.</p> <p>For instance, assuming that the inlier data are Gaussian distributed, it will estimate the inlier location and covariance in a robust way (i.e. whithout being influenced by outliers). The Mahalanobis distances obtained from this estimate is used to derive a measure of outlyingness. This strategy is illustrated below.</p> <div class="figure align-center"> <a class="reference external image-reference" href="../../auto_examples/covariance/plot_mahalanobis_distances/"><img alt="../_images/sphx_glr_plot_mahalanobis_distances_0011.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_mahalanobis_distances_0011.png" style="width: 600.0px; height: 450.0px;"></a> </div> <div class="topic"> <p class="topic-title first">Examples:</p> <ul class="simple"> <li>See <a class="reference internal" href="../../auto_examples/covariance/plot_mahalanobis_distances/#sphx-glr-auto-examples-covariance-plot-mahalanobis-distances-py"><span class="std std-ref">Robust covariance estimation and Mahalanobis distances relevance</span></a> for an illustration of the difference between using a standard (<a class="reference internal" href="../generated/sklearn.covariance.empiricalcovariance/#sklearn.covariance.EmpiricalCovariance" title="sklearn.covariance.EmpiricalCovariance"><code>covariance.EmpiricalCovariance</code></a>) or a robust estimate (<a class="reference internal" href="../generated/sklearn.covariance.mincovdet/#sklearn.covariance.MinCovDet" title="sklearn.covariance.MinCovDet"><code>covariance.MinCovDet</code></a>) of location and covariance to assess the degree of outlyingness of an observation.</li> </ul> </div> <div class="topic"> <p class="topic-title first">References:</p> <table class="docutils citation" frame="void" id="rd1999" rules="none">   <tr>
<td class="label">[RD1999]</td>
<td>Rousseeuw, P.J., Van Driessen, K. “A fast algorithm for the minimum covariance determinant estimator” Technometrics 41(3), 212 (1999)</td>
</tr>  </table> </div>   <h3 id="id2">2.7.2.2. Isolation Forest</h3> <p id="isolation-forest">One efficient way of performing outlier detection in high-dimensional datasets is to use random forests. The <a class="reference internal" href="../generated/sklearn.ensemble.isolationforest/#sklearn.ensemble.IsolationForest" title="sklearn.ensemble.IsolationForest"><code>ensemble.IsolationForest</code></a> ‘isolates’ observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature.</p> <p>Since recursive partitioning can be represented by a tree structure, the number of splittings required to isolate a sample is equivalent to the path length from the root node to the terminating node.</p> <p>This path length, averaged over a forest of such random trees, is a measure of abnormality and our decision function.</p> <p>Random partitioning produces noticeably shorter paths for anomalies. Hence, when a forest of random trees collectively produce shorter path lengths for particular samples, they are highly likely to be anomalies.</p> <p>This strategy is illustrated below.</p> <div class="figure align-center"> <a class="reference external image-reference" href="../../auto_examples/ensemble/plot_isolation_forest/"><img alt="../_images/sphx_glr_plot_isolation_forest_0011.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_isolation_forest_0011.png" style="width: 600.0px; height: 450.0px;"></a> </div> <div class="topic"> <p class="topic-title first">Examples:</p> <ul class="simple"> <li>See <a class="reference internal" href="../../auto_examples/ensemble/plot_isolation_forest/#sphx-glr-auto-examples-ensemble-plot-isolation-forest-py"><span class="std std-ref">IsolationForest example</span></a> for an illustration of the use of IsolationForest.</li> <li>See <a class="reference internal" href="../../auto_examples/covariance/plot_outlier_detection/#sphx-glr-auto-examples-covariance-plot-outlier-detection-py"><span class="std std-ref">Outlier detection with several methods.</span></a> for a comparison of <a class="reference internal" href="../generated/sklearn.ensemble.isolationforest/#sklearn.ensemble.IsolationForest" title="sklearn.ensemble.IsolationForest"><code>ensemble.IsolationForest</code></a> with <a class="reference internal" href="../generated/sklearn.svm.oneclasssvm/#sklearn.svm.OneClassSVM" title="sklearn.svm.OneClassSVM"><code>svm.OneClassSVM</code></a> (tuned to perform like an outlier detection method) and a covariance-based outlier detection with <a class="reference internal" href="../generated/sklearn.covariance.mincovdet/#sklearn.covariance.MinCovDet" title="sklearn.covariance.MinCovDet"><code>covariance.MinCovDet</code></a>.</li> </ul> </div> <div class="topic"> <p class="topic-title first">References:</p> <table class="docutils citation" frame="void" id="ltz2008" rules="none">   <tr>
<td class="label">[LTZ2008]</td>
<td>Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. “Isolation forest.” Data Mining, 2008. ICDM‘08. Eighth IEEE International Conference on.</td>
</tr>  </table> </div>   <h3 id="one-class-svm-versus-elliptic-envelope-versus-isolation-forest">2.7.2.3. One-class SVM versus Elliptic Envelope versus Isolation Forest</h3> <p>Strictly-speaking, the One-class SVM is not an outlier-detection method, but a novelty-detection method: its training set should not be contaminated by outliers as it may fit them. That said, outlier detection in high-dimension, or without any assumptions on the distribution of the inlying data is very challenging, and a One-class SVM gives useful results in these situations.</p> <p>The examples below illustrate how the performance of the <a class="reference internal" href="../generated/sklearn.covariance.ellipticenvelope/#sklearn.covariance.EllipticEnvelope" title="sklearn.covariance.EllipticEnvelope"><code>covariance.EllipticEnvelope</code></a> degrades as the data is less and less unimodal. The <a class="reference internal" href="../generated/sklearn.svm.oneclasssvm/#sklearn.svm.OneClassSVM" title="sklearn.svm.OneClassSVM"><code>svm.OneClassSVM</code></a> works better on data with multiple modes and <a class="reference internal" href="../generated/sklearn.ensemble.isolationforest/#sklearn.ensemble.IsolationForest" title="sklearn.ensemble.IsolationForest"><code>ensemble.IsolationForest</code></a> performs well in every cases.</p> <table class="docutils" id="id3"> <caption><span class="caption-text"><strong>Comparing One-class SVM approach, and elliptic envelope</strong></span></caption>   <tr class="row-odd">
<td>For a inlier mode well-centered and elliptic, the <a class="reference internal" href="../generated/sklearn.svm.oneclasssvm/#sklearn.svm.OneClassSVM" title="sklearn.svm.OneClassSVM"><code>svm.OneClassSVM</code></a> is not able to benefit from the rotational symmetry of the inlier population. In addition, it fits a bit the outliers present in the training set. On the opposite, the decision rule based on fitting an <a class="reference internal" href="../generated/sklearn.covariance.ellipticenvelope/#sklearn.covariance.EllipticEnvelope" title="sklearn.covariance.EllipticEnvelope"><code>covariance.EllipticEnvelope</code></a> learns an ellipse, which fits well the inlier distribution. The <a class="reference internal" href="../generated/sklearn.ensemble.isolationforest/#sklearn.ensemble.IsolationForest" title="sklearn.ensemble.IsolationForest"><code>ensemble.IsolationForest</code></a> performs as well.</td> <td><a class="reference external" href="../../auto_examples/covariance/plot_outlier_detection/"><img alt="outlier1" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_outlier_detection_0011.png" style="width: 540.0px; height: 180.0px;"></a></td> </tr> <tr class="row-even">
<td>As the inlier distribution becomes bimodal, the <a class="reference internal" href="../generated/sklearn.covariance.ellipticenvelope/#sklearn.covariance.EllipticEnvelope" title="sklearn.covariance.EllipticEnvelope"><code>covariance.EllipticEnvelope</code></a> does not fit well the inliers. However, we can see that both <a class="reference internal" href="../generated/sklearn.ensemble.isolationforest/#sklearn.ensemble.IsolationForest" title="sklearn.ensemble.IsolationForest"><code>ensemble.IsolationForest</code></a> and <a class="reference internal" href="../generated/sklearn.svm.oneclasssvm/#sklearn.svm.OneClassSVM" title="sklearn.svm.OneClassSVM"><code>svm.OneClassSVM</code></a> have difficulties to detect the two modes, and that the <a class="reference internal" href="../generated/sklearn.svm.oneclasssvm/#sklearn.svm.OneClassSVM" title="sklearn.svm.OneClassSVM"><code>svm.OneClassSVM</code></a> tends to overfit: because it has not model of inliers, it interprets a region where, by chance some outliers are clustered, as inliers.</td> <td><a class="reference external" href="../../auto_examples/covariance/plot_outlier_detection/"><img alt="outlier2" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_outlier_detection_0021.png" style="width: 540.0px; height: 180.0px;"></a></td> </tr> <tr class="row-odd">
<td>If the inlier distribution is strongly non Gaussian, the <a class="reference internal" href="../generated/sklearn.svm.oneclasssvm/#sklearn.svm.OneClassSVM" title="sklearn.svm.OneClassSVM"><code>svm.OneClassSVM</code></a> is able to recover a reasonable approximation as well as <a class="reference internal" href="../generated/sklearn.ensemble.isolationforest/#sklearn.ensemble.IsolationForest" title="sklearn.ensemble.IsolationForest"><code>ensemble.IsolationForest</code></a>, whereas the <a class="reference internal" href="../generated/sklearn.covariance.ellipticenvelope/#sklearn.covariance.EllipticEnvelope" title="sklearn.covariance.EllipticEnvelope"><code>covariance.EllipticEnvelope</code></a> completely fails.</td> <td><a class="reference external" href="../../auto_examples/covariance/plot_outlier_detection/"><img alt="outlier3" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_outlier_detection_0031.png" style="width: 540.0px; height: 180.0px;"></a></td> </tr>  </table> <div class="topic"> <p class="topic-title first">Examples:</p> <ul class="simple"> <li>See <a class="reference internal" href="../../auto_examples/covariance/plot_outlier_detection/#sphx-glr-auto-examples-covariance-plot-outlier-detection-py"><span class="std std-ref">Outlier detection with several methods.</span></a> for a comparison of the <a class="reference internal" href="../generated/sklearn.svm.oneclasssvm/#sklearn.svm.OneClassSVM" title="sklearn.svm.OneClassSVM"><code>svm.OneClassSVM</code></a> (tuned to perform like an outlier detection method), the <a class="reference internal" href="../generated/sklearn.ensemble.isolationforest/#sklearn.ensemble.IsolationForest" title="sklearn.ensemble.IsolationForest"><code>ensemble.IsolationForest</code></a> and a covariance-based outlier detection with <a class="reference internal" href="../generated/sklearn.covariance.mincovdet/#sklearn.covariance.MinCovDet" title="sklearn.covariance.MinCovDet"><code>covariance.MinCovDet</code></a>.</li> </ul> </div>
<div class="_attribution">
  <p class="_attribution-p">
    © 2007–2016 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="http://scikit-learn.org/stable/modules/outlier_detection.html" class="_attribution-link" target="_blank">http://scikit-learn.org/stable/modules/outlier_detection.html</a>
  </p>
</div>

			</div>
		</div>
	</section>

	</div>
</body>
</html>
