
<!DOCTYPE HTML>

<html lang="en">

<head>
  <meta charset="utf-8">
  <title>6. Strategies to Scale Computationally - Scikit-learn - W3cubDocs</title>
  
  <meta name="description" content="For some applications the amount of examples, features (or both) and&#47;or the speed at which they need to be processed are challenging for &hellip;">
  <meta name="keywords" content="strategies, scale, computationally, bigger, data, -, scikit-learn, scikit_learn">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="http://docs.w3cub.com/scikit_learn/modules/scaling_strategies/">
  <link href="/favicon.png" rel="icon">
  <link type="text/css" rel="stylesheet" href="/assets/application-50364fff564ce3b6327021805f3f00e2957b441cf27f576a7dd4ff63bbc47047.css">
  <script type="text/javascript" src="/assets/application-db64bfd54ceb42be11af7995804cf4902548419ceb79d509b0b7d62c22d98e6f.js"></script>
  <script src="/json/scikit_learn.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body>
	<div class="_app">
	<header class="_header">
  
  <form class="_search">
    <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
    <a class="_search-clear"></a>
    <div class="_search-tag"></div>
  </form>
  
  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/scikit_learn/" class="_nav-link" title="" style="margin-left:0;">scikit-learn</a></span>
  
  <nav class="_nav">
    <a href="/app/" class="_nav-link ">App</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		<div class="_list">
			
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<div class="_page _sphinx">
				
<h1 id="scaling-strategies">6. Strategies to scale computationally: bigger data</h1> <p id="strategies-to-scale-computationally-bigger-data">For some applications the amount of examples, features (or both) and/or the speed at which they need to be processed are challenging for traditional approaches. In these cases scikit-learn has a number of options you can consider to make your system scale.</p>  <h2 id="scaling-with-instances-using-out-of-core-learning">6.1. Scaling with instances using out-of-core learning</h2> <p>Out-of-core (or “external memory”) learning is a technique used to learn from data that cannot fit in a computer’s main memory (RAM).</p> <p>Here is sketch of a system designed to achieve this goal:</p>  <ol class="arabic simple"> <li>a way to stream instances</li> <li>a way to extract features from instances</li> <li>an incremental algorithm</li> </ol>   <h3 id="streaming-instances">6.1.1. Streaming instances</h3> <p>Basically, 1. may be a reader that yields instances from files on a hard drive, a database, from a network stream etc. However, details on how to achieve this are beyond the scope of this documentation.</p>   <h3 id="extracting-features">6.1.2. Extracting features</h3> <p>2. could be any relevant way to extract features among the different <a class="reference internal" href="../feature_extraction/#feature-extraction"><span class="std std-ref">feature extraction</span></a> methods supported by scikit-learn. However, when working with data that needs vectorization and where the set of features or values is not known in advance one should take explicit care. A good example is text classification where unknown terms are likely to be found during training. It is possible to use a statefull vectorizer if making multiple passes over the data is reasonable from an application point of view. Otherwise, one can turn up the difficulty by using a stateless feature extractor. Currently the preferred way to do this is to use the so-called <a class="reference internal" href="../feature_extraction/#feature-hashing"><span class="std std-ref">hashing trick</span></a> as implemented by <a class="reference internal" href="../generated/sklearn.feature_extraction.featurehasher/#sklearn.feature_extraction.FeatureHasher" title="sklearn.feature_extraction.FeatureHasher"><code>sklearn.feature_extraction.FeatureHasher</code></a> for datasets with categorical variables represented as list of Python dicts or <a class="reference internal" href="../generated/sklearn.feature_extraction.text.hashingvectorizer/#sklearn.feature_extraction.text.HashingVectorizer" title="sklearn.feature_extraction.text.HashingVectorizer"><code>sklearn.feature_extraction.text.HashingVectorizer</code></a> for text documents.</p>   <h3 id="incremental-learning">6.1.3. Incremental learning</h3> <p>Finally, for 3. we have a number of options inside scikit-learn. Although all algorithms cannot learn incrementally (i.e. without seeing all the instances at once), all estimators implementing the <code>partial_fit</code> API are candidates. Actually, the ability to learn incrementally from a mini-batch of instances (sometimes called “online learning”) is key to out-of-core learning as it guarantees that at any given time there will be only a small amount of instances in the main memory. Choosing a good size for the mini-batch that balances relevancy and memory footprint could involve some tuning <a class="footnote-reference" href="#id2" id="id1">[1]</a>.</p> <p>Here is a list of incremental estimators for different tasks:</p>  <ul> <li>
<dl class="first docutils"> <dt>Classification</dt> <dd>
<ul class="first last simple"> <li><a class="reference internal" href="../generated/sklearn.naive_bayes.multinomialnb/#sklearn.naive_bayes.MultinomialNB" title="sklearn.naive_bayes.MultinomialNB"><code>sklearn.naive_bayes.MultinomialNB</code></a></li> <li><a class="reference internal" href="../generated/sklearn.naive_bayes.bernoullinb/#sklearn.naive_bayes.BernoulliNB" title="sklearn.naive_bayes.BernoulliNB"><code>sklearn.naive_bayes.BernoulliNB</code></a></li> <li><a class="reference internal" href="../generated/sklearn.linear_model.perceptron/#sklearn.linear_model.Perceptron" title="sklearn.linear_model.Perceptron"><code>sklearn.linear_model.Perceptron</code></a></li> <li><a class="reference internal" href="../generated/sklearn.linear_model.sgdclassifier/#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code>sklearn.linear_model.SGDClassifier</code></a></li> <li><a class="reference internal" href="../generated/sklearn.linear_model.passiveaggressiveclassifier/#sklearn.linear_model.PassiveAggressiveClassifier" title="sklearn.linear_model.PassiveAggressiveClassifier"><code>sklearn.linear_model.PassiveAggressiveClassifier</code></a></li> </ul> </dd> </dl> </li> <li>
<dl class="first docutils"> <dt>Regression</dt> <dd>
<ul class="first last simple"> <li><a class="reference internal" href="../generated/sklearn.linear_model.sgdregressor/#sklearn.linear_model.SGDRegressor" title="sklearn.linear_model.SGDRegressor"><code>sklearn.linear_model.SGDRegressor</code></a></li> <li><a class="reference internal" href="../generated/sklearn.linear_model.passiveaggressiveregressor/#sklearn.linear_model.PassiveAggressiveRegressor" title="sklearn.linear_model.PassiveAggressiveRegressor"><code>sklearn.linear_model.PassiveAggressiveRegressor</code></a></li> </ul> </dd> </dl> </li> <li>
<dl class="first docutils"> <dt>Clustering</dt> <dd>
<ul class="first last simple"> <li><a class="reference internal" href="../generated/sklearn.cluster.minibatchkmeans/#sklearn.cluster.MiniBatchKMeans" title="sklearn.cluster.MiniBatchKMeans"><code>sklearn.cluster.MiniBatchKMeans</code></a></li> </ul> </dd> </dl> </li> <li>
<dl class="first docutils"> <dt>Decomposition / feature Extraction</dt> <dd>
<ul class="first last simple"> <li><a class="reference internal" href="../generated/sklearn.decomposition.minibatchdictionarylearning/#sklearn.decomposition.MiniBatchDictionaryLearning" title="sklearn.decomposition.MiniBatchDictionaryLearning"><code>sklearn.decomposition.MiniBatchDictionaryLearning</code></a></li> <li><a class="reference internal" href="../generated/sklearn.decomposition.incrementalpca/#sklearn.decomposition.IncrementalPCA" title="sklearn.decomposition.IncrementalPCA"><code>sklearn.decomposition.IncrementalPCA</code></a></li> <li><a class="reference internal" href="../generated/sklearn.decomposition.latentdirichletallocation/#sklearn.decomposition.LatentDirichletAllocation" title="sklearn.decomposition.LatentDirichletAllocation"><code>sklearn.decomposition.LatentDirichletAllocation</code></a></li> <li><a class="reference internal" href="../generated/sklearn.cluster.minibatchkmeans/#sklearn.cluster.MiniBatchKMeans" title="sklearn.cluster.MiniBatchKMeans"><code>sklearn.cluster.MiniBatchKMeans</code></a></li> </ul> </dd> </dl> </li> </ul>  <p>For classification, a somewhat important thing to note is that although a stateless feature extraction routine may be able to cope with new/unseen attributes, the incremental learner itself may be unable to cope with new/unseen targets classes. In this case you have to pass all the possible classes to the first <code>partial_fit</code> call using the <code>classes=</code> parameter.</p> <p>Another aspect to consider when choosing a proper algorithm is that all of them don’t put the same importance on each example over time. Namely, the <code>Perceptron</code> is still sensitive to badly labeled examples even after many examples whereas the <code>SGD*</code> and <code>PassiveAggressive*</code> families are more robust to this kind of artifacts. Conversely, the later also tend to give less importance to remarkably different, yet properly labeled examples when they come late in the stream as their learning rate decreases over time.</p>   <h3 id="examples">6.1.4. Examples</h3> <p>Finally, we have a full-fledged example of <a class="reference internal" href="../../auto_examples/applications/plot_out_of_core_classification/#sphx-glr-auto-examples-applications-plot-out-of-core-classification-py"><span class="std std-ref">Out-of-core classification of text documents</span></a>. It is aimed at providing a starting point for people wanting to build out-of-core learning systems and demonstrates most of the notions discussed above.</p> <p>Furthermore, it also shows the evolution of the performance of different algorithms with the number of processed examples.</p> <p class="centered"> <strong><a class="reference external" href="../../auto_examples/applications/plot_out_of_core_classification/"><img alt="accuracy_over_time" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_out_of_core_classification_0011.png" style="width: 640.0px; height: 480.0px;"></a></strong></p>
<p>Now looking at the computation time of the different parts, we see that the vectorization is much more expensive than learning itself. From the different algorithms, <code>MultinomialNB</code> is the most expensive, but its overhead can be mitigated by increasing the size of the mini-batches (exercise: change <code>minibatch_size</code> to 100 and 10000 in the program and compare).</p> <p class="centered"> <strong><a class="reference external" href="../../auto_examples/applications/plot_out_of_core_classification/"><img alt="computation_time" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_out_of_core_classification_0031.png" style="width: 640.0px; height: 480.0px;"></a></strong></p>  <h3 id="notes">6.1.5. Notes</h3> <table class="docutils footnote" frame="void" id="id2" rules="none">   <tr>
<td class="label"><a class="fn-backref" href="#id1">[1]</a></td>
<td>Depending on the algorithm the mini-batch size can influence results or not. SGD*, PassiveAggressive*, and discrete NaiveBayes are truly online and are not affected by batch size. Conversely, MiniBatchKMeans convergence rate is affected by the batch size. Also, its memory footprint can vary dramatically with batch size.</td>
</tr>  </table>
<div class="_attribution">
  <p class="_attribution-p">
    © 2007–2016 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="http://scikit-learn.org/stable/modules/scaling_strategies.html" class="_attribution-link" target="_blank">http://scikit-learn.org/stable/modules/scaling_strategies.html</a>
  </p>
</div>

			</div>
		</div>
	</section>

	</div>
</body>
</html>
