
<!DOCTYPE HTML>

<html lang="en">

<head>
  <meta charset="utf-8">
  <title>3.5. Validation Curves - Scikit-learn - W3cubDocs</title>
  
  <meta name="description" content="Every estimator has its advantages and drawbacks. Its generalization error can be decomposed in terms of bias, variance and noise. The bias of an &hellip;">
  <meta name="keywords" content="validation, curves, plotting, scores, evaluate, models, -, scikit-learn, scikit_learn">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="http://docs.w3cub.com/scikit_learn/modules/learning_curve/">
  <link href="/favicon.png" rel="icon">
  <link type="text/css" rel="stylesheet" href="/assets/application-50364fff564ce3b6327021805f3f00e2957b441cf27f576a7dd4ff63bbc47047.css">
  <script type="text/javascript" src="/assets/application-db64bfd54ceb42be11af7995804cf4902548419ceb79d509b0b7d62c22d98e6f.js"></script>
  <script src="/json/scikit_learn.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body>
	<div class="_app">
	<header class="_header">
  
  <form class="_search">
    <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
    <a class="_search-clear"></a>
    <div class="_search-tag"></div>
  </form>
  
  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/scikit_learn/" class="_nav-link" title="" style="margin-left:0;">scikit-learn</a></span>
  
  <nav class="_nav">
    <a href="/app/" class="_nav-link ">App</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		<div class="_list">
			
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<div class="_page _sphinx">
				
<h1 id="learning-curves">3.5. Validation curves: plotting scores to evaluate models</h1> <p id="validation-curves-plotting-scores-to-evaluate-models">Every estimator has its advantages and drawbacks. Its generalization error can be decomposed in terms of bias, variance and noise. The <strong>bias</strong> of an estimator is its average error for different training sets. The <strong>variance</strong> of an estimator indicates how sensitive it is to varying training sets. Noise is a property of the data.</p> <p>In the following plot, we see a function <img class="math" src="http://scikit-learn.org/stable/_images/math/ca95547ed61b0a4590e7ce2865ee70fe3cbec299.png" alt="f(x) = \cos (\frac{3}{2} \pi x)"> and some noisy samples from that function. We use three different estimators to fit the function: linear regression with polynomial features of degree 1, 4 and 15. We see that the first estimator can at best provide only a poor fit to the samples and the true function because it is too simple (high bias), the second estimator approximates it almost perfectly and the last estimator approximates the training data perfectly but does not fit the true function very well, i.e. it is very sensitive to varying training data (high variance).</p> <div class="figure align-center"> <a class="reference external image-reference" href="../../auto_examples/model_selection/plot_underfitting_overfitting/"><img alt="../_images/sphx_glr_plot_underfitting_overfitting_0011.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_underfitting_overfitting_0011.png" style="width: 700.0px; height: 250.0px;"></a> </div> <p>Bias and variance are inherent properties of estimators and we usually have to select learning algorithms and hyperparameters so that both bias and variance are as low as possible (see <a class="reference external" href="https://en.wikipedia.org/wiki/Bias-variance_dilemma" target="_blank">Bias-variance dilemma</a>). Another way to reduce the variance of a model is to use more training data. However, you should only collect more training data if the true function is too complex to be approximated by an estimator with a lower variance.</p> <p>In the simple one-dimensional problem that we have seen in the example it is easy to see whether the estimator suffers from bias or variance. However, in high-dimensional spaces, models can become very difficult to visualize. For this reason, it is often helpful to use the tools described below.</p> <div class="topic"> <p class="topic-title first">Examples:</p> <ul class="simple"> <li><a class="reference internal" href="../../auto_examples/model_selection/plot_underfitting_overfitting/#sphx-glr-auto-examples-model-selection-plot-underfitting-overfitting-py"><span class="std std-ref">Underfitting vs. Overfitting</span></a></li> <li><a class="reference internal" href="../../auto_examples/model_selection/plot_validation_curve/#sphx-glr-auto-examples-model-selection-plot-validation-curve-py"><span class="std std-ref">Plotting Validation Curves</span></a></li> <li><a class="reference internal" href="../../auto_examples/model_selection/plot_learning_curve/#sphx-glr-auto-examples-model-selection-plot-learning-curve-py"><span class="std std-ref">Plotting Learning Curves</span></a></li> </ul> </div>  <h2 id="id1">3.5.1. Validation curve</h2> <p id="validation-curve">To validate a model we need a scoring function (see <a class="reference internal" href="../model_evaluation/#model-evaluation"><span class="std std-ref">Model evaluation: quantifying the quality of predictions</span></a>), for example accuracy for classifiers. The proper way of choosing multiple hyperparameters of an estimator are of course grid search or similar methods (see <a class="reference internal" href="../grid_search/#grid-search"><span class="std std-ref">Tuning the hyper-parameters of an estimator</span></a>) that select the hyperparameter with the maximum score on a validation set or multiple validation sets. Note that if we optimized the hyperparameters based on a validation score the validation score is biased and not a good estimate of the generalization any longer. To get a proper estimate of the generalization we have to compute the score on another test set.</p> <p>However, it is sometimes helpful to plot the influence of a single hyperparameter on the training score and the validation score to find out whether the estimator is overfitting or underfitting for some hyperparameter values.</p> <p>The function <a class="reference internal" href="../generated/sklearn.model_selection.validation_curve/#sklearn.model_selection.validation_curve" title="sklearn.model_selection.validation_curve"><code>validation_curve</code></a> can help in this case:</p> <pre data-language="python">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from sklearn.model_selection import validation_curve
&gt;&gt;&gt; from sklearn.datasets import load_iris
&gt;&gt;&gt; from sklearn.linear_model import Ridge

&gt;&gt;&gt; np.random.seed(0)
&gt;&gt;&gt; iris = load_iris()
&gt;&gt;&gt; X, y = iris.data, iris.target
&gt;&gt;&gt; indices = np.arange(y.shape[0])
&gt;&gt;&gt; np.random.shuffle(indices)
&gt;&gt;&gt; X, y = X[indices], y[indices]

&gt;&gt;&gt; train_scores, valid_scores = validation_curve(Ridge(), X, y, "alpha",
...                                               np.logspace(-7, 3, 3))
&gt;&gt;&gt; train_scores           
array([[ 0.94...,  0.92...,  0.92...],
       [ 0.94...,  0.92...,  0.92...],
       [ 0.47...,  0.45...,  0.42...]])
&gt;&gt;&gt; valid_scores           
array([[ 0.90...,  0.92...,  0.94...],
       [ 0.90...,  0.92...,  0.94...],
       [ 0.44...,  0.39...,  0.45...]])
</pre> <p>If the training score and the validation score are both low, the estimator will be underfitting. If the training score is high and the validation score is low, the estimator is overfitting and otherwise it is working very well. A low training score and a high validation score is usually not possible. All three cases can be found in the plot below where we vary the parameter <img class="math" src="http://scikit-learn.org/stable/_images/math/3666981dc77862de77b6ecfcb64aad59b425cbaf.png" alt="\gamma"> of an SVM on the digits dataset.</p> <div class="figure align-center"> <a class="reference external image-reference" href="../../auto_examples/model_selection/plot_validation_curve/"><img alt="../_images/sphx_glr_plot_validation_curve_0011.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_validation_curve_0011.png" style="width: 400.0px; height: 300.0px;"></a> </div>   <h2 id="id2">3.5.2. Learning curve</h2> <p id="learning-curve">A learning curve shows the validation and training score of an estimator for varying numbers of training samples. It is a tool to find out how much we benefit from adding more training data and whether the estimator suffers more from a variance error or a bias error. If both the validation score and the training score converge to a value that is too low with increasing size of the training set, we will not benefit much from more training data. In the following plot you can see an example: naive Bayes roughly converges to a low score.</p> <div class="figure align-center"> <a class="reference external image-reference" href="../../auto_examples/model_selection/plot_learning_curve/"><img alt="../_images/sphx_glr_plot_learning_curve_0011.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_learning_curve_0011.png" style="width: 400.0px; height: 300.0px;"></a> </div> <p>We will probably have to use an estimator or a parametrization of the current estimator that can learn more complex concepts (i.e. has a lower bias). If the training score is much greater than the validation score for the maximum number of training samples, adding more training samples will most likely increase generalization. In the following plot you can see that the SVM could benefit from more training examples.</p> <div class="figure align-center"> <a class="reference external image-reference" href="../../auto_examples/model_selection/plot_learning_curve/"><img alt="../_images/sphx_glr_plot_learning_curve_0021.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_learning_curve_0021.png" style="width: 400.0px; height: 300.0px;"></a> </div> <p>We can use the function <a class="reference internal" href="../generated/sklearn.model_selection.learning_curve/#sklearn.model_selection.learning_curve" title="sklearn.model_selection.learning_curve"><code>learning_curve</code></a> to generate the values that are required to plot such a learning curve (number of samples that have been used, the average scores on the training sets and the average scores on the validation sets):</p> <pre data-language="python">&gt;&gt;&gt; from sklearn.model_selection import learning_curve
&gt;&gt;&gt; from sklearn.svm import SVC

&gt;&gt;&gt; train_sizes, train_scores, valid_scores = learning_curve(
...     SVC(kernel='linear'), X, y, train_sizes=[50, 80, 110], cv=5)
&gt;&gt;&gt; train_sizes            
array([ 50, 80, 110])
&gt;&gt;&gt; train_scores           
array([[ 0.98...,  0.98 ,  0.98...,  0.98...,  0.98...],
       [ 0.98...,  1.   ,  0.98...,  0.98...,  0.98...],
       [ 0.98...,  1.   ,  0.98...,  0.98...,  0.99...]])
&gt;&gt;&gt; valid_scores           
array([[ 1. ,  0.93...,  1. ,  1. ,  0.96...],
       [ 1. ,  0.96...,  1. ,  1. ,  0.96...],
       [ 1. ,  0.96...,  1. ,  1. ,  0.96...]])
</pre>
<div class="_attribution">
  <p class="_attribution-p">
    © 2007–2016 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="http://scikit-learn.org/stable/modules/learning_curve.html" class="_attribution-link" target="_blank">http://scikit-learn.org/stable/modules/learning_curve.html</a>
  </p>
</div>

			</div>
		</div>
	</section>

	</div>
</body>
</html>
