
<!DOCTYPE HTML>

<html lang="en">

<head>
  <meta charset="utf-8">
  <title>1.5. Stochastic Gradient Descent - Scikit-learn - W3cubDocs</title>
  
  <meta name="description" content="Stochastic Gradient Descent (SGD) is a simple yet very efficient approach to discriminative learning of linear classifiers under convex loss &hellip;">
  <meta name="keywords" content="stochastic, gradient, descent, -, scikit-learn, scikit_learn">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="http://docs.w3cub.com/scikit_learn/modules/sgd/">
  <link href="/favicon.png" rel="icon">
  <link type="text/css" rel="stylesheet" href="/assets/application-50364fff564ce3b6327021805f3f00e2957b441cf27f576a7dd4ff63bbc47047.css">
  <script type="text/javascript" src="/assets/application-db64bfd54ceb42be11af7995804cf4902548419ceb79d509b0b7d62c22d98e6f.js"></script>
  <script src="/json/scikit_learn.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body>
	<div class="_app">
	<header class="_header">
  
  <form class="_search">
    <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
    <a class="_search-clear"></a>
    <div class="_search-tag"></div>
  </form>
  
  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/scikit_learn/" class="_nav-link" title="" style="margin-left:0;">scikit-learn</a></span>
  
  <nav class="_nav">
    <a href="/app/" class="_nav-link ">App</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		<div class="_list">
			
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<div class="_page _sphinx">
				
<h1 id="sgd">1.5. Stochastic Gradient Descent</h1> <p id="stochastic-gradient-descent"><strong>Stochastic Gradient Descent (SGD)</strong> is a simple yet very efficient approach to discriminative learning of linear classifiers under convex loss functions such as (linear) <a class="reference external" href="https://en.wikipedia.org/wiki/Support_vector_machine" target="_blank">Support Vector Machines</a> and <a class="reference external" href="https://en.wikipedia.org/wiki/Logistic_regression" target="_blank">Logistic Regression</a>. Even though SGD has been around in the machine learning community for a long time, it has received a considerable amount of attention just recently in the context of large-scale learning.</p> <p>SGD has been successfully applied to large-scale and sparse machine learning problems often encountered in text classification and natural language processing. Given that the data is sparse, the classifiers in this module easily scale to problems with more than 10^5 training examples and more than 10^5 features.</p> <p>The advantages of Stochastic Gradient Descent are:</p>  <ul class="simple"> <li>Efficiency.</li> <li>Ease of implementation (lots of opportunities for code tuning).</li> </ul>  <p>The disadvantages of Stochastic Gradient Descent include:</p>  <ul class="simple"> <li>SGD requires a number of hyperparameters such as the regularization parameter and the number of iterations.</li> <li>SGD is sensitive to feature scaling.</li> </ul>   <h2 id="classification">1.5.1. Classification</h2> <div class="admonition warning"> <p class="first admonition-title">Warning</p> <p class="last">Make sure you permute (shuffle) your training data before fitting the model or use <code>shuffle=True</code> to shuffle after each iteration.</p> </div> <p>The class <a class="reference internal" href="../generated/sklearn.linear_model.sgdclassifier/#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code>SGDClassifier</code></a> implements a plain stochastic gradient descent learning routine which supports different loss functions and penalties for classification.</p> <div class="figure align-center"> <a class="reference external image-reference" href="../../auto_examples/linear_model/plot_sgd_separating_hyperplane/"><img alt="../_images/sphx_glr_plot_sgd_separating_hyperplane_0011.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_sgd_separating_hyperplane_0011.png" style="width: 600.0px; height: 450.0px;"></a> </div> <p>As other classifiers, SGD has to be fitted with two arrays: an array X of size [n_samples, n_features] holding the training samples, and an array Y of size [n_samples] holding the target values (class labels) for the training samples:</p> <pre data-language="python">&gt;&gt;&gt; from sklearn.linear_model import SGDClassifier
&gt;&gt;&gt; X = [[0., 0.], [1., 1.]]
&gt;&gt;&gt; y = [0, 1]
&gt;&gt;&gt; clf = SGDClassifier(loss="hinge", penalty="l2")
&gt;&gt;&gt; clf.fit(X, y)
SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,
       eta0=0.0, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=None, shuffle=True,
       verbose=0, warm_start=False)
</pre> <p>After being fitted, the model can then be used to predict new values:</p> <pre data-language="python">&gt;&gt;&gt; clf.predict([[2., 2.]])
array([1])
</pre> <p>SGD fits a linear model to the training data. The member <code>coef_</code> holds the model parameters:</p> <pre data-language="python">&gt;&gt;&gt; clf.coef_                                         
array([[ 9.9...,  9.9...]])
</pre> <p>Member <code>intercept_</code> holds the intercept (aka offset or bias):</p> <pre data-language="python">&gt;&gt;&gt; clf.intercept_                                    
array([-9.9...])
</pre> <p>Whether or not the model should use an intercept, i.e. a biased hyperplane, is controlled by the parameter <code>fit_intercept</code>.</p> <p>To get the signed distance to the hyperplane use <a class="reference internal" href="../generated/sklearn.linear_model.sgdclassifier/#sklearn.linear_model.SGDClassifier.decision_function" title="sklearn.linear_model.SGDClassifier.decision_function"><code>SGDClassifier.decision_function</code></a>:</p> <pre data-language="python">&gt;&gt;&gt; clf.decision_function([[2., 2.]])                 
array([ 29.6...])
</pre> <p>The concrete loss function can be set via the <code>loss</code> parameter. <a class="reference internal" href="../generated/sklearn.linear_model.sgdclassifier/#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code>SGDClassifier</code></a> supports the following loss functions:</p>  <ul class="simple"> <li>
<code>loss="hinge"</code>: (soft-margin) linear Support Vector Machine,</li> <li>
<code>loss="modified_huber"</code>: smoothed hinge loss,</li> <li>
<code>loss="log"</code>: logistic regression,</li> <li>and all regression losses below.</li> </ul>  <p>The first two loss functions are lazy, they only update the model parameters if an example violates the margin constraint, which makes training very efficient and may result in sparser models, even when L2 penalty is used.</p> <p>Using <code>loss="log"</code> or <code>loss="modified_huber"</code> enables the <code>predict_proba</code> method, which gives a vector of probability estimates <img class="math" src="http://scikit-learn.org/stable/_images/math/f1618e77c8e15b4a154134e2816452a588992532.png" alt="P(y|x)"> per sample <img class="math" src="http://scikit-learn.org/stable/_images/math/a59f68a4202623bb859a7093f0316bf466e6f75d.png" alt="x">:</p> <pre data-language="python">&gt;&gt;&gt; clf = SGDClassifier(loss="log").fit(X, y)
&gt;&gt;&gt; clf.predict_proba([[1., 1.]])                      
array([[ 0.00...,  0.99...]])
</pre> <p>The concrete penalty can be set via the <code>penalty</code> parameter. SGD supports the following penalties:</p>  <ul class="simple"> <li>
<code>penalty="l2"</code>: L2 norm penalty on <code>coef_</code>.</li> <li>
<code>penalty="l1"</code>: L1 norm penalty on <code>coef_</code>.</li> <li>
<code>penalty="elasticnet"</code>: Convex combination of L2 and L1; <code>(1 - l1_ratio) * L2 + l1_ratio * L1</code>.</li> </ul>  <p>The default setting is <code>penalty="l2"</code>. The L1 penalty leads to sparse solutions, driving most coefficients to zero. The Elastic Net solves some deficiencies of the L1 penalty in the presence of highly correlated attributes. The parameter <code>l1_ratio</code> controls the convex combination of L1 and L2 penalty.</p> <p><a class="reference internal" href="../generated/sklearn.linear_model.sgdclassifier/#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code>SGDClassifier</code></a> supports multi-class classification by combining multiple binary classifiers in a “one versus all” (OVA) scheme. For each of the <img class="math" src="http://scikit-learn.org/stable/_images/math/684381a21cd73ebbf43b63a087d3f7410ee99ce8.png" alt="K"> classes, a binary classifier is learned that discriminates between that and all other <img class="math" src="http://scikit-learn.org/stable/_images/math/d1fb41a058b1d3126362bd00674f573b11212607.png" alt="K-1"> classes. At testing time, we compute the confidence score (i.e. the signed distances to the hyperplane) for each classifier and choose the class with the highest confidence. The Figure below illustrates the OVA approach on the iris dataset. The dashed lines represent the three OVA classifiers; the background colors show the decision surface induced by the three classifiers.</p> <div class="figure align-center"> <a class="reference external image-reference" href="../../auto_examples/linear_model/plot_sgd_iris/"><img alt="../_images/sphx_glr_plot_sgd_iris_0011.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_sgd_iris_0011.png" style="width: 600.0px; height: 450.0px;"></a> </div> <p>In the case of multi-class classification <code>coef_</code> is a two-dimensionally array of <code>shape=[n_classes, n_features]</code> and <code>intercept_</code> is a one dimensional array of <code>shape=[n_classes]</code>. The i-th row of <code>coef_</code> holds the weight vector of the OVA classifier for the i-th class; classes are indexed in ascending order (see attribute <code>classes_</code>). Note that, in principle, since they allow to create a probability model, <code>loss="log"</code> and <code>loss="modified_huber"</code> are more suitable for one-vs-all classification.</p> <p><a class="reference internal" href="../generated/sklearn.linear_model.sgdclassifier/#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code>SGDClassifier</code></a> supports both weighted classes and weighted instances via the fit parameters <code>class_weight</code> and <code>sample_weight</code>. See the examples below and the doc string of <a class="reference internal" href="../generated/sklearn.linear_model.sgdclassifier/#sklearn.linear_model.SGDClassifier.fit" title="sklearn.linear_model.SGDClassifier.fit"><code>SGDClassifier.fit</code></a> for further information.</p> <div class="topic"> <p class="topic-title first">Examples:</p> <ul class="simple"> <li>
<a class="reference internal" href="../../auto_examples/linear_model/plot_sgd_separating_hyperplane/#sphx-glr-auto-examples-linear-model-plot-sgd-separating-hyperplane-py"><span class="std std-ref">SGD: Maximum margin separating hyperplane</span></a>,</li> <li><a class="reference internal" href="../../auto_examples/linear_model/plot_sgd_iris/#sphx-glr-auto-examples-linear-model-plot-sgd-iris-py"><span class="std std-ref">Plot multi-class SGD on the iris dataset</span></a></li> <li><a class="reference internal" href="../../auto_examples/linear_model/plot_sgd_weighted_samples/#sphx-glr-auto-examples-linear-model-plot-sgd-weighted-samples-py"><span class="std std-ref">SGD: Weighted samples</span></a></li> <li><a class="reference internal" href="../../auto_examples/linear_model/plot_sgd_comparison/#sphx-glr-auto-examples-linear-model-plot-sgd-comparison-py"><span class="std std-ref">Comparing various online solvers</span></a></li> <li>
<a class="reference internal" href="../../auto_examples/svm/plot_separating_hyperplane_unbalanced/#sphx-glr-auto-examples-svm-plot-separating-hyperplane-unbalanced-py"><span class="std std-ref">SVM: Separating hyperplane for unbalanced classes</span></a> (See the <code>Note</code>)</li> </ul> </div> <p><a class="reference internal" href="../generated/sklearn.linear_model.sgdclassifier/#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code>SGDClassifier</code></a> supports averaged SGD (ASGD). Averaging can be enabled by setting <code>`average=True`</code>. ASGD works by averaging the coefficients of the plain SGD over each iteration over a sample. When using ASGD the learning rate can be larger and even constant leading on some datasets to a speed up in training time.</p> <p>For classification with a logistic loss, another variant of SGD with an averaging strategy is available with Stochastic Average Gradient (SAG) algorithm, available as a solver in <a class="reference internal" href="../generated/sklearn.linear_model.logisticregression/#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model.LogisticRegression"><code>LogisticRegression</code></a>.</p>   <h2 id="regression">1.5.2. Regression</h2> <p>The class <a class="reference internal" href="../generated/sklearn.linear_model.sgdregressor/#sklearn.linear_model.SGDRegressor" title="sklearn.linear_model.SGDRegressor"><code>SGDRegressor</code></a> implements a plain stochastic gradient descent learning routine which supports different loss functions and penalties to fit linear regression models. <a class="reference internal" href="../generated/sklearn.linear_model.sgdregressor/#sklearn.linear_model.SGDRegressor" title="sklearn.linear_model.SGDRegressor"><code>SGDRegressor</code></a> is well suited for regression problems with a large number of training samples (&gt; 10.000), for other problems we recommend <a class="reference internal" href="../generated/sklearn.linear_model.ridge/#sklearn.linear_model.Ridge" title="sklearn.linear_model.Ridge"><code>Ridge</code></a>, <a class="reference internal" href="../generated/sklearn.linear_model.lasso/#sklearn.linear_model.Lasso" title="sklearn.linear_model.Lasso"><code>Lasso</code></a>, or <a class="reference internal" href="../generated/sklearn.linear_model.elasticnet/#sklearn.linear_model.ElasticNet" title="sklearn.linear_model.ElasticNet"><code>ElasticNet</code></a>.</p> <p>The concrete loss function can be set via the <code>loss</code> parameter. <a class="reference internal" href="../generated/sklearn.linear_model.sgdregressor/#sklearn.linear_model.SGDRegressor" title="sklearn.linear_model.SGDRegressor"><code>SGDRegressor</code></a> supports the following loss functions:</p>  <ul class="simple"> <li>
<code>loss="squared_loss"</code>: Ordinary least squares,</li> <li>
<code>loss="huber"</code>: Huber loss for robust regression,</li> <li>
<code>loss="epsilon_insensitive"</code>: linear Support Vector Regression.</li> </ul>  <p>The Huber and epsilon-insensitive loss functions can be used for robust regression. The width of the insensitive region has to be specified via the parameter <code>epsilon</code>. This parameter depends on the scale of the target variables.</p> <p><a class="reference internal" href="../generated/sklearn.linear_model.sgdregressor/#sklearn.linear_model.SGDRegressor" title="sklearn.linear_model.SGDRegressor"><code>SGDRegressor</code></a> supports averaged SGD as <a class="reference internal" href="../generated/sklearn.linear_model.sgdclassifier/#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code>SGDClassifier</code></a>. Averaging can be enabled by setting <code>`average=True`</code>.</p> <p>For regression with a squared loss and a l2 penalty, another variant of SGD with an averaging strategy is available with Stochastic Average Gradient (SAG) algorithm, available as a solver in <a class="reference internal" href="../generated/sklearn.linear_model.ridge/#sklearn.linear_model.Ridge" title="sklearn.linear_model.Ridge"><code>Ridge</code></a>.</p>   <h2 id="stochastic-gradient-descent-for-sparse-data">1.5.3. Stochastic Gradient Descent for sparse data</h2> <div class="admonition note"> <p class="first admonition-title">Note</p> <p class="last">The sparse implementation produces slightly different results than the dense implementation due to a shrunk learning rate for the intercept.</p> </div> <p>There is built-in support for sparse data given in any matrix in a format supported by <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/sparse.html" target="_blank">scipy.sparse</a>. For maximum efficiency, however, use the CSR matrix format as defined in <a class="reference external" href="http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html" target="_blank">scipy.sparse.csr_matrix</a>.</p> <div class="topic"> <p class="topic-title first">Examples:</p> <ul class="simple"> <li><a class="reference internal" href="../../auto_examples/text/document_classification_20newsgroups/#sphx-glr-auto-examples-text-document-classification-20newsgroups-py"><span class="std std-ref">Classification of text documents using sparse features</span></a></li> </ul> </div>   <h2 id="complexity">1.5.4. Complexity</h2> <p>The major advantage of SGD is its efficiency, which is basically linear in the number of training examples. If X is a matrix of size (n, p) training has a cost of <img class="math" src="http://scikit-learn.org/stable/_images/math/d6d26b34a5b03fbea63cfb88418b145be4afa16e.png" alt="O(k n \bar p)">, where k is the number of iterations (epochs) and <img class="math" src="http://scikit-learn.org/stable/_images/math/fe649a4b036454a15ebd0b3725425a7755eb6dc2.png" alt="\bar p"> is the average number of non-zero attributes per sample.</p> <p>Recent theoretical results, however, show that the runtime to get some desired optimization accuracy does not increase as the training set size increases.</p>   <h2 id="tips-on-practical-use">1.5.5. Tips on Practical Use</h2>  <ul> <li>
<p class="first">Stochastic Gradient Descent is sensitive to feature scaling, so it is highly recommended to scale your data. For example, scale each attribute on the input vector X to [0,1] or [-1,+1], or standardize it to have mean 0 and variance 1. Note that the <em>same</em> scaling must be applied to the test vector to obtain meaningful results. This can be easily done using <code>StandardScaler</code>:</p> <pre data-language="python">from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(X_train)  # Don't cheat - fit only on training data
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)  # apply same transformation to test data
</pre> <p>If your attributes have an intrinsic scale (e.g. word frequencies or indicator features) scaling is not needed.</p> </li> <li>Finding a reasonable regularization term <img class="math" src="http://scikit-learn.org/stable/_images/math/877d234f4cec6974ce218fc2e975a486a7972dfd.png" alt="\alpha"> is best done using <code>GridSearchCV</code>, usually in the range <code>10.0**-np.arange(1,7)</code>. </li> <li>Empirically, we found that SGD converges after observing approx. 10^6 training samples. Thus, a reasonable first guess for the number of iterations is <code>n_iter = np.ceil(10**6 / n)</code>, where <code>n</code> is the size of the training set. </li> <li>If you apply SGD to features extracted using PCA we found that it is often wise to scale the feature values by some constant <code>c</code> such that the average L2 norm of the training data equals one. </li> <li>We found that Averaged SGD works best with a larger number of features and a higher eta0 </li> </ul>  <div class="topic"> <p class="topic-title first">References:</p> <ul class="simple"> <li>
<a class="reference external" href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf" target="_blank">“Efficient BackProp”</a> Y. LeCun, L. Bottou, G. Orr, K. Müller - In Neural Networks: Tricks of the Trade 1998.</li> </ul> </div>   <h2 id="sgd-mathematical-formulation">1.5.6. Mathematical formulation</h2> <p id="mathematical-formulation">Given a set of training examples <img class="math" src="http://scikit-learn.org/stable/_images/math/b6b95353bc4a876c1d99214e8bab93f5cf9076a0.png" alt="(x_1, y_1), \ldots, (x_n, y_n)"> where <img class="math" src="http://scikit-learn.org/stable/_images/math/bed637067e6e774b87cc8e3c059b2f97ce7be531.png" alt="x_i \in \mathbf{R}^n"> and <img class="math" src="http://scikit-learn.org/stable/_images/math/72cc647a6fea5baa575125773c28b6e5be5133d7.png" alt="y_i \in \{-1,1\}">, our goal is to learn a linear scoring function <img class="math" src="http://scikit-learn.org/stable/_images/math/485c5d017bb28ce9151f2d301eb4fc45269b276c.png" alt="f(x) = w^T x + b"> with model parameters <img class="math" src="http://scikit-learn.org/stable/_images/math/858a736d009336cd95d17ba520bffcd11c6b0c28.png" alt="w \in \mathbf{R}^m"> and intercept <img class="math" src="http://scikit-learn.org/stable/_images/math/5c3f945125220355508d71c6f63549c669edb675.png" alt="b \in \mathbf{R}">. In order to make predictions, we simply look at the sign of <img class="math" src="http://scikit-learn.org/stable/_images/math/eda52292f6952f7e27fef52e7ce8393981770d2c.png" alt="f(x)">. A common choice to find the model parameters is by minimizing the regularized training error given by</p> <div class="math"> <p><img src="http://scikit-learn.org/stable/_images/math/62d4b457d5f92d158e0f1f1bf3b73116e34d3931.png" alt="E(w,b) = \frac{1}{n}\sum_{i=1}^{n} L(y_i, f(x_i)) + \alpha R(w)"></p> </div>
<p>where <img class="math" src="http://scikit-learn.org/stable/_images/math/ae2b750f71e1fc0daaa3de9a85d42794d7cd1326.png" alt="L"> is a loss function that measures model (mis)fit and <img class="math" src="http://scikit-learn.org/stable/_images/math/a00254b18ffa992f0ef19f6e6e095b83c8f85e94.png" alt="R"> is a regularization term (aka penalty) that penalizes model complexity; <img class="math" src="http://scikit-learn.org/stable/_images/math/969cb5f6781dc1ccd2500d69e55fdb01ae91b31a.png" alt="\alpha &gt; 0"> is a non-negative hyperparameter.</p> <p>Different choices for <img class="math" src="http://scikit-learn.org/stable/_images/math/ae2b750f71e1fc0daaa3de9a85d42794d7cd1326.png" alt="L"> entail different classifiers such as</p>  <ul class="simple"> <li>Hinge: (soft-margin) Support Vector Machines.</li> <li>Log: Logistic Regression.</li> <li>Least-Squares: Ridge Regression.</li> <li>Epsilon-Insensitive: (soft-margin) Support Vector Regression.</li> </ul>  <p>All of the above loss functions can be regarded as an upper bound on the misclassification error (Zero-one loss) as shown in the Figure below.</p> <div class="figure align-center"> <a class="reference external image-reference" href="../../auto_examples/linear_model/plot_sgd_loss_functions/"><img alt="../_images/sphx_glr_plot_sgd_loss_functions_0011.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_sgd_loss_functions_0011.png" style="width: 600.0px; height: 450.0px;"></a> </div> <p>Popular choices for the regularization term <img class="math" src="http://scikit-learn.org/stable/_images/math/a00254b18ffa992f0ef19f6e6e095b83c8f85e94.png" alt="R"> include:</p>  <ul class="simple"> <li>L2 norm: <img class="math" src="http://scikit-learn.org/stable/_images/math/4fa07873ede84da7b6e2c3b6fe47b6fca2af4a55.png" alt="R(w) := \frac{1}{2} \sum_{i=1}^{n} w_i^2">,</li> <li>L1 norm: <img class="math" src="http://scikit-learn.org/stable/_images/math/fadb54be33141d99ef411fba024ea3c7ec2de22a.png" alt="R(w) := \sum_{i=1}^{n} |w_i|">, which leads to sparse solutions.</li> <li>Elastic Net: <img class="math" src="http://scikit-learn.org/stable/_images/math/885050c5d01823261cfba1f447fd02b5b6fc20db.png" alt="R(w) := \frac{\rho}{2} \sum_{i=1}^{n} w_i^2 + (1-\rho) \sum_{i=1}^{n} |w_i|">, a convex combination of L2 and L1, where <img class="math" src="http://scikit-learn.org/stable/_images/math/9a51ab9a0b521705e1e8762fac6bdd6f11771758.png" alt="\rho"> is given by <code>1 - l1_ratio</code>.</li> </ul>  <p>The Figure below shows the contours of the different regularization terms in the parameter space when <img class="math" src="http://scikit-learn.org/stable/_images/math/9d8d44da8ab5aa213680932767fcd1fc38f8b2cb.png" alt="R(w) = 1">.</p> <div class="figure align-center"> <a class="reference external image-reference" href="../../auto_examples/linear_model/plot_sgd_penalties/"><img alt="../_images/sphx_glr_plot_sgd_penalties_0011.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_sgd_penalties_0011.png" style="width: 600.0px; height: 450.0px;"></a> </div>  <h3 id="id1">1.5.6.1. SGD</h3> <p>Stochastic gradient descent is an optimization method for unconstrained optimization problems. In contrast to (batch) gradient descent, SGD approximates the true gradient of <img class="math" src="http://scikit-learn.org/stable/_images/math/1ec33d1cabcca22f7b60252e7e40179bee3b102e.png" alt="E(w,b)"> by considering a single training example at a time.</p> <p>The class <a class="reference internal" href="../generated/sklearn.linear_model.sgdclassifier/#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code>SGDClassifier</code></a> implements a first-order SGD learning routine. The algorithm iterates over the training examples and for each example updates the model parameters according to the update rule given by</p> <div class="math"> <p><img src="http://scikit-learn.org/stable/_images/math/72cbe4d2bf214f47779e8e308f9cad17b46acb07.png" alt="w \leftarrow w - \eta (\alpha \frac{\partial R(w)}{\partial w}
+ \frac{\partial L(w^T x_i + b, y_i)}{\partial w})"></p> </div>
<p>where <img class="math" src="http://scikit-learn.org/stable/_images/math/5635a7c34414599c2452d72430811e816b460335.png" alt="\eta"> is the learning rate which controls the step-size in the parameter space. The intercept <img class="math" src="http://scikit-learn.org/stable/_images/math/57c9d14bb082716df9000146882ce365335d08f1.png" alt="b"> is updated similarly but without regularization.</p> <p>The learning rate <img class="math" src="http://scikit-learn.org/stable/_images/math/5635a7c34414599c2452d72430811e816b460335.png" alt="\eta"> can be either constant or gradually decaying. For classification, the default learning rate schedule (<code>learning_rate='optimal'</code>) is given by</p> <div class="math"> <p><img src="http://scikit-learn.org/stable/_images/math/4f5acaa58eefe708361216e63f1108fc8751bd22.png" alt="\eta^{(t)} = \frac {1}{\alpha  (t_0 + t)}"></p> </div>
<p>where <img class="math" src="http://scikit-learn.org/stable/_images/math/5ec053cf70dc1c98cc297322250569eda193e7a4.png" alt="t"> is the time step (there are a total of <code>n_samples * n_iter</code> time steps), <img class="math" src="http://scikit-learn.org/stable/_images/math/1410bd3aa3d34373bc7de52c132b4fd6f154019a.png" alt="t_0"> is determined based on a heuristic proposed by Léon Bottou such that the expected initial updates are comparable with the expected size of the weights (this assuming that the norm of the training samples is approx. 1). The exact definition can be found in <code>_init_t</code> in <code>BaseSGD</code>.</p> <p>For regression the default learning rate schedule is inverse scaling (<code>learning_rate='invscaling'</code>), given by</p> <div class="math"> <p><img src="http://scikit-learn.org/stable/_images/math/93c34305150894c3956e9f2816c1b9f30f33b200.png" alt="\eta^{(t)} = \frac{eta_0}{t^{power\_t}}"></p> </div>
<p>where <img class="math" src="http://scikit-learn.org/stable/_images/math/64dc796cf3af805cbab2f810ad7ee70d880bfd49.png" alt="eta_0"> and <img class="math" src="http://scikit-learn.org/stable/_images/math/b6e8ac1dcea3e0265093fae39c534076991c3af2.png" alt="power\_t"> are hyperparameters chosen by the user via <code>eta0</code> and <code>power_t</code>, resp.</p> <p>For a constant learning rate use <code>learning_rate='constant'</code> and use <code>eta0</code> to specify the learning rate.</p> <p>The model parameters can be accessed through the members <code>coef_</code> and <code>intercept_</code>:</p>  <ul class="simple"> <li>Member <code>coef_</code> holds the weights <img class="math" src="http://scikit-learn.org/stable/_images/math/ecd1ee2a1cd226b40c37e079aca62398d4b774f5.png" alt="w">
</li> <li>Member <code>intercept_</code> holds <img class="math" src="http://scikit-learn.org/stable/_images/math/57c9d14bb082716df9000146882ce365335d08f1.png" alt="b">
</li> </ul>  <div class="topic"> <p class="topic-title first">References:</p> <ul class="simple"> <li>
<a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.58.7377" target="_blank">“Solving large scale linear prediction problems using stochastic gradient descent algorithms”</a> T. Zhang - In Proceedings of ICML ‘04.</li> <li>
<a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.124.4696" target="_blank">“Regularization and variable selection via the elastic net”</a> H. Zou, T. Hastie - Journal of the Royal Statistical Society Series B, 67 (2), 301-320.</li> <li>
<a class="reference external" href="http://arxiv.org/pdf/1107.2490v2.pdf" target="_blank">“Towards Optimal One Pass Large Scale Learning with Averaged Stochastic Gradient Descent”</a> Xu, Wei</li> </ul> </div>    <h2 id="implementation-details">1.5.7. Implementation details</h2> <p>The implementation of SGD is influenced by the <a class="reference external" href="http://leon.bottou.org/projects/sgd" target="_blank">Stochastic Gradient SVM</a> of Léon Bottou. Similar to SvmSGD, the weight vector is represented as the product of a scalar and a vector which allows an efficient weight update in the case of L2 regularization. In the case of sparse feature vectors, the intercept is updated with a smaller learning rate (multiplied by 0.01) to account for the fact that it is updated more frequently. Training examples are picked up sequentially and the learning rate is lowered after each observed example. We adopted the learning rate schedule from Shalev-Shwartz et al. 2007. For multi-class classification, a “one versus all” approach is used. We use the truncated gradient algorithm proposed by Tsuruoka et al. 2009 for L1 regularization (and the Elastic Net). The code is written in Cython.</p> <div class="topic"> <p class="topic-title first">References:</p> <ul class="simple"> <li>
<a class="reference external" href="http://leon.bottou.org/projects/sgd" target="_blank">“Stochastic Gradient Descent”</a> L. Bottou - Website, 2010.</li> <li>
<a class="reference external" href="http://leon.bottou.org/slides/largescale/lstut.pdf" target="_blank">“The Tradeoffs of Large Scale Machine Learning”</a> L. Bottou - Website, 2011.</li> <li>
<a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.74.8513" target="_blank">“Pegasos: Primal estimated sub-gradient solver for svm”</a> S. Shalev-Shwartz, Y. Singer, N. Srebro - In Proceedings of ICML ‘07.</li> <li>
<a class="reference external" href="http://www.aclweb.org/anthology/P/P09/P09-1054.pdf" target="_blank">“Stochastic gradient descent training for l1-regularized log-linear models with cumulative penalty”</a> Y. Tsuruoka, J. Tsujii, S. Ananiadou - In Proceedings of the AFNLP/ACL ‘09.</li> </ul> </div>
<div class="_attribution">
  <p class="_attribution-p">
    © 2007–2016 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="http://scikit-learn.org/stable/modules/sgd.html" class="_attribution-link" target="_blank">http://scikit-learn.org/stable/modules/sgd.html</a>
  </p>
</div>

			</div>
		</div>
	</section>

	</div>
</body>
</html>
