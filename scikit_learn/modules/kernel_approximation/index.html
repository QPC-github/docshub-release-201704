
<!DOCTYPE HTML>

<html lang="en">

<head>
  <meta charset="utf-8">
  <title>4.6. Kernel Approximation - Scikit-learn - W3cubDocs</title>
  
  <meta name="description" content="This submodule contains functions that approximate the feature mappings that correspond to certain kernels, as they are used for example in support &hellip;">
  <meta name="keywords" content="kernel, approximation, -, scikit-learn, scikit_learn">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="http://docs.w3cub.com/scikit_learn/modules/kernel_approximation/">
  <link href="/favicon.png" rel="icon">
  <link type="text/css" rel="stylesheet" href="/assets/application-50364fff564ce3b6327021805f3f00e2957b441cf27f576a7dd4ff63bbc47047.css">
  <script type="text/javascript" src="/assets/application-db64bfd54ceb42be11af7995804cf4902548419ceb79d509b0b7d62c22d98e6f.js"></script>
  <script src="/json/scikit_learn.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body>
	<div class="_app">
	<header class="_header">
  
  <form class="_search">
    <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
    <a class="_search-clear"></a>
    <div class="_search-tag"></div>
  </form>
  
  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/scikit_learn/" class="_nav-link" title="" style="margin-left:0;">scikit-learn</a></span>
  
  <nav class="_nav">
    <a href="/app/" class="_nav-link ">App</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		<div class="_list">
			
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<div class="_page _sphinx">
				
<h1 id="id1">4.6. Kernel Approximation</h1> <p id="kernel-approximation">This submodule contains functions that approximate the feature mappings that correspond to certain kernels, as they are used for example in support vector machines (see <a class="reference internal" href="../svm/#svm"><span class="std std-ref">Support Vector Machines</span></a>). The following feature functions perform non-linear transformations of the input, which can serve as a basis for linear classification or other algorithms.</p> <p>The advantage of using approximate explicit feature maps compared to the <a class="reference external" href="https://en.wikipedia.org/wiki/Kernel_trick" target="_blank">kernel trick</a>, which makes use of feature maps implicitly, is that explicit mappings can be better suited for online learning and can significantly reduce the cost of learning with very large datasets. Standard kernelized SVMs do not scale well to large datasets, but using an approximate kernel map it is possible to use much more efficient linear SVMs. In particular, the combination of kernel map approximations with <a class="reference internal" href="../generated/sklearn.linear_model.sgdclassifier/#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code>SGDClassifier</code></a> can make non-linear learning on large datasets possible.</p> <p>Since there has not been much empirical work using approximate embeddings, it is advisable to compare results against exact kernel methods when possible.</p> <div class="admonition seealso"> <p class="first admonition-title">See also</p> <p class="last"><a class="reference internal" href="../linear_model/#polynomial-regression"><span class="std std-ref">Polynomial regression: extending linear models with basis functions</span></a> for an exact polynomial transformation.</p> </div>  <h2 id="nystroem-kernel-approx">4.6.1. Nystroem Method for Kernel Approximation</h2> <p id="nystroem-method-for-kernel-approximation">The Nystroem method, as implemented in <a class="reference internal" href="../generated/sklearn.kernel_approximation.nystroem/#sklearn.kernel_approximation.Nystroem" title="sklearn.kernel_approximation.Nystroem"><code>Nystroem</code></a> is a general method for low-rank approximations of kernels. It achieves this by essentially subsampling the data on which the kernel is evaluated. By default <a class="reference internal" href="../generated/sklearn.kernel_approximation.nystroem/#sklearn.kernel_approximation.Nystroem" title="sklearn.kernel_approximation.Nystroem"><code>Nystroem</code></a> uses the <code>rbf</code> kernel, but it can use any kernel function or a precomputed kernel matrix. The number of samples used - which is also the dimensionality of the features computed - is given by the parameter <code>n_components</code>.</p>   <h2 id="rbf-kernel-approx">4.6.2. Radial Basis Function Kernel</h2> <p id="radial-basis-function-kernel">The <a class="reference internal" href="../generated/sklearn.kernel_approximation.rbfsampler/#sklearn.kernel_approximation.RBFSampler" title="sklearn.kernel_approximation.RBFSampler"><code>RBFSampler</code></a> constructs an approximate mapping for the radial basis function kernel, also known as <em>Random Kitchen Sinks</em> <a class="reference internal" href="#rr2007" id="id2">[RR2007]</a>. This transformation can be used to explicitly model a kernel map, prior to applying a linear algorithm, for example a linear SVM:</p> <pre data-language="python">&gt;&gt;&gt; from sklearn.kernel_approximation import RBFSampler
&gt;&gt;&gt; from sklearn.linear_model import SGDClassifier
&gt;&gt;&gt; X = [[0, 0], [1, 1], [1, 0], [0, 1]]
&gt;&gt;&gt; y = [0, 0, 1, 1]
&gt;&gt;&gt; rbf_feature = RBFSampler(gamma=1, random_state=1)
&gt;&gt;&gt; X_features = rbf_feature.fit_transform(X)
&gt;&gt;&gt; clf = SGDClassifier()   
&gt;&gt;&gt; clf.fit(X_features, y)
SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,
       eta0=0.0, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=1,
       penalty='l2', power_t=0.5, random_state=None, shuffle=True,
       verbose=0, warm_start=False)
&gt;&gt;&gt; clf.score(X_features, y)
1.0
</pre> <p>The mapping relies on a Monte Carlo approximation to the kernel values. The <code>fit</code> function performs the Monte Carlo sampling, whereas the <code>transform</code> method performs the mapping of the data. Because of the inherent randomness of the process, results may vary between different calls to the <code>fit</code> function.</p> <p>The <code>fit</code> function takes two arguments: <code>n_components</code>, which is the target dimensionality of the feature transform, and <code>gamma</code>, the parameter of the RBF-kernel. A higher <code>n_components</code> will result in a better approximation of the kernel and will yield results more similar to those produced by a kernel SVM. Note that “fitting” the feature function does not actually depend on the data given to the <code>fit</code> function. Only the dimensionality of the data is used. Details on the method can be found in <a class="reference internal" href="#rr2007" id="id3">[RR2007]</a>.</p> <p>For a given value of <code>n_components</code> <a class="reference internal" href="../generated/sklearn.kernel_approximation.rbfsampler/#sklearn.kernel_approximation.RBFSampler" title="sklearn.kernel_approximation.RBFSampler"><code>RBFSampler</code></a> is often less accurate as <a class="reference internal" href="../generated/sklearn.kernel_approximation.nystroem/#sklearn.kernel_approximation.Nystroem" title="sklearn.kernel_approximation.Nystroem"><code>Nystroem</code></a>. <a class="reference internal" href="../generated/sklearn.kernel_approximation.rbfsampler/#sklearn.kernel_approximation.RBFSampler" title="sklearn.kernel_approximation.RBFSampler"><code>RBFSampler</code></a> is cheaper to compute, though, making use of larger feature spaces more efficient.</p> <div class="figure align-center" id="id8"> <a class="reference external image-reference" href="../../auto_examples/plot_kernel_approximation/"><img alt="../_images/sphx_glr_plot_kernel_approximation_0021.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_kernel_approximation_0021.png" style="width: 600.0px; height: 250.0px;"></a> <p class="caption"><span class="caption-text">Comparing an exact RBF kernel (left) with the approximation (right)</span></p> </div> <div class="topic"> <p class="topic-title first">Examples:</p> <ul class="simple"> <li><a class="reference internal" href="../../auto_examples/plot_kernel_approximation/#sphx-glr-auto-examples-plot-kernel-approximation-py"><span class="std std-ref">Explicit feature map approximation for RBF kernels</span></a></li> </ul> </div>   <h2 id="additive-chi-kernel-approx">4.6.3. Additive Chi Squared Kernel</h2> <p id="additive-chi-squared-kernel">The additive chi squared kernel is a kernel on histograms, often used in computer vision.</p> <p>The additive chi squared kernel as used here is given by</p> <div class="math"> <p><img src="http://scikit-learn.org/stable/_images/math/ac2861573c0c123e910c00dad341ba98cd110d3d.png" alt="k(x, y) = \sum_i \frac{2x_iy_i}{x_i+y_i}"></p> </div>
<p>This is not exactly the same as <code>sklearn.metrics.additive_chi2_kernel</code>. The authors of <a class="reference internal" href="#vz2010" id="id4">[VZ2010]</a> prefer the version above as it is always positive definite. Since the kernel is additive, it is possible to treat all components <img class="math" src="http://scikit-learn.org/stable/_images/math/7720e563212e11bf72de255ab82c2a3b97c1a7f5.png" alt="x_i"> separately for embedding. This makes it possible to sample the Fourier transform in regular intervals, instead of approximating using Monte Carlo sampling.</p> <p>The class <a class="reference internal" href="../generated/sklearn.kernel_approximation.additivechi2sampler/#sklearn.kernel_approximation.AdditiveChi2Sampler" title="sklearn.kernel_approximation.AdditiveChi2Sampler"><code>AdditiveChi2Sampler</code></a> implements this component wise deterministic sampling. Each component is sampled <img class="math" src="http://scikit-learn.org/stable/_images/math/e11f2701c4a39c7fe543a6c4150b421d50f1c159.png" alt="n"> times, yielding <img class="math" src="http://scikit-learn.org/stable/_images/math/6e4e44b685a19e1346d2a7e3a81cc6367a85691d.png" alt="2n+1"> dimensions per input dimension (the multiple of two stems from the real and complex part of the Fourier transform). In the literature, <img class="math" src="http://scikit-learn.org/stable/_images/math/e11f2701c4a39c7fe543a6c4150b421d50f1c159.png" alt="n"> is usually chosen to be 1 or 2, transforming the dataset to size <code>n_samples * 5 * n_features</code> (in the case of <img class="math" src="http://scikit-learn.org/stable/_images/math/19fd639abdfb31b2bdaf813580ab3d4568716d28.png" alt="n=2">).</p> <p>The approximate feature map provided by <a class="reference internal" href="../generated/sklearn.kernel_approximation.additivechi2sampler/#sklearn.kernel_approximation.AdditiveChi2Sampler" title="sklearn.kernel_approximation.AdditiveChi2Sampler"><code>AdditiveChi2Sampler</code></a> can be combined with the approximate feature map provided by <a class="reference internal" href="../generated/sklearn.kernel_approximation.rbfsampler/#sklearn.kernel_approximation.RBFSampler" title="sklearn.kernel_approximation.RBFSampler"><code>RBFSampler</code></a> to yield an approximate feature map for the exponentiated chi squared kernel. See the <a class="reference internal" href="#vz2010" id="id5">[VZ2010]</a> for details and <a class="reference internal" href="#vvz2010" id="id6">[VVZ2010]</a> for combination with the <a class="reference internal" href="../generated/sklearn.kernel_approximation.rbfsampler/#sklearn.kernel_approximation.RBFSampler" title="sklearn.kernel_approximation.RBFSampler"><code>RBFSampler</code></a>.</p>   <h2 id="skewed-chi-kernel-approx">4.6.4. Skewed Chi Squared Kernel</h2> <p id="skewed-chi-squared-kernel">The skewed chi squared kernel is given by:</p> <div class="math"> <p><img src="http://scikit-learn.org/stable/_images/math/8aba1f9f945a4d929c209a2814a869155f88d172.png" alt="k(x,y) = \prod_i \frac{2\sqrt{x_i+c}\sqrt{y_i+c}}{x_i + y_i + 2c}"></p> </div>
<p>It has properties that are similar to the exponentiated chi squared kernel often used in computer vision, but allows for a simple Monte Carlo approximation of the feature map.</p> <p>The usage of the <a class="reference internal" href="../generated/sklearn.kernel_approximation.skewedchi2sampler/#sklearn.kernel_approximation.SkewedChi2Sampler" title="sklearn.kernel_approximation.SkewedChi2Sampler"><code>SkewedChi2Sampler</code></a> is the same as the usage described above for the <a class="reference internal" href="../generated/sklearn.kernel_approximation.rbfsampler/#sklearn.kernel_approximation.RBFSampler" title="sklearn.kernel_approximation.RBFSampler"><code>RBFSampler</code></a>. The only difference is in the free parameter, that is called <img class="math" src="http://scikit-learn.org/stable/_images/math/ae12a24f88803b5895632e4848d87d46483c492c.png" alt="c">. For a motivation for this mapping and the mathematical details see <a class="reference internal" href="#ls2010" id="id7">[LS2010]</a>.</p>   <h2 id="mathematical-details">4.6.5. Mathematical Details</h2> <p>Kernel methods like support vector machines or kernelized PCA rely on a property of reproducing kernel Hilbert spaces. For any positive definite kernel function <img class="math" src="http://scikit-learn.org/stable/_images/math/0b7c1e16a3a8a849bb8ffdcdbf86f65fd1f30438.png" alt="k"> (a so called Mercer kernel), it is guaranteed that there exists a mapping <img class="math" src="http://scikit-learn.org/stable/_images/math/c2f31c22645274c375eff7920cfdfdc18d60341f.png" alt="\phi"> into a Hilbert space <img class="math" src="http://scikit-learn.org/stable/_images/math/19b5bf31d66194f0134f27b1556f4818a6b98549.png" alt="\mathcal{H}">, such that</p> <div class="math"> <p><img src="http://scikit-learn.org/stable/_images/math/2ca953039633babcf676a0a4d14c9a81747fd262.png" alt="k(x,y) = \langle \phi(x), \phi(y) \rangle"></p> </div>
<p>Where <img class="math" src="http://scikit-learn.org/stable/_images/math/400f0c93eeee083c4e94c33380145098a12d9980.png" alt="\langle \cdot, \cdot \rangle"> denotes the inner product in the Hilbert space.</p> <p>If an algorithm, such as a linear support vector machine or PCA, relies only on the scalar product of data points <img class="math" src="http://scikit-learn.org/stable/_images/math/7720e563212e11bf72de255ab82c2a3b97c1a7f5.png" alt="x_i">, one may use the value of <img class="math" src="http://scikit-learn.org/stable/_images/math/988eefe7912f7206254738a54182922dcd4461eb.png" alt="k(x_i, x_j)">, which corresponds to applying the algorithm to the mapped data points <img class="math" src="http://scikit-learn.org/stable/_images/math/26df575525cdf71b2df07ab54bf38795ec58aeba.png" alt="\phi(x_i)">. The advantage of using <img class="math" src="http://scikit-learn.org/stable/_images/math/0b7c1e16a3a8a849bb8ffdcdbf86f65fd1f30438.png" alt="k"> is that the mapping <img class="math" src="http://scikit-learn.org/stable/_images/math/c2f31c22645274c375eff7920cfdfdc18d60341f.png" alt="\phi"> never has to be calculated explicitly, allowing for arbitrary large features (even infinite).</p> <p>One drawback of kernel methods is, that it might be necessary to store many kernel values <img class="math" src="http://scikit-learn.org/stable/_images/math/988eefe7912f7206254738a54182922dcd4461eb.png" alt="k(x_i, x_j)"> during optimization. If a kernelized classifier is applied to new data <img class="math" src="http://scikit-learn.org/stable/_images/math/2c215c9f2de81ec37e5661af71de2717b10b84e0.png" alt="y_j">, <img class="math" src="http://scikit-learn.org/stable/_images/math/39262ff094ef21b5c09227e146e0bb492b4071ea.png" alt="k(x_i, y_j)"> needs to be computed to make predictions, possibly for many different <img class="math" src="http://scikit-learn.org/stable/_images/math/7720e563212e11bf72de255ab82c2a3b97c1a7f5.png" alt="x_i"> in the training set.</p> <p>The classes in this submodule allow to approximate the embedding <img class="math" src="http://scikit-learn.org/stable/_images/math/c2f31c22645274c375eff7920cfdfdc18d60341f.png" alt="\phi">, thereby working explicitly with the representations <img class="math" src="http://scikit-learn.org/stable/_images/math/26df575525cdf71b2df07ab54bf38795ec58aeba.png" alt="\phi(x_i)">, which obviates the need to apply the kernel or store training examples.</p> <div class="topic"> <p class="topic-title first">References:</p> <table class="docutils citation" frame="void" id="rr2007" rules="none">   <tr>
<td class="label">[RR2007]</td>
<td>
<em>(<a class="fn-backref" href="#id2">1</a>, <a class="fn-backref" href="#id3">2</a>)</em> <a class="reference external" href="http://www.robots.ox.ac.uk/~vgg/rg/papers/randomfeatures.pdf" target="_blank">“Random features for large-scale kernel machines”</a> Rahimi, A. and Recht, B. - Advances in neural information processing 2007,</td>
</tr>  </table> <table class="docutils citation" frame="void" id="ls2010" rules="none">   <tr>
<td class="label"><a class="fn-backref" href="#id7">[LS2010]</a></td>
<td>
<a class="reference external" href="http://www.maths.lth.se/matematiklth/personal/sminchis/papers/lis_dagm10.pdf" target="_blank">“Random Fourier approximations for skewed multiplicative histogram kernels”</a> Random Fourier approximations for skewed multiplicative histogram kernels - Lecture Notes for Computer Sciencd (DAGM)</td>
</tr>  </table> <table class="docutils citation" frame="void" id="vz2010" rules="none">   <tr>
<td class="label">[VZ2010]</td>
<td>
<em>(<a class="fn-backref" href="#id4">1</a>, <a class="fn-backref" href="#id5">2</a>)</em> <a class="reference external" href="https://www.robots.ox.ac.uk/~vgg/publications/2011/Vedaldi11/vedaldi11.pdf" target="_blank">“Efficient additive kernels via explicit feature maps”</a> Vedaldi, A. and Zisserman, A. - Computer Vision and Pattern Recognition 2010</td>
</tr>  </table> <table class="docutils citation" frame="void" id="vvz2010" rules="none">   <tr>
<td class="label"><a class="fn-backref" href="#id6">[VVZ2010]</a></td>
<td>
<a class="reference external" href="https://www.robots.ox.ac.uk/~vgg/publications/2010/Sreekanth10/sreekanth10.pdf" target="_blank">“Generalized RBF feature maps for Efficient Detection”</a> Vempati, S. and Vedaldi, A. and Zisserman, A. and Jawahar, CV - 2010</td>
</tr>  </table> </div>
<div class="_attribution">
  <p class="_attribution-p">
    © 2007–2016 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="http://scikit-learn.org/stable/modules/kernel_approximation.html" class="_attribution-link" target="_blank">http://scikit-learn.org/stable/modules/kernel_approximation.html</a>
  </p>
</div>

			</div>
		</div>
	</section>

	</div>
</body>
</html>
