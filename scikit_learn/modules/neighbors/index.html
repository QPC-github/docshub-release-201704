
<!DOCTYPE HTML>

<html lang="en">

<head>
  <meta charset="utf-8">
  <title>1.6. Nearest Neighbors - Scikit-learn - W3cubDocs</title>
  
  <meta name="description" content="sklearn.neighbors provides functionality for unsupervised and supervised neighbors-based learning methods. Unsupervised nearest neighbors is the &hellip;">
  <meta name="keywords" content="nearest, neighbors, -, scikit-learn, scikit_learn">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="http://docs.w3cub.com/scikit_learn/modules/neighbors/">
  <link href="/favicon.png" rel="icon">
  <link type="text/css" rel="stylesheet" href="/assets/application-50364fff564ce3b6327021805f3f00e2957b441cf27f576a7dd4ff63bbc47047.css">
  <script type="text/javascript" src="/assets/application-db64bfd54ceb42be11af7995804cf4902548419ceb79d509b0b7d62c22d98e6f.js"></script>
  <script src="/json/scikit_learn.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body>
	<div class="_app">
	<header class="_header">
  
  <form class="_search">
    <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
    <a class="_search-clear"></a>
    <div class="_search-tag"></div>
  </form>
  
  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/scikit_learn/" class="_nav-link" title="" style="margin-left:0;">scikit-learn</a></span>
  
  <nav class="_nav">
    <a href="/app/" class="_nav-link ">App</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		<div class="_list">
			
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<div class="_page _sphinx">
				
<h1 id="neighbors">1.6. Nearest Neighbors</h1> <p id="nearest-neighbors"><a class="reference internal" href="../classes/#module-sklearn.neighbors" title="sklearn.neighbors"><code>sklearn.neighbors</code></a> provides functionality for unsupervised and supervised neighbors-based learning methods. Unsupervised nearest neighbors is the foundation of many other learning methods, notably manifold learning and spectral clustering. Supervised neighbors-based learning comes in two flavors: <a class="reference internal" href="#classification">classification</a> for data with discrete labels, and <a class="reference internal" href="#regression">regression</a> for data with continuous labels.</p> <p>The principle behind nearest neighbor methods is to find a predefined number of training samples closest in distance to the new point, and predict the label from these. The number of samples can be a user-defined constant (k-nearest neighbor learning), or vary based on the local density of points (radius-based neighbor learning). The distance can, in general, be any metric measure: standard Euclidean distance is the most common choice. Neighbors-based methods are known as <em>non-generalizing</em> machine learning methods, since they simply “remember” all of its training data (possibly transformed into a fast indexing structure such as a <a class="reference internal" href="#ball-tree"><span class="std std-ref">Ball Tree</span></a> or <a class="reference internal" href="#kd-tree"><span class="std std-ref">KD Tree</span></a>.).</p> <p>Despite its simplicity, nearest neighbors has been successful in a large number of classification and regression problems, including handwritten digits or satellite image scenes. Being a non-parametric method, it is often successful in classification situations where the decision boundary is very irregular.</p> <p>The classes in <a class="reference internal" href="../classes/#module-sklearn.neighbors" title="sklearn.neighbors"><code>sklearn.neighbors</code></a> can handle either Numpy arrays or <code>scipy.sparse</code> matrices as input. For dense matrices, a large number of possible distance metrics are supported. For sparse matrices, arbitrary Minkowski metrics are supported for searches.</p> <p>There are many learning routines which rely on nearest neighbors at their core. One example is <a class="reference internal" href="../density/#kernel-density"><span class="std std-ref">kernel density estimation</span></a>, discussed in the <a class="reference internal" href="../density/#density-estimation"><span class="std std-ref">density estimation</span></a> section.</p>  <h2 id="unsupervised-neighbors">1.6.1. Unsupervised Nearest Neighbors</h2> <p id="unsupervised-nearest-neighbors"><a class="reference internal" href="../generated/sklearn.neighbors.nearestneighbors/#sklearn.neighbors.NearestNeighbors" title="sklearn.neighbors.NearestNeighbors"><code>NearestNeighbors</code></a> implements unsupervised nearest neighbors learning. It acts as a uniform interface to three different nearest neighbors algorithms: <a class="reference internal" href="../generated/sklearn.neighbors.balltree/#sklearn.neighbors.BallTree" title="sklearn.neighbors.BallTree"><code>BallTree</code></a>, <a class="reference internal" href="../generated/sklearn.neighbors.kdtree/#sklearn.neighbors.KDTree" title="sklearn.neighbors.KDTree"><code>KDTree</code></a>, and a brute-force algorithm based on routines in <a class="reference internal" href="../classes/#module-sklearn.metrics.pairwise" title="sklearn.metrics.pairwise"><code>sklearn.metrics.pairwise</code></a>. The choice of neighbors search algorithm is controlled through the keyword <code>'algorithm'</code>, which must be one of <code>['auto', 'ball_tree', 'kd_tree', 'brute']</code>. When the default value <code>'auto'</code> is passed, the algorithm attempts to determine the best approach from the training data. For a discussion of the strengths and weaknesses of each option, see <a class="reference internal" href="#nearest-neighbor-algorithms">Nearest Neighbor Algorithms</a>.</p>  <div class="admonition warning"> <p class="first admonition-title">Warning</p> <p class="last">Regarding the Nearest Neighbors algorithms, if two neighbors, neighbor <img class="math" src="http://scikit-learn.org/stable/_images/math/f8dcb9a4a77e36cd21ed14f3d85f52fabc7b124f.png" alt="k+1"> and <img class="math" src="http://scikit-learn.org/stable/_images/math/0b7c1e16a3a8a849bb8ffdcdbf86f65fd1f30438.png" alt="k">, have identical distances but different labels, the results will depend on the ordering of the training data.</p> </div>   <h3 id="finding-the-nearest-neighbors">1.6.1.1. Finding the Nearest Neighbors</h3> <p>For the simple task of finding the nearest neighbors between two sets of data, the unsupervised algorithms within <a class="reference internal" href="../classes/#module-sklearn.neighbors" title="sklearn.neighbors"><code>sklearn.neighbors</code></a> can be used:</p> <pre data-language="python">&gt;&gt;&gt; from sklearn.neighbors import NearestNeighbors
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
&gt;&gt;&gt; nbrs = NearestNeighbors(n_neighbors=2, algorithm='ball_tree').fit(X)
&gt;&gt;&gt; distances, indices = nbrs.kneighbors(X)
&gt;&gt;&gt; indices                                           
array([[0, 1],
       [1, 0],
       [2, 1],
       [3, 4],
       [4, 3],
       [5, 4]]...)
&gt;&gt;&gt; distances
array([[ 0.        ,  1.        ],
       [ 0.        ,  1.        ],
       [ 0.        ,  1.41421356],
       [ 0.        ,  1.        ],
       [ 0.        ,  1.        ],
       [ 0.        ,  1.41421356]])
</pre> <p>Because the query set matches the training set, the nearest neighbor of each point is the point itself, at a distance of zero.</p> <p>It is also possible to efficiently produce a sparse graph showing the connections between neighboring points:</p> <pre data-language="python">&gt;&gt;&gt; nbrs.kneighbors_graph(X).toarray()
array([[ 1.,  1.,  0.,  0.,  0.,  0.],
       [ 1.,  1.,  0.,  0.,  0.,  0.],
       [ 0.,  1.,  1.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  1.,  1.,  0.],
       [ 0.,  0.,  0.,  1.,  1.,  0.],
       [ 0.,  0.,  0.,  0.,  1.,  1.]])
</pre> <p>Our dataset is structured such that points nearby in index order are nearby in parameter space, leading to an approximately block-diagonal matrix of K-nearest neighbors. Such a sparse graph is useful in a variety of circumstances which make use of spatial relationships between points for unsupervised learning: in particular, see <a class="reference internal" href="../generated/sklearn.manifold.isomap/#sklearn.manifold.Isomap" title="sklearn.manifold.Isomap"><code>sklearn.manifold.Isomap</code></a>, <a class="reference internal" href="../generated/sklearn.manifold.locallylinearembedding/#sklearn.manifold.LocallyLinearEmbedding" title="sklearn.manifold.LocallyLinearEmbedding"><code>sklearn.manifold.LocallyLinearEmbedding</code></a>, and <a class="reference internal" href="../generated/sklearn.cluster.spectralclustering/#sklearn.cluster.SpectralClustering" title="sklearn.cluster.SpectralClustering"><code>sklearn.cluster.SpectralClustering</code></a>.</p>   <h3 id="kdtree-and-balltree-classes">1.6.1.2. KDTree and BallTree Classes</h3> <p>Alternatively, one can use the <a class="reference internal" href="../generated/sklearn.neighbors.kdtree/#sklearn.neighbors.KDTree" title="sklearn.neighbors.KDTree"><code>KDTree</code></a> or <a class="reference internal" href="../generated/sklearn.neighbors.balltree/#sklearn.neighbors.BallTree" title="sklearn.neighbors.BallTree"><code>BallTree</code></a> classes directly to find nearest neighbors. This is the functionality wrapped by the <a class="reference internal" href="../generated/sklearn.neighbors.nearestneighbors/#sklearn.neighbors.NearestNeighbors" title="sklearn.neighbors.NearestNeighbors"><code>NearestNeighbors</code></a> class used above. The Ball Tree and KD Tree have the same interface; we’ll show an example of using the KD Tree here:</p> <pre data-language="python">&gt;&gt;&gt; from sklearn.neighbors import KDTree
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
&gt;&gt;&gt; kdt = KDTree(X, leaf_size=30, metric='euclidean')
&gt;&gt;&gt; kdt.query(X, k=2, return_distance=False)          
array([[0, 1],
       [1, 0],
       [2, 1],
       [3, 4],
       [4, 3],
       [5, 4]]...)
</pre> <p>Refer to the <a class="reference internal" href="../generated/sklearn.neighbors.kdtree/#sklearn.neighbors.KDTree" title="sklearn.neighbors.KDTree"><code>KDTree</code></a> and <a class="reference internal" href="../generated/sklearn.neighbors.balltree/#sklearn.neighbors.BallTree" title="sklearn.neighbors.BallTree"><code>BallTree</code></a> class documentation for more information on the options available for neighbors searches, including specification of query strategies, of various distance metrics, etc. For a list of available metrics, see the documentation of the <a class="reference internal" href="../generated/sklearn.neighbors.distancemetric/#sklearn.neighbors.DistanceMetric" title="sklearn.neighbors.DistanceMetric"><code>DistanceMetric</code></a> class.</p>    <h2 id="classification">1.6.2. Nearest Neighbors Classification</h2> <p id="nearest-neighbors-classification">Neighbors-based classification is a type of <em>instance-based learning</em> or <em>non-generalizing learning</em>: it does not attempt to construct a general internal model, but simply stores instances of the training data. Classification is computed from a simple majority vote of the nearest neighbors of each point: a query point is assigned the data class which has the most representatives within the nearest neighbors of the point.</p> <p>scikit-learn implements two different nearest neighbors classifiers: <a class="reference internal" href="../generated/sklearn.neighbors.kneighborsclassifier/#sklearn.neighbors.KNeighborsClassifier" title="sklearn.neighbors.KNeighborsClassifier"><code>KNeighborsClassifier</code></a> implements learning based on the <img class="math" src="http://scikit-learn.org/stable/_images/math/0b7c1e16a3a8a849bb8ffdcdbf86f65fd1f30438.png" alt="k"> nearest neighbors of each query point, where <img class="math" src="http://scikit-learn.org/stable/_images/math/0b7c1e16a3a8a849bb8ffdcdbf86f65fd1f30438.png" alt="k"> is an integer value specified by the user. <a class="reference internal" href="../generated/sklearn.neighbors.radiusneighborsclassifier/#sklearn.neighbors.RadiusNeighborsClassifier" title="sklearn.neighbors.RadiusNeighborsClassifier"><code>RadiusNeighborsClassifier</code></a> implements learning based on the number of neighbors within a fixed radius <img class="math" src="http://scikit-learn.org/stable/_images/math/eaa6ad49a7f78fe5a13b486690163bf2dc7e3e60.png" alt="r"> of each training point, where <img class="math" src="http://scikit-learn.org/stable/_images/math/eaa6ad49a7f78fe5a13b486690163bf2dc7e3e60.png" alt="r"> is a floating-point value specified by the user.</p> <p>The <img class="math" src="http://scikit-learn.org/stable/_images/math/0b7c1e16a3a8a849bb8ffdcdbf86f65fd1f30438.png" alt="k">-neighbors classification in <a class="reference internal" href="../generated/sklearn.neighbors.kneighborsclassifier/#sklearn.neighbors.KNeighborsClassifier" title="sklearn.neighbors.KNeighborsClassifier"><code>KNeighborsClassifier</code></a> is the more commonly used of the two techniques. The optimal choice of the value <img class="math" src="http://scikit-learn.org/stable/_images/math/0b7c1e16a3a8a849bb8ffdcdbf86f65fd1f30438.png" alt="k"> is highly data-dependent: in general a larger <img class="math" src="http://scikit-learn.org/stable/_images/math/0b7c1e16a3a8a849bb8ffdcdbf86f65fd1f30438.png" alt="k"> suppresses the effects of noise, but makes the classification boundaries less distinct.</p> <p>In cases where the data is not uniformly sampled, radius-based neighbors classification in <a class="reference internal" href="../generated/sklearn.neighbors.radiusneighborsclassifier/#sklearn.neighbors.RadiusNeighborsClassifier" title="sklearn.neighbors.RadiusNeighborsClassifier"><code>RadiusNeighborsClassifier</code></a> can be a better choice. The user specifies a fixed radius <img class="math" src="http://scikit-learn.org/stable/_images/math/eaa6ad49a7f78fe5a13b486690163bf2dc7e3e60.png" alt="r">, such that points in sparser neighborhoods use fewer nearest neighbors for the classification. For high-dimensional parameter spaces, this method becomes less effective due to the so-called “curse of dimensionality”.</p> <p>The basic nearest neighbors classification uses uniform weights: that is, the value assigned to a query point is computed from a simple majority vote of the nearest neighbors. Under some circumstances, it is better to weight the neighbors such that nearer neighbors contribute more to the fit. This can be accomplished through the <code>weights</code> keyword. The default value, <code>weights = 'uniform'</code>, assigns uniform weights to each neighbor. <code>weights = 'distance'</code> assigns weights proportional to the inverse of the distance from the query point. Alternatively, a user-defined function of the distance can be supplied which is used to compute the weights.</p> <p class="centered"> <strong><a class="reference external" href="../../auto_examples/neighbors/plot_classification/"><img alt="classification_1" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_classification_0011.png" style="width: 400.0px; height: 300.0px;"></a> <a class="reference external" href="../../auto_examples/neighbors/plot_classification/"><img alt="classification_2" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_classification_0021.png" style="width: 400.0px; height: 300.0px;"></a></strong></p>
<div class="topic"> <p class="topic-title first">Examples:</p> <ul class="simple"> <li>
<a class="reference internal" href="../../auto_examples/neighbors/plot_classification/#sphx-glr-auto-examples-neighbors-plot-classification-py"><span class="std std-ref">Nearest Neighbors Classification</span></a>: an example of classification using nearest neighbors.</li> </ul> </div>   <h2 id="regression">1.6.3. Nearest Neighbors Regression</h2> <p id="nearest-neighbors-regression">Neighbors-based regression can be used in cases where the data labels are continuous rather than discrete variables. The label assigned to a query point is computed based the mean of the labels of its nearest neighbors.</p> <p>scikit-learn implements two different neighbors regressors: <a class="reference internal" href="../generated/sklearn.neighbors.kneighborsregressor/#sklearn.neighbors.KNeighborsRegressor" title="sklearn.neighbors.KNeighborsRegressor"><code>KNeighborsRegressor</code></a> implements learning based on the <img class="math" src="http://scikit-learn.org/stable/_images/math/0b7c1e16a3a8a849bb8ffdcdbf86f65fd1f30438.png" alt="k"> nearest neighbors of each query point, where <img class="math" src="http://scikit-learn.org/stable/_images/math/0b7c1e16a3a8a849bb8ffdcdbf86f65fd1f30438.png" alt="k"> is an integer value specified by the user. <a class="reference internal" href="../generated/sklearn.neighbors.radiusneighborsregressor/#sklearn.neighbors.RadiusNeighborsRegressor" title="sklearn.neighbors.RadiusNeighborsRegressor"><code>RadiusNeighborsRegressor</code></a> implements learning based on the neighbors within a fixed radius <img class="math" src="http://scikit-learn.org/stable/_images/math/eaa6ad49a7f78fe5a13b486690163bf2dc7e3e60.png" alt="r"> of the query point, where <img class="math" src="http://scikit-learn.org/stable/_images/math/eaa6ad49a7f78fe5a13b486690163bf2dc7e3e60.png" alt="r"> is a floating-point value specified by the user.</p> <p>The basic nearest neighbors regression uses uniform weights: that is, each point in the local neighborhood contributes uniformly to the classification of a query point. Under some circumstances, it can be advantageous to weight points such that nearby points contribute more to the regression than faraway points. This can be accomplished through the <code>weights</code> keyword. The default value, <code>weights = 'uniform'</code>, assigns equal weights to all points. <code>weights = 'distance'</code> assigns weights proportional to the inverse of the distance from the query point. Alternatively, a user-defined function of the distance can be supplied, which will be used to compute the weights.</p> <div class="figure align-center"> <a class="reference external image-reference" href="../../auto_examples/neighbors/plot_regression/"><img alt="../_images/sphx_glr_plot_regression_0011.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_regression_0011.png" style="width: 600.0px; height: 450.0px;"></a> </div> <p>The use of multi-output nearest neighbors for regression is demonstrated in <a class="reference internal" href="../../auto_examples/plot_multioutput_face_completion/#sphx-glr-auto-examples-plot-multioutput-face-completion-py"><span class="std std-ref">Face completion with a multi-output estimators</span></a>. In this example, the inputs X are the pixels of the upper half of faces and the outputs Y are the pixels of the lower half of those faces.</p> <div class="figure align-center"> <a class="reference external image-reference" href="../../auto_examples/plot_multioutput_face_completion/"><img alt="../_images/sphx_glr_plot_multioutput_face_completion_0011.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_multioutput_face_completion_0011.png" style="width: 750.0px; height: 847.5px;"></a> </div> <div class="topic"> <p class="topic-title first">Examples:</p> <ul class="simple"> <li>
<a class="reference internal" href="../../auto_examples/neighbors/plot_regression/#sphx-glr-auto-examples-neighbors-plot-regression-py"><span class="std std-ref">Nearest Neighbors regression</span></a>: an example of regression using nearest neighbors.</li> <li>
<a class="reference internal" href="../../auto_examples/plot_multioutput_face_completion/#sphx-glr-auto-examples-plot-multioutput-face-completion-py"><span class="std std-ref">Face completion with a multi-output estimators</span></a>: an example of multi-output regression using nearest neighbors.</li> </ul> </div>   <h2 id="nearest-neighbor-algorithms">1.6.4. Nearest Neighbor Algorithms</h2>  <h3 id="id1">1.6.4.1. Brute Force</h3> <p id="brute-force">Fast computation of nearest neighbors is an active area of research in machine learning. The most naive neighbor search implementation involves the brute-force computation of distances between all pairs of points in the dataset: for <img class="math" src="http://scikit-learn.org/stable/_images/math/f4170ed8938b79490d8923857962695514a8e4cb.png" alt="N"> samples in <img class="math" src="http://scikit-learn.org/stable/_images/math/3cf5aa5abe312e6fd4996975341a43aa566e8d8d.png" alt="D"> dimensions, this approach scales as <img class="math" src="http://scikit-learn.org/stable/_images/math/757e6525671255d8689148b8b5d1fb62403f0d5e.png" alt="O[D N^2]">. Efficient brute-force neighbors searches can be very competitive for small data samples. However, as the number of samples <img class="math" src="http://scikit-learn.org/stable/_images/math/f4170ed8938b79490d8923857962695514a8e4cb.png" alt="N"> grows, the brute-force approach quickly becomes infeasible. In the classes within <a class="reference internal" href="../classes/#module-sklearn.neighbors" title="sklearn.neighbors"><code>sklearn.neighbors</code></a>, brute-force neighbors searches are specified using the keyword <code>algorithm = 'brute'</code>, and are computed using the routines available in <a class="reference internal" href="../classes/#module-sklearn.metrics.pairwise" title="sklearn.metrics.pairwise"><code>sklearn.metrics.pairwise</code></a>.</p>   <h3 id="kd-tree">1.6.4.2. K-D Tree</h3> <p id="k-d-tree">To address the computational inefficiencies of the brute-force approach, a variety of tree-based data structures have been invented. In general, these structures attempt to reduce the required number of distance calculations by efficiently encoding aggregate distance information for the sample. The basic idea is that if point <img class="math" src="http://scikit-learn.org/stable/_images/math/0043fe6507e9b1d112e07a2801e24927e267dd50.png" alt="A"> is very distant from point <img class="math" src="http://scikit-learn.org/stable/_images/math/9805f44feec6f81d376d09e88b8236635edbb3c8.png" alt="B">, and point <img class="math" src="http://scikit-learn.org/stable/_images/math/9805f44feec6f81d376d09e88b8236635edbb3c8.png" alt="B"> is very close to point <img class="math" src="http://scikit-learn.org/stable/_images/math/afce44aa7c55836ca9345404c22fc7b599d2ed84.png" alt="C">, then we know that points <img class="math" src="http://scikit-learn.org/stable/_images/math/0043fe6507e9b1d112e07a2801e24927e267dd50.png" alt="A"> and <img class="math" src="http://scikit-learn.org/stable/_images/math/afce44aa7c55836ca9345404c22fc7b599d2ed84.png" alt="C"> are very distant, <em>without having to explicitly calculate their distance</em>. In this way, the computational cost of a nearest neighbors search can be reduced to <img class="math" src="http://scikit-learn.org/stable/_images/math/b9b92f23dce9bff3b9b0f68f3051f307da0fa7d9.png" alt="O[D N \log(N)]"> or better. This is a significant improvement over brute-force for large <img class="math" src="http://scikit-learn.org/stable/_images/math/f4170ed8938b79490d8923857962695514a8e4cb.png" alt="N">.</p> <p>An early approach to taking advantage of this aggregate information was the <em>KD tree</em> data structure (short for <em>K-dimensional tree</em>), which generalizes two-dimensional <em>Quad-trees</em> and 3-dimensional <em>Oct-trees</em> to an arbitrary number of dimensions. The KD tree is a binary tree structure which recursively partitions the parameter space along the data axes, dividing it into nested orthotopic regions into which data points are filed. The construction of a KD tree is very fast: because partitioning is performed only along the data axes, no <img class="math" src="http://scikit-learn.org/stable/_images/math/3cf5aa5abe312e6fd4996975341a43aa566e8d8d.png" alt="D">-dimensional distances need to be computed. Once constructed, the nearest neighbor of a query point can be determined with only <img class="math" src="http://scikit-learn.org/stable/_images/math/d62a221f87954d151fa2c39566f1a243a294e625.png" alt="O[\log(N)]"> distance computations. Though the KD tree approach is very fast for low-dimensional (<img class="math" src="http://scikit-learn.org/stable/_images/math/7842f70bd20e56ab5363a6ae2466b4e9e4af6594.png" alt="D &lt; 20">) neighbors searches, it becomes inefficient as <img class="math" src="http://scikit-learn.org/stable/_images/math/3cf5aa5abe312e6fd4996975341a43aa566e8d8d.png" alt="D"> grows very large: this is one manifestation of the so-called “curse of dimensionality”. In scikit-learn, KD tree neighbors searches are specified using the keyword <code>algorithm = 'kd_tree'</code>, and are computed using the class <a class="reference internal" href="../generated/sklearn.neighbors.kdtree/#sklearn.neighbors.KDTree" title="sklearn.neighbors.KDTree"><code>KDTree</code></a>.</p> <div class="topic"> <p class="topic-title first">References:</p> <ul class="simple"> <li>
<a class="reference external" href="http://dl.acm.org/citation.cfm?doid=361002.361007" target="_blank">“Multidimensional binary search trees used for associative searching”</a>, Bentley, J.L., Communications of the ACM (1975)</li> </ul> </div>   <h3 id="id2">1.6.4.3. Ball Tree</h3> <p id="ball-tree">To address the inefficiencies of KD Trees in higher dimensions, the <em>ball tree</em> data structure was developed. Where KD trees partition data along Cartesian axes, ball trees partition data in a series of nesting hyper-spheres. This makes tree construction more costly than that of the KD tree, but results in a data structure which can be very efficient on highly-structured data, even in very high dimensions.</p> <p>A ball tree recursively divides the data into nodes defined by a centroid <img class="math" src="http://scikit-learn.org/stable/_images/math/afce44aa7c55836ca9345404c22fc7b599d2ed84.png" alt="C"> and radius <img class="math" src="http://scikit-learn.org/stable/_images/math/eaa6ad49a7f78fe5a13b486690163bf2dc7e3e60.png" alt="r">, such that each point in the node lies within the hyper-sphere defined by <img class="math" src="http://scikit-learn.org/stable/_images/math/eaa6ad49a7f78fe5a13b486690163bf2dc7e3e60.png" alt="r"> and <img class="math" src="http://scikit-learn.org/stable/_images/math/afce44aa7c55836ca9345404c22fc7b599d2ed84.png" alt="C">. The number of candidate points for a neighbor search is reduced through use of the <em>triangle inequality</em>:</p> <div class="math"> <p><img src="http://scikit-learn.org/stable/_images/math/e184e57ed29688d6164835e952cebb8d0a68657a.png" alt="|x+y| \leq |x| + |y|"></p> </div>
<p>With this setup, a single distance calculation between a test point and the centroid is sufficient to determine a lower and upper bound on the distance to all points within the node. Because of the spherical geometry of the ball tree nodes, it can out-perform a <em>KD-tree</em> in high dimensions, though the actual performance is highly dependent on the structure of the training data. In scikit-learn, ball-tree-based neighbors searches are specified using the keyword <code>algorithm = 'ball_tree'</code>, and are computed using the class <a class="reference internal" href="../generated/sklearn.neighbors.balltree/#sklearn.neighbors.BallTree" title="sklearn.neighbors.BallTree"><code>sklearn.neighbors.BallTree</code></a>. Alternatively, the user can work with the <a class="reference internal" href="../generated/sklearn.neighbors.balltree/#sklearn.neighbors.BallTree" title="sklearn.neighbors.BallTree"><code>BallTree</code></a> class directly.</p> <div class="topic"> <p class="topic-title first">References:</p> <ul class="simple"> <li>
<a class="reference external" href="http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.91.8209" target="_blank">“Five balltree construction algorithms”</a>, Omohundro, S.M., International Computer Science Institute Technical Report (1989)</li> </ul> </div>   <h3 id="choice-of-nearest-neighbors-algorithm">1.6.4.4. Choice of Nearest Neighbors Algorithm</h3> <p>The optimal algorithm for a given dataset is a complicated choice, and depends on a number of factors:</p> <ul> <li>
<p class="first">number of samples <img class="math" src="http://scikit-learn.org/stable/_images/math/f4170ed8938b79490d8923857962695514a8e4cb.png" alt="N"> (i.e. <code>n_samples</code>) and dimensionality <img class="math" src="http://scikit-learn.org/stable/_images/math/3cf5aa5abe312e6fd4996975341a43aa566e8d8d.png" alt="D"> (i.e. <code>n_features</code>).</p> <ul class="simple"> <li>
<em>Brute force</em> query time grows as <img class="math" src="http://scikit-learn.org/stable/_images/math/75043d4d5bf1dc90a2f7ea3a1c487ca5db2919fb.png" alt="O[D N]">
</li> <li>
<em>Ball tree</em> query time grows as approximately <img class="math" src="http://scikit-learn.org/stable/_images/math/974c5f1d8585eb528dfe40e806e691b93d32fcce.png" alt="O[D \log(N)]">
</li> <li>
<em>KD tree</em> query time changes with <img class="math" src="http://scikit-learn.org/stable/_images/math/3cf5aa5abe312e6fd4996975341a43aa566e8d8d.png" alt="D"> in a way that is difficult to precisely characterise. For small <img class="math" src="http://scikit-learn.org/stable/_images/math/3cf5aa5abe312e6fd4996975341a43aa566e8d8d.png" alt="D"> (less than 20 or so) the cost is approximately <img class="math" src="http://scikit-learn.org/stable/_images/math/37e9375fd0210ca33a205eba70c66ba4b41ddc96.png" alt="O[D\log(N)]">, and the KD tree query can be very efficient. For larger <img class="math" src="http://scikit-learn.org/stable/_images/math/3cf5aa5abe312e6fd4996975341a43aa566e8d8d.png" alt="D">, the cost increases to nearly <img class="math" src="http://scikit-learn.org/stable/_images/math/b1b02908acc20145bfec9cde71e0ee5ab4881b69.png" alt="O[DN]">, and the overhead due to the tree structure can lead to queries which are slower than brute force.</li> </ul> <p>For small data sets (<img class="math" src="http://scikit-learn.org/stable/_images/math/f4170ed8938b79490d8923857962695514a8e4cb.png" alt="N"> less than 30 or so), <img class="math" src="http://scikit-learn.org/stable/_images/math/146255392e86d17d31079c064c2828163cc276f6.png" alt="\log(N)"> is comparable to <img class="math" src="http://scikit-learn.org/stable/_images/math/f4170ed8938b79490d8923857962695514a8e4cb.png" alt="N">, and brute force algorithms can be more efficient than a tree-based approach. Both <a class="reference internal" href="../generated/sklearn.neighbors.kdtree/#sklearn.neighbors.KDTree" title="sklearn.neighbors.KDTree"><code>KDTree</code></a> and <a class="reference internal" href="../generated/sklearn.neighbors.balltree/#sklearn.neighbors.BallTree" title="sklearn.neighbors.BallTree"><code>BallTree</code></a> address this through providing a <em>leaf size</em> parameter: this controls the number of samples at which a query switches to brute-force. This allows both algorithms to approach the efficiency of a brute-force computation for small <img class="math" src="http://scikit-learn.org/stable/_images/math/f4170ed8938b79490d8923857962695514a8e4cb.png" alt="N">.</p> </li> <li>
<p class="first">data structure: <em>intrinsic dimensionality</em> of the data and/or <em>sparsity</em> of the data. Intrinsic dimensionality refers to the dimension <img class="math" src="http://scikit-learn.org/stable/_images/math/87a92a98b4d03a624fe14157b41596719aa0f965.png" alt="d \le D"> of a manifold on which the data lies, which can be linearly or non-linearly embedded in the parameter space. Sparsity refers to the degree to which the data fills the parameter space (this is to be distinguished from the concept as used in “sparse” matrices. The data matrix may have no zero entries, but the <strong>structure</strong> can still be “sparse” in this sense).</p> <ul class="simple"> <li>
<em>Brute force</em> query time is unchanged by data structure.</li> <li>
<em>Ball tree</em> and <em>KD tree</em> query times can be greatly influenced by data structure. In general, sparser data with a smaller intrinsic dimensionality leads to faster query times. Because the KD tree internal representation is aligned with the parameter axes, it will not generally show as much improvement as ball tree for arbitrarily structured data.</li> </ul> <p>Datasets used in machine learning tend to be very structured, and are very well-suited for tree-based queries.</p> </li> <li>
<p class="first">number of neighbors <img class="math" src="http://scikit-learn.org/stable/_images/math/0b7c1e16a3a8a849bb8ffdcdbf86f65fd1f30438.png" alt="k"> requested for a query point.</p> <ul class="simple"> <li>
<em>Brute force</em> query time is largely unaffected by the value of <img class="math" src="http://scikit-learn.org/stable/_images/math/0b7c1e16a3a8a849bb8ffdcdbf86f65fd1f30438.png" alt="k">
</li> <li>
<em>Ball tree</em> and <em>KD tree</em> query time will become slower as <img class="math" src="http://scikit-learn.org/stable/_images/math/0b7c1e16a3a8a849bb8ffdcdbf86f65fd1f30438.png" alt="k"> increases. This is due to two effects: first, a larger <img class="math" src="http://scikit-learn.org/stable/_images/math/0b7c1e16a3a8a849bb8ffdcdbf86f65fd1f30438.png" alt="k"> leads to the necessity to search a larger portion of the parameter space. Second, using <img class="math" src="http://scikit-learn.org/stable/_images/math/06dd4627885a0bebaad0e4d6e0fb5db2e746654a.png" alt="k &gt; 1"> requires internal queueing of results as the tree is traversed.</li> </ul> <p>As <img class="math" src="http://scikit-learn.org/stable/_images/math/0b7c1e16a3a8a849bb8ffdcdbf86f65fd1f30438.png" alt="k"> becomes large compared to <img class="math" src="http://scikit-learn.org/stable/_images/math/f4170ed8938b79490d8923857962695514a8e4cb.png" alt="N">, the ability to prune branches in a tree-based query is reduced. In this situation, Brute force queries can be more efficient.</p> </li> <li>number of query points. Both the ball tree and the KD Tree require a construction phase. The cost of this construction becomes negligible when amortized over many queries. If only a small number of queries will be performed, however, the construction can make up a significant fraction of the total cost. If very few query points will be required, brute force is better than a tree-based method. </li> </ul> <p>Currently, <code>algorithm = 'auto'</code> selects <code>'kd_tree'</code> if <img class="math" src="http://scikit-learn.org/stable/_images/math/c13e5702e2e5ad681738f9a435ec8aa018e78d5b.png" alt="k &lt; N/2"> and the <code>'effective_metric_'</code> is in the <code>'VALID_METRICS'</code> list of <code>'kd_tree'</code>. It selects <code>'ball_tree'</code> if <img class="math" src="http://scikit-learn.org/stable/_images/math/c13e5702e2e5ad681738f9a435ec8aa018e78d5b.png" alt="k &lt; N/2"> and the <code>'effective_metric_'</code> is not in the <code>'VALID_METRICS'</code> list of <code>'kd_tree'</code>. It selects <code>'brute'</code> if <img class="math" src="http://scikit-learn.org/stable/_images/math/981b8ac54fb75a8a2bd03480d3c08ecfa0e2027d.png" alt="k &gt;= N/2">. This choice is based on the assumption that the number of query points is at least the same order as the number of training points, and that <code>leaf_size</code> is close to its default value of <code>30</code>.</p>   <h3 id="effect-of-leaf-size">1.6.4.5. Effect of <code>leaf_size</code>
</h3> <p>As noted above, for small sample sizes a brute force search can be more efficient than a tree-based query. This fact is accounted for in the ball tree and KD tree by internally switching to brute force searches within leaf nodes. The level of this switch can be specified with the parameter <code>leaf_size</code>. This parameter choice has many effects:</p> <dl class="docutils"> <dt><strong>construction time</strong></dt> <dd>A larger <code>leaf_size</code> leads to a faster tree construction time, because fewer nodes need to be created</dd> <dt><strong>query time</strong></dt> <dd>Both a large or small <code>leaf_size</code> can lead to suboptimal query cost. For <code>leaf_size</code> approaching 1, the overhead involved in traversing nodes can significantly slow query times. For <code>leaf_size</code> approaching the size of the training set, queries become essentially brute force. A good compromise between these is <code>leaf_size = 30</code>, the default value of the parameter.</dd> <dt><strong>memory</strong></dt> <dd>As <code>leaf_size</code> increases, the memory required to store a tree structure decreases. This is especially important in the case of ball tree, which stores a <img class="math" src="http://scikit-learn.org/stable/_images/math/3cf5aa5abe312e6fd4996975341a43aa566e8d8d.png" alt="D">-dimensional centroid for each node. The required storage space for <a class="reference internal" href="../generated/sklearn.neighbors.balltree/#sklearn.neighbors.BallTree" title="sklearn.neighbors.BallTree"><code>BallTree</code></a> is approximately <code>1 / leaf_size</code> times the size of the training set.</dd> </dl> <p><code>leaf_size</code> is not referenced for brute force queries.</p>    <h2 id="id3">1.6.5. Nearest Centroid Classifier</h2> <p id="nearest-centroid-classifier">The <a class="reference internal" href="../generated/sklearn.neighbors.nearestcentroid/#sklearn.neighbors.NearestCentroid" title="sklearn.neighbors.NearestCentroid"><code>NearestCentroid</code></a> classifier is a simple algorithm that represents each class by the centroid of its members. In effect, this makes it similar to the label updating phase of the <code>sklearn.KMeans</code> algorithm. It also has no parameters to choose, making it a good baseline classifier. It does, however, suffer on non-convex classes, as well as when classes have drastically different variances, as equal variance in all dimensions is assumed. See Linear Discriminant Analysis (<a class="reference internal" href="../generated/sklearn.discriminant_analysis.lineardiscriminantanalysis/#sklearn.discriminant_analysis.LinearDiscriminantAnalysis" title="sklearn.discriminant_analysis.LinearDiscriminantAnalysis"><code>sklearn.discriminant_analysis.LinearDiscriminantAnalysis</code></a>) and Quadratic Discriminant Analysis (<a class="reference internal" href="../generated/sklearn.discriminant_analysis.quadraticdiscriminantanalysis/#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis" title="sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis"><code>sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis</code></a>) for more complex methods that do not make this assumption. Usage of the default <a class="reference internal" href="../generated/sklearn.neighbors.nearestcentroid/#sklearn.neighbors.NearestCentroid" title="sklearn.neighbors.NearestCentroid"><code>NearestCentroid</code></a> is simple:</p> <pre data-language="python">&gt;&gt;&gt; from sklearn.neighbors.nearest_centroid import NearestCentroid
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
&gt;&gt;&gt; y = np.array([1, 1, 1, 2, 2, 2])
&gt;&gt;&gt; clf = NearestCentroid()
&gt;&gt;&gt; clf.fit(X, y)
NearestCentroid(metric='euclidean', shrink_threshold=None)
&gt;&gt;&gt; print(clf.predict([[-0.8, -1]]))
[1]
</pre>  <h3 id="nearest-shrunken-centroid">1.6.5.1. Nearest Shrunken Centroid</h3> <p>The <a class="reference internal" href="../generated/sklearn.neighbors.nearestcentroid/#sklearn.neighbors.NearestCentroid" title="sklearn.neighbors.NearestCentroid"><code>NearestCentroid</code></a> classifier has a <code>shrink_threshold</code> parameter, which implements the nearest shrunken centroid classifier. In effect, the value of each feature for each centroid is divided by the within-class variance of that feature. The feature values are then reduced by <code>shrink_threshold</code>. Most notably, if a particular feature value crosses zero, it is set to zero. In effect, this removes the feature from affecting the classification. This is useful, for example, for removing noisy features.</p> <p>In the example below, using a small shrink threshold increases the accuracy of the model from 0.81 to 0.82.</p> <p class="centered"> <strong><a class="reference external" href="../../auto_examples/neighbors/plot_nearest_centroid/"><img alt="nearest_centroid_1" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_nearest_centroid_0011.png" style="width: 400.0px; height: 300.0px;"></a> <a class="reference external" href="../../auto_examples/neighbors/plot_nearest_centroid/"><img alt="nearest_centroid_2" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_nearest_centroid_0021.png" style="width: 400.0px; height: 300.0px;"></a></strong></p>
<div class="topic"> <p class="topic-title first">Examples:</p> <ul class="simple"> <li>
<a class="reference internal" href="../../auto_examples/neighbors/plot_nearest_centroid/#sphx-glr-auto-examples-neighbors-plot-nearest-centroid-py"><span class="std std-ref">Nearest Centroid Classification</span></a>: an example of classification using nearest centroid with different shrink thresholds.</li> </ul> </div>    <h2 id="id4">1.6.6. Approximate Nearest Neighbors</h2> <p id="approximate-nearest-neighbors">There are many efficient exact nearest neighbor search algorithms for low dimensions <img class="math" src="http://scikit-learn.org/stable/_images/math/b9d10b54744d07746b97f53c55eb98046fd76c8c.png" alt="d"> (approximately 50). However these algorithms perform poorly with respect to space and query time when <img class="math" src="http://scikit-learn.org/stable/_images/math/b9d10b54744d07746b97f53c55eb98046fd76c8c.png" alt="d"> increases. These algorithms are not any better than comparing query point to each point from the database in a high dimension (see <a class="reference internal" href="#brute-force"><span class="std std-ref">Brute Force</span></a>). This is a well-known consequence of the phenomenon called “The Curse of Dimensionality”.</p> <p>There are certain applications where we do not need the exact nearest neighbors but having a “good guess” would suffice. When answers do not have to be exact, the <a class="reference internal" href="../generated/sklearn.neighbors.lshforest/#sklearn.neighbors.LSHForest" title="sklearn.neighbors.LSHForest"><code>LSHForest</code></a> class implements an approximate nearest neighbor search. Approximate nearest neighbor search methods have been designed to try to speedup query time with high dimensional data. These techniques are useful when the aim is to characterize the neighborhood rather than identifying the exact neighbors themselves (eg: k-nearest neighbors classification and regression). Some of the most popular approximate nearest neighbor search techniques are locality sensitive hashing, best bin fit and balanced box-decomposition tree based search.</p>  <h3 id="locality-sensitive-hashing-forest">1.6.6.1. Locality Sensitive Hashing Forest</h3> <p>The vanilla implementation of locality sensitive hashing has a hyper-parameter that is hard to tune in practice, therefore scikit-learn implements a variant called <a class="reference internal" href="../generated/sklearn.neighbors.lshforest/#sklearn.neighbors.LSHForest" title="sklearn.neighbors.LSHForest"><code>LSHForest</code></a> that has more reasonable hyperparameters. Both methods use internally random hyperplanes to index the samples into buckets and actual cosine similarities are only computed for samples that collide with the query hence achieving sublinear scaling. (see <a class="reference internal" href="#mathematical-description-of-lsh"><span class="std std-ref">Mathematical description of Locality Sensitive Hashing</span></a>).</p> <p><a class="reference internal" href="../generated/sklearn.neighbors.lshforest/#sklearn.neighbors.LSHForest" title="sklearn.neighbors.LSHForest"><code>LSHForest</code></a> has two main hyper-parameters: <code>n_estimators</code> and <code>n_candidates</code>. The accuracy of queries can be controlled using these parameters as demonstrated in the following plots:</p> <div class="figure align-center"> <a class="reference external image-reference" href="../../auto_examples/neighbors/plot_approximate_nearest_neighbors_hyperparameters/"><img alt="../_images/sphx_glr_plot_approximate_nearest_neighbors_hyperparameters_0011.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_approximate_nearest_neighbors_hyperparameters_0011.png" style="width: 400.0px; height: 300.0px;"></a> </div> <div class="figure align-center"> <a class="reference external image-reference" href="../../auto_examples/neighbors/plot_approximate_nearest_neighbors_hyperparameters/"><img alt="../_images/sphx_glr_plot_approximate_nearest_neighbors_hyperparameters_0021.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_approximate_nearest_neighbors_hyperparameters_0021.png" style="width: 400.0px; height: 300.0px;"></a> </div> <p>As a rule of thumb, a user can set <code>n_estimators</code> to a large enough value (e.g. between 10 and 50) and then adjust <code>n_candidates</code> to trade off accuracy for query time.</p> <p>For small data sets, the brute force method for exact nearest neighbor search can be faster than LSH Forest. However LSH Forest has a sub-linear query time scalability with the index size. The exact break even point where LSH Forest queries become faster than brute force depends on the dimensionality, structure of the dataset, required level of precision, characteristics of the runtime environment such as availability of BLAS optimizations, number of CPU cores and size of the CPU caches. Following graphs depict scalability of LSHForest queries with index size.</p> <div class="figure align-center"> <a class="reference external image-reference" href="../../auto_examples/neighbors/plot_approximate_nearest_neighbors_scalability/"><img alt="../_images/sphx_glr_plot_approximate_nearest_neighbors_scalability_0011.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_approximate_nearest_neighbors_scalability_0011.png" style="width: 400.0px; height: 300.0px;"></a> </div> <div class="figure align-center"> <a class="reference external image-reference" href="../../auto_examples/neighbors/plot_approximate_nearest_neighbors_scalability/"><img alt="../_images/sphx_glr_plot_approximate_nearest_neighbors_scalability_0021.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_approximate_nearest_neighbors_scalability_0021.png" style="width: 400.0px; height: 300.0px;"></a> </div> <div class="figure align-center"> <a class="reference external image-reference" href="../../auto_examples/neighbors/plot_approximate_nearest_neighbors_scalability/"><img alt="../_images/sphx_glr_plot_approximate_nearest_neighbors_scalability_0031.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_approximate_nearest_neighbors_scalability_0031.png" style="width: 400.0px; height: 300.0px;"></a> </div> <p>For fixed <a class="reference internal" href="../generated/sklearn.neighbors.lshforest/#sklearn.neighbors.LSHForest" title="sklearn.neighbors.LSHForest"><code>LSHForest</code></a> parameters, the accuracy of queries tends to slowly decrease with larger datasets. The error bars on the previous plots represent standard deviation across different queries.</p> <div class="topic"> <p class="topic-title first">Examples:</p> <ul class="simple"> <li>
<a class="reference internal" href="../../auto_examples/neighbors/plot_approximate_nearest_neighbors_hyperparameters/#sphx-glr-auto-examples-neighbors-plot-approximate-nearest-neighbors-hyperparameters-py"><span class="std std-ref">Hyper-parameters of Approximate Nearest Neighbors</span></a>: an example of the behavior of hyperparameters of approximate nearest neighbor search using LSH Forest.</li> <li>
<a class="reference internal" href="../../auto_examples/neighbors/plot_approximate_nearest_neighbors_scalability/#sphx-glr-auto-examples-neighbors-plot-approximate-nearest-neighbors-scalability-py"><span class="std std-ref">Scalability of Approximate Nearest Neighbors</span></a>: an example of scalability of approximate nearest neighbor search using LSH Forest.</li> </ul> </div>   <h3 id="mathematical-description-of-lsh">1.6.6.2. Mathematical description of Locality Sensitive Hashing</h3> <p id="mathematical-description-of-locality-sensitive-hashing">Locality sensitive hashing (LSH) techniques have been used in many areas where nearest neighbor search is performed in high dimensions. The main concept behind LSH is to hash each data point in the database using multiple (often simple) hash functions to form a digest (also called a <em>hash</em>). At this point the probability of collision - where two objects have similar digests - is much higher for the points which are close to each other than that of the distant points. We describe the requirements for a hash function family to be locality sensitive as follows.</p> <p>A family <img class="math" src="http://scikit-learn.org/stable/_images/math/44d4c3f22452acebfb3ac644e23508b67500be90.png" alt="H"> of functions from a domain <img class="math" src="http://scikit-learn.org/stable/_images/math/1dbc400fcc213305415872f9f625cd2828f97a00.png" alt="S"> to a range <img class="math" src="http://scikit-learn.org/stable/_images/math/169c982b5e649cd1bd6445df5583affe427ea269.png" alt="U"> is called <img class="math" src="http://scikit-learn.org/stable/_images/math/70595d4e8a0cbdd8617a2f5b72d46c2ca7e4843e.png" alt="(r, e , p1 , p2 )">-sensitive, with <img class="math" src="http://scikit-learn.org/stable/_images/math/3051ae12023b8c69c207c48413be5c4bc738c471.png" alt="r, e &gt; 0">, <img class="math" src="http://scikit-learn.org/stable/_images/math/4f48406b4c0cbdb6d59c32a261637d5428acdfb0.png" alt="p_1 &gt; p_2 &gt; 0">, if for any <img class="math" src="http://scikit-learn.org/stable/_images/math/4e7be5fd643db3539184616ade499abd7fcade02.png" alt="p, q \in S">, the following conditions hold (<img class="math" src="http://scikit-learn.org/stable/_images/math/3cf5aa5abe312e6fd4996975341a43aa566e8d8d.png" alt="D"> is the distance function):</p> <ul class="simple"> <li>If <img class="math" src="http://scikit-learn.org/stable/_images/math/90a114f26f426db08807f7c7a9ba299896484708.png" alt="D(p,q) &lt;= r"> then <img class="math" src="http://scikit-learn.org/stable/_images/math/42fa810b684c1ac9520177829961da46d08b7274.png" alt="P_H[h(p) = h(q)] &gt;= p_1">,</li> <li>If <img class="math" src="http://scikit-learn.org/stable/_images/math/ab87e2e8790ac301dd7e1f903d4f2bace9f41c4f.png" alt="D(p,q) &gt; r(1 + e)"> then <img class="math" src="http://scikit-learn.org/stable/_images/math/833d29883bebc2bd25ed0444f1c9004e1e12e573.png" alt="P_H[h(p) = h(q)] &lt;= p_2">.</li> </ul> <p>As defined, nearby points within a distance of <img class="math" src="http://scikit-learn.org/stable/_images/math/eaa6ad49a7f78fe5a13b486690163bf2dc7e3e60.png" alt="r"> to each other are likely to collide with probability <img class="math" src="http://scikit-learn.org/stable/_images/math/228f316f91f921b34d0d336eb51c8ffab9064b0a.png" alt="p_1">. In contrast, distant points which are located with the distance more than <img class="math" src="http://scikit-learn.org/stable/_images/math/8c618a132d4355d2cb07a7f8554af64440eee50c.png" alt="r(1 + e)"> have a small probability of <img class="math" src="http://scikit-learn.org/stable/_images/math/9d97f98ae30ef6d0a625749afe33bce99458f167.png" alt="p_2"> of collision. Suppose there is a family of LSH function <img class="math" src="http://scikit-learn.org/stable/_images/math/44d4c3f22452acebfb3ac644e23508b67500be90.png" alt="H">. An <em>LSH index</em> is built as follows:</p> <ol class="arabic simple"> <li>Choose <img class="math" src="http://scikit-learn.org/stable/_images/math/0b7c1e16a3a8a849bb8ffdcdbf86f65fd1f30438.png" alt="k"> functions <img class="math" src="http://scikit-learn.org/stable/_images/math/8c9828f35703158f49c358897973e1a20378ffcc.png" alt="h_1, h_2, … h_k"> uniformly at random (with replacement) from <img class="math" src="http://scikit-learn.org/stable/_images/math/44d4c3f22452acebfb3ac644e23508b67500be90.png" alt="H">. For any <img class="math" src="http://scikit-learn.org/stable/_images/math/a4398bb42aaae334a73687922d07fd753c56eb19.png" alt="p \in S">, place <img class="math" src="http://scikit-learn.org/stable/_images/math/27d463da4622be5b3ef1d4176ced7d7a323c6425.png" alt="p"> in the bucket with label <img class="math" src="http://scikit-learn.org/stable/_images/math/86b1b7a705ea6e6f35c1095219c1254a5cbb8db3.png" alt="g(p) = (h_1(p), h_2(p), … h_k(p))">. Observe that if each <img class="math" src="http://scikit-learn.org/stable/_images/math/d03a4b38f7e618d6826778cf0a8b906fbbaa935a.png" alt="h_i"> outputs one “digit”, each bucket has a k-digit label.</li> <li>Independently perform step 1 <img class="math" src="http://scikit-learn.org/stable/_images/math/b359488b993294ebbc2c7b30ab8f749dcbc6826d.png" alt="l"> times to construct <img class="math" src="http://scikit-learn.org/stable/_images/math/b359488b993294ebbc2c7b30ab8f749dcbc6826d.png" alt="l"> separate estimators, with hash functions <img class="math" src="http://scikit-learn.org/stable/_images/math/f62aec5e6c13a81f9a933cab06a58e1bc8caef7d.png" alt="g_1, g_2, … g_l">.</li> </ol> <p>The reason to concatenate hash functions in the step 1 is to decrease the probability of the collision of distant points as much as possible. The probability drops from <img class="math" src="http://scikit-learn.org/stable/_images/math/9d97f98ae30ef6d0a625749afe33bce99458f167.png" alt="p_2"> to <img class="math" src="http://scikit-learn.org/stable/_images/math/e4de9dbf1084d644ec098de23b373392d93d33d1.png" alt="p_2^k"> which is negligibly small for large <img class="math" src="http://scikit-learn.org/stable/_images/math/0b7c1e16a3a8a849bb8ffdcdbf86f65fd1f30438.png" alt="k">. The choice of <img class="math" src="http://scikit-learn.org/stable/_images/math/0b7c1e16a3a8a849bb8ffdcdbf86f65fd1f30438.png" alt="k"> is strongly dependent on the data set size and structure and is therefore hard to tune in practice. There is a side effect of having a large <img class="math" src="http://scikit-learn.org/stable/_images/math/0b7c1e16a3a8a849bb8ffdcdbf86f65fd1f30438.png" alt="k">; it has the potential of decreasing the chance of nearby points getting collided. To address this issue, multiple estimators are constructed in step 2.</p> <p>The requirement to tune <img class="math" src="http://scikit-learn.org/stable/_images/math/0b7c1e16a3a8a849bb8ffdcdbf86f65fd1f30438.png" alt="k"> for a given dataset makes classical LSH cumbersome to use in practice. The LSH Forest variant has benn designed to alleviate this requirement by automatically adjusting the number of digits used to hash the samples.</p> <p>LSH Forest is formulated with prefix trees with each leaf of a tree corresponding to an actual data point in the database. There are <img class="math" src="http://scikit-learn.org/stable/_images/math/b359488b993294ebbc2c7b30ab8f749dcbc6826d.png" alt="l"> such trees which compose the forest and they are constructed using independently drawn random sequence of hash functions from <img class="math" src="http://scikit-learn.org/stable/_images/math/44d4c3f22452acebfb3ac644e23508b67500be90.png" alt="H">. In this implementation, “Random Projections” is being used as the LSH technique which is an approximation for the cosine distance. The length of the sequence of hash functions is kept fixed at 32. Moreover, a prefix tree is implemented using sorted arrays and binary search.</p> <p>There are two phases of tree traversals used in order to answer a query to find the <img class="math" src="http://scikit-learn.org/stable/_images/math/edba97b4c0d864d26e92ea7ea73767fa38eef3f7.png" alt="m"> nearest neighbors of a point <img class="math" src="http://scikit-learn.org/stable/_images/math/620a3ce6403ec82f1347af9985bc03f7a9382f4a.png" alt="q">. First, a top-down traversal is performed using a binary search to identify the leaf having the longest prefix match (maximum depth) with <img class="math" src="http://scikit-learn.org/stable/_images/math/620a3ce6403ec82f1347af9985bc03f7a9382f4a.png" alt="q">‘s label after subjecting <img class="math" src="http://scikit-learn.org/stable/_images/math/620a3ce6403ec82f1347af9985bc03f7a9382f4a.png" alt="q"> to the same hash functions. <img class="math" src="http://scikit-learn.org/stable/_images/math/cf97aad519ba6671670f4e101c49779533de6ff8.png" alt="M &gt;&gt; m"> points (total candidates) are extracted from the forest, moving up from the previously found maximum depth towards the root synchronously across all trees in the bottom-up traversal. <code>M</code> is set to <img class="math" src="http://scikit-learn.org/stable/_images/math/a68271e91067c0ccecf399d0be18663098d55adf.png" alt="cl"> where <img class="math" src="http://scikit-learn.org/stable/_images/math/ae12a24f88803b5895632e4848d87d46483c492c.png" alt="c">, the number of candidates extracted from each tree, is a constant. Finally, the similarity of each of these <img class="math" src="http://scikit-learn.org/stable/_images/math/450a8e2c2320d77181e0d4fc68c947e9a5de8ecb.png" alt="M"> points against point <img class="math" src="http://scikit-learn.org/stable/_images/math/620a3ce6403ec82f1347af9985bc03f7a9382f4a.png" alt="q"> is calculated and the top <img class="math" src="http://scikit-learn.org/stable/_images/math/edba97b4c0d864d26e92ea7ea73767fa38eef3f7.png" alt="m"> points are returned as the nearest neighbors of <img class="math" src="http://scikit-learn.org/stable/_images/math/620a3ce6403ec82f1347af9985bc03f7a9382f4a.png" alt="q">. Since most of the time in these queries is spent calculating the distances to candidates, the speedup compared to brute force search is approximately <img class="math" src="http://scikit-learn.org/stable/_images/math/dcdc5dff154b24eb169f12f30eeead7852a5e8a6.png" alt="N/M">, where <img class="math" src="http://scikit-learn.org/stable/_images/math/f4170ed8938b79490d8923857962695514a8e4cb.png" alt="N"> is the number of points in database.</p> <div class="topic"> <p class="topic-title first">References:</p> <ul class="simple"> <li>
<a class="reference external" href="http://web.mit.edu/andoni/www/papers/cSquared.pdf" target="_blank">“Near-Optimal Hashing Algorithms for Approximate Nearest Neighbor in High Dimensions”</a>, Alexandr, A., Indyk, P., Foundations of Computer Science, 2006. FOCS ‘06. 47th Annual IEEE Symposium</li> <li>
<a class="reference external" href="http://infolab.stanford.edu/~bawa/Pub/similarity.pdf" target="_blank">“LSH Forest: Self-Tuning Indexes for Similarity Search”</a>, Bawa, M., Condie, T., Ganesan, P., WWW ‘05 Proceedings of the 14th international conference on World Wide Web Pages 651-660</li> </ul> </div>
<div class="_attribution">
  <p class="_attribution-p">
    © 2007–2016 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="http://scikit-learn.org/stable/modules/neighbors.html" class="_attribution-link" target="_blank">http://scikit-learn.org/stable/modules/neighbors.html</a>
  </p>
</div>

			</div>
		</div>
	</section>

	</div>
</body>
</html>
