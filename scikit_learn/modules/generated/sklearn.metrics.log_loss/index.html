
<!DOCTYPE HTML>

<html lang="en">

<head>
  <meta charset="utf-8">
  <title>sklearn.metrics.log_loss() - Scikit-learn - W3cubDocs</title>
  
  <meta name="description" content=" Log loss, aka logistic loss or cross-entropy loss. ">
  <meta name="keywords" content="sklearn, metrics, log, loss, -, scikit-learn, scikit_learn">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="http://docs.w3cub.com/scikit_learn/modules/generated/sklearn.metrics.log_loss/">
  <link href="/favicon.png" rel="icon">
  <link type="text/css" rel="stylesheet" href="/assets/application-50364fff564ce3b6327021805f3f00e2957b441cf27f576a7dd4ff63bbc47047.css">
  <script type="text/javascript" src="/assets/application-db64bfd54ceb42be11af7995804cf4902548419ceb79d509b0b7d62c22d98e6f.js"></script>
  <script src="/json/scikit_learn.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body>
	<div class="_app">
	<header class="_header">
  
  <form class="_search">
    <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
    <a class="_search-clear"></a>
    <div class="_search-tag"></div>
  </form>
  
  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/scikit_learn/" class="_nav-link" title="" style="margin-left:0;">scikit-learn</a></span>
  
  <nav class="_nav">
    <a href="/app/" class="_nav-link ">App</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		<div class="_list">
			
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<div class="_page _sphinx">
				
<h1 id="sklearn-metrics-log-loss">sklearn.metrics.log_loss</h1> <dl class="function"> <dt id="sklearn.metrics.log_loss">
<code>sklearn.metrics.log_loss(y_true, y_pred, eps=1e-15, normalize=True, sample_weight=None, labels=None)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/metrics/classification.py#L1544" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Log loss, aka logistic loss or cross-entropy loss.</p> <p>This is the loss function used in (multinomial) logistic regression and extensions of it such as neural networks, defined as the negative log-likelihood of the true labels given a probabilistic classifier’s predictions. The log loss is only defined for two or more labels. For a single sample with true label yt in {0,1} and estimated probability yp that yt = 1, the log loss is</p>  -log P(yt|yp) = -(yt log(yp) + (1 - yt) log(1 - yp)) <p>Read more in the <a class="reference internal" href="../../model_evaluation/#log-loss"><span class="std std-ref">User Guide</span></a>.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr class="field-odd field">
<th class="field-name">Parameters:</th>
<td class="field-body">
<p class="first"><strong>y_true</strong> : array-like or label indicator matrix</p>  <p>Ground truth (correct) labels for n_samples samples.</p>  <p><strong>y_pred</strong> : array-like of float, shape = (n_samples, n_classes) or (n_samples,)</p>  <p>Predicted probabilities, as returned by a classifier’s predict_proba method. If <code>y_pred.shape = (n_samples,)</code> the probabilities provided are assumed to be that of the positive class. The labels in <code>y_pred</code> are assumed to be ordered alphabetically, as done by <code>preprocessing.LabelBinarizer</code>.</p>  <p><strong>eps</strong> : float</p>  <p>Log loss is undefined for p=0 or p=1, so probabilities are clipped to max(eps, min(1 - eps, p)).</p>  <p><strong>normalize</strong> : bool, optional (default=True)</p>  <p>If true, return the mean loss per sample. Otherwise, return the sum of the per-sample losses.</p>  <p><strong>sample_weight</strong> : array-like of shape = [n_samples], optional</p>  <p>Sample weights.</p>  <p><strong>labels</strong> : array-like, optional (default=None)</p>  <p>If not provided, labels will be inferred from y_true. If <code>labels</code> is <code>None</code> and <code>y_pred</code> has shape (n_samples,) the labels are assumed to be binary and are inferred from <code>y_true</code>. .. versionadded:: 0.18</p>  </td> </tr> <tr class="field-even field">
<th class="field-name">Returns:</th>
<td class="field-body">
<p class="first last"><strong>loss</strong> : float</p> </td> </tr>  </table> <h4 class="rubric">Notes</h4> <p>The logarithm used is the natural logarithm (base-e).</p> <h4 class="rubric">References</h4> <p>C.M. Bishop (2006). Pattern Recognition and Machine Learning. Springer, p. 209.</p> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; log_loss(["spam", "ham", "ham", "spam"],  
...          [[.1, .9], [.9, .1], [.8, .2], [.35, .65]])
0.21616...
</pre> </dd>
</dl>  <h2 id="examples-using-sklearn-metrics-log-loss">Examples using <code>sklearn.metrics.log_loss</code>
</h2> <div class="sphx-glr-thumbcontainer" tooltip="This example illustrates how sigmoid calibration changes predicted probabilities for a 3-class ...">
<div class="figure" id="id1"> <img alt="../../_images/sphx_glr_plot_calibration_multiclass_thumb.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_calibration_multiclass_thumb.png"> <p class="caption"><span class="caption-text"><a class="reference internal" href="../../../auto_examples/calibration/plot_calibration_multiclass/#sphx-glr-auto-examples-calibration-plot-calibration-multiclass-py"><span class="std std-ref">Probability Calibration for 3-class classification</span></a></span></p> </div> </div>
<div class="sphx-glr-thumbcontainer" tooltip="This example illustrates the predicted probability of GPC for an RBF kernel with different choi...">
<div class="figure" id="id2"> <img alt="../../_images/sphx_glr_plot_gpc_thumb.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_gpc_thumb.png"> <p class="caption"><span class="caption-text"><a class="reference internal" href="../../../auto_examples/gaussian_process/plot_gpc/#sphx-glr-auto-examples-gaussian-process-plot-gpc-py"><span class="std std-ref">Probabilistic predictions with Gaussian process classification (GPC)</span></a></span></p> </div> </div>
<div class="_attribution">
  <p class="_attribution-p">
    © 2007–2016 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html" class="_attribution-link" target="_blank">http://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html</a>
  </p>
</div>

			</div>
		</div>
	</section>

	</div>
</body>
</html>
