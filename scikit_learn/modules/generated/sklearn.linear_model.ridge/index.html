
<!DOCTYPE HTML>

<html lang="en">

<head>
  <meta charset="utf-8">
  <title>linear_model.Ridge() - Scikit-learn - W3cubDocs</title>
  
  <meta name="description" content=" Linear least squares with l2 regularization. ">
  <meta name="keywords" content="sklearn, linear, model, ridge, -, scikit-learn, scikit_learn">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="http://docs.w3cub.com/scikit_learn/modules/generated/sklearn.linear_model.ridge/">
  <link href="/favicon.png" rel="icon">
  <link type="text/css" rel="stylesheet" href="/assets/application-50364fff564ce3b6327021805f3f00e2957b441cf27f576a7dd4ff63bbc47047.css">
  <script type="text/javascript" src="/assets/application-db64bfd54ceb42be11af7995804cf4902548419ceb79d509b0b7d62c22d98e6f.js"></script>
  <script src="/json/scikit_learn.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body>
	<div class="_app">
	<header class="_header">
  
  <form class="_search">
    <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
    <a class="_search-clear"></a>
    <div class="_search-tag"></div>
  </form>
  
  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/scikit_learn/" class="_nav-link" title="" style="margin-left:0;">scikit-learn</a></span>
  
  <nav class="_nav">
    <a href="/app/" class="_nav-link ">App</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		<div class="_list">
			
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<div class="_page _sphinx">
				
<h1 id="sklearn-linear-model-ridge">sklearn.linear_model.Ridge</h1> <dl class="class"> <dt id="sklearn.linear_model.Ridge">
<code>class sklearn.linear_model.Ridge(alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver='auto', random_state=None)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/linear_model/ridge.py#L494" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Linear least squares with l2 regularization.</p> <p>This model solves a regression model where the loss function is the linear least squares function and regularization is given by the l2-norm. Also known as Ridge Regression or Tikhonov regularization. This estimator has built-in support for multi-variate regression (i.e., when y is a 2d-array of shape [n_samples, n_targets]).</p> <p>Read more in the <a class="reference internal" href="../../linear_model/#ridge-regression"><span class="std std-ref">User Guide</span></a>.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr class="field-odd field">
<th class="field-name">Parameters:</th>
<td class="field-body">
<p class="first"><strong>alpha</strong> : {float, array-like}, shape (n_targets)</p>  <p>Regularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization. Alpha corresponds to <code>C^-1</code> in other linear models such as LogisticRegression or LinearSVC. If an array is passed, penalties are assumed to be specific to the targets. Hence they must correspond in number.</p>  <p><strong>copy_X</strong> : boolean, optional, default True</p>  <p>If True, X will be copied; else, it may be overwritten.</p>  <p><strong>fit_intercept</strong> : boolean</p>  <p>Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered).</p>  <p><strong>max_iter</strong> : int, optional</p>  <p>Maximum number of iterations for conjugate gradient solver. For ‘sparse_cg’ and ‘lsqr’ solvers, the default value is determined by scipy.sparse.linalg. For ‘sag’ solver, the default value is 1000.</p>  <p><strong>normalize</strong> : boolean, optional, default False</p>  <p>If True, the regressors X will be normalized before regression. This parameter is ignored when <code>fit_intercept</code> is set to False. When the regressors are normalized, note that this makes the hyperparameters learnt more robust and almost independent of the number of samples. The same property is not valid for standardized data. However, if you wish to standardize, please use <code>preprocessing.StandardScaler</code> before calling <code>fit</code> on an estimator with <code>normalize=False</code>.</p>  <p><strong>solver</strong> : {‘auto’, ‘svd’, ‘cholesky’, ‘lsqr’, ‘sparse_cg’, ‘sag’}</p>  <p>Solver to use in the computational routines:</p> <ul class="simple"> <li>‘auto’ chooses the solver automatically based on the type of data.</li> <li>‘svd’ uses a Singular Value Decomposition of X to compute the Ridge coefficients. More stable for singular matrices than ‘cholesky’.</li> <li>‘cholesky’ uses the standard scipy.linalg.solve function to obtain a closed-form solution.</li> <li>‘sparse_cg’ uses the conjugate gradient solver as found in scipy.sparse.linalg.cg. As an iterative algorithm, this solver is more appropriate than ‘cholesky’ for large-scale data (possibility to set <code>tol</code> and <code>max_iter</code>).</li> <li>‘lsqr’ uses the dedicated regularized least-squares routine scipy.sparse.linalg.lsqr. It is the fastest but may not be available in old scipy versions. It also uses an iterative procedure.</li> <li>‘sag’ uses a Stochastic Average Gradient descent. It also uses an iterative procedure, and is often faster than other solvers when both n_samples and n_features are large. Note that ‘sag’ fast convergence is only guaranteed on features with approximately the same scale. You can preprocess the data with a scaler from sklearn.preprocessing.</li> </ul> <p>All last four solvers support both dense and sparse data. However, only ‘sag’ supports sparse input when <code>fit_intercept</code> is True.</p> <div class="versionadded"> <p><span class="versionmodified">New in version 0.17: </span>Stochastic Average Gradient descent solver.</p> </div>  <p><strong>tol</strong> : float</p>  <p>Precision of the solution.</p>  <p><strong>random_state</strong> : int seed, RandomState instance, or None (default)</p>  <p>The seed of the pseudo random number generator to use when shuffling the data. Used only in ‘sag’ solver.</p> <div class="versionadded"> <p><span class="versionmodified">New in version 0.17: </span><em>random_state</em> to support Stochastic Average Gradient.</p> </div>  </td> </tr> <tr class="field-even field">
<th class="field-name">Attributes:</th>
<td class="field-body">
<p class="first"><strong>coef_</strong> : array, shape (n_features,) or (n_targets, n_features)</p>  <p>Weight vector(s).</p>  <p><strong>intercept_</strong> : float | array, shape = (n_targets,)</p>  <p>Independent term in decision function. Set to 0.0 if <code>fit_intercept = False</code>.</p>  <p><strong>n_iter_</strong> : array or None, shape (n_targets,)</p>  <p>Actual number of iterations for each target. Available only for sag and lsqr solvers. Other solvers will return None.</p> <div class="versionadded"> <p><span class="versionmodified">New in version 0.17.</span></p> </div>  </td> </tr>  </table> <div class="admonition seealso"> <p class="first admonition-title">See also</p> <p class="last"><a class="reference internal" href="../sklearn.linear_model.ridgeclassifier/#sklearn.linear_model.RidgeClassifier" title="sklearn.linear_model.RidgeClassifier"><code>RidgeClassifier</code></a>, <a class="reference internal" href="../sklearn.linear_model.ridgecv/#sklearn.linear_model.RidgeCV" title="sklearn.linear_model.RidgeCV"><code>RidgeCV</code></a>, <a class="reference internal" href="../sklearn.kernel_ridge.kernelridge/#sklearn.kernel_ridge.KernelRidge" title="sklearn.kernel_ridge.KernelRidge"><code>sklearn.kernel_ridge.KernelRidge</code></a></p> </div> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; from sklearn.linear_model import Ridge
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; n_samples, n_features = 10, 5
&gt;&gt;&gt; np.random.seed(0)
&gt;&gt;&gt; y = np.random.randn(n_samples)
&gt;&gt;&gt; X = np.random.randn(n_samples, n_features)
&gt;&gt;&gt; clf = Ridge(alpha=1.0)
&gt;&gt;&gt; clf.fit(X, y) 
Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,
      normalize=False, random_state=None, solver='auto', tol=0.001)
</pre> <h4 class="rubric">Methods</h4> <table class="longtable docutils">   <tr class="row-odd">
<td>
<a class="reference internal" href="#sklearn.linear_model.Ridge.decision_function" title="sklearn.linear_model.Ridge.decision_function"><code>decision_function</code></a>(*args, **kwargs)</td> <td>DEPRECATED: and will be removed in 0.19.</td> </tr> <tr class="row-even">
<td>
<a class="reference internal" href="#sklearn.linear_model.Ridge.fit" title="sklearn.linear_model.Ridge.fit"><code>fit</code></a>(X, y[, sample_weight])</td> <td>Fit Ridge regression model</td> </tr> <tr class="row-odd">
<td>
<a class="reference internal" href="#sklearn.linear_model.Ridge.get_params" title="sklearn.linear_model.Ridge.get_params"><code>get_params</code></a>([deep])</td> <td>Get parameters for this estimator.</td> </tr> <tr class="row-even">
<td>
<a class="reference internal" href="#sklearn.linear_model.Ridge.predict" title="sklearn.linear_model.Ridge.predict"><code>predict</code></a>(X)</td> <td>Predict using the linear model</td> </tr> <tr class="row-odd">
<td>
<a class="reference internal" href="#sklearn.linear_model.Ridge.score" title="sklearn.linear_model.Ridge.score"><code>score</code></a>(X, y[, sample_weight])</td> <td>Returns the coefficient of determination R^2 of the prediction.</td> </tr> <tr class="row-even">
<td>
<a class="reference internal" href="#sklearn.linear_model.Ridge.set_params" title="sklearn.linear_model.Ridge.set_params"><code>set_params</code></a>(**params)</td> <td>Set the parameters of this estimator.</td> </tr>  </table> <dl class="method"> <dt id="sklearn.linear_model.Ridge.__init__">
<code>__init__(alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver='auto', random_state=None)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/linear_model/ridge.py#L616" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="sklearn.linear_model.Ridge.decision_function">
<code>decision_function(*args, **kwargs)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/utils/deprecation.py#L69" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>DEPRECATED: and will be removed in 0.19.</p> <p>Decision function of the linear model.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr class="field-odd field">
<th class="field-name">Parameters:</th>
<td class="field-body">
<p class="first"><strong>X</strong> : {array-like, sparse matrix}, shape = (n_samples, n_features)</p>  <p>Samples.</p>  </td> </tr> <tr class="field-even field">
<th class="field-name">Returns:</th>
<td class="field-body">
<p class="first"><strong>C</strong> : array, shape = (n_samples,)</p>  <p>Returns predicted values.</p>  </td> </tr>  </table> </dd>
</dl> <dl class="method"> <dt id="sklearn.linear_model.Ridge.fit">
<code>fit(X, y, sample_weight=None)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/linear_model/ridge.py#L624" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Fit Ridge regression model</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr class="field-odd field">
<th class="field-name">Parameters:</th>
<td class="field-body">
<p class="first"><strong>X</strong> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>  <p>Training data</p>  <p><strong>y</strong> : array-like, shape = [n_samples] or [n_samples, n_targets]</p>  <p>Target values</p>  <p><strong>sample_weight</strong> : float or numpy array of shape [n_samples]</p>  <p>Individual weights for each sample</p>  </td> </tr> <tr class="field-even field">
<th class="field-name">Returns:</th>
<td class="field-body">
<p class="first last"><strong>self</strong> : returns an instance of self.</p> </td> </tr>  </table> </dd>
</dl> <dl class="method"> <dt id="sklearn.linear_model.Ridge.get_params">
<code>get_params(deep=True)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/base.py#L220" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Get parameters for this estimator.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr class="field-odd field">
<th class="field-name">Parameters:</th>
<td class="field-body">
<p class="first"><strong>deep: boolean, optional</strong> :</p>  <p>If True, will return the parameters for this estimator and contained subobjects that are estimators.</p>  </td> </tr> <tr class="field-even field">
<th class="field-name">Returns:</th>
<td class="field-body">
<p class="first"><strong>params</strong> : mapping of string to any</p>  <p>Parameter names mapped to their values.</p>  </td> </tr>  </table> </dd>
</dl> <dl class="method"> <dt id="sklearn.linear_model.Ridge.predict">
<code>predict(X)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/linear_model/base.py#L255" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Predict using the linear model</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr class="field-odd field">
<th class="field-name">Parameters:</th>
<td class="field-body">
<p class="first"><strong>X</strong> : {array-like, sparse matrix}, shape = (n_samples, n_features)</p>  <p>Samples.</p>  </td> </tr> <tr class="field-even field">
<th class="field-name">Returns:</th>
<td class="field-body">
<p class="first"><strong>C</strong> : array, shape = (n_samples,)</p>  <p>Returns predicted values.</p>  </td> </tr>  </table> </dd>
</dl> <dl class="method"> <dt id="sklearn.linear_model.Ridge.score">
<code>score(X, y, sample_weight=None)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/base.py#L357" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns the coefficient of determination R^2 of the prediction.</p> <p>The coefficient R^2 is defined as (1 - u/v), where u is the regression sum of squares ((y_true - y_pred) ** 2).sum() and v is the residual sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr class="field-odd field">
<th class="field-name">Parameters:</th>
<td class="field-body">
<p class="first"><strong>X</strong> : array-like, shape = (n_samples, n_features)</p>  <p>Test samples.</p>  <p><strong>y</strong> : array-like, shape = (n_samples) or (n_samples, n_outputs)</p>  <p>True values for X.</p>  <p><strong>sample_weight</strong> : array-like, shape = [n_samples], optional</p>  <p>Sample weights.</p>  </td> </tr> <tr class="field-even field">
<th class="field-name">Returns:</th>
<td class="field-body">
<p class="first"><strong>score</strong> : float</p>  <p>R^2 of self.predict(X) wrt. y.</p>  </td> </tr>  </table> </dd>
</dl> <dl class="method"> <dt id="sklearn.linear_model.Ridge.set_params">
<code>set_params(**params)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/base.py#L257" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Set the parameters of this estimator.</p> <p>The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <code>&lt;component&gt;__&lt;parameter&gt;</code> so that it’s possible to update each component of a nested object.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr class="field-odd field">
<th class="field-name">Returns:</th>
<td class="field-body">
<strong>self</strong> :</td> </tr>  </table> </dd>
</dl> </dd>
</dl>  <h2 id="examples-using-sklearn-linear-model-ridge">Examples using <code>sklearn.linear_model.Ridge</code>
</h2> <div class="sphx-glr-thumbcontainer" tooltip="This is an example showing the prediction latency of various scikit-learn estimators.">
<div class="figure" id="id1"> <img alt="../../_images/sphx_glr_plot_prediction_latency_thumb.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_prediction_latency_thumb.png"> <p class="caption"><span class="caption-text"><a class="reference internal" href="../../../auto_examples/applications/plot_prediction_latency/#sphx-glr-auto-examples-applications-plot-prediction-latency-py"><span class="std std-ref">Prediction Latency</span></a></span></p> </div> </div>
<div class="sphx-glr-thumbcontainer" tooltip="This example shows the reconstruction of an image from a set of parallel projections, acquired ...">
<div class="figure" id="id2"> <img alt="../../_images/sphx_glr_plot_tomography_l1_reconstruction_thumb.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_tomography_l1_reconstruction_thumb.png"> <p class="caption"><span class="caption-text"><a class="reference internal" href="../../../auto_examples/applications/plot_tomography_l1_reconstruction/#sphx-glr-auto-examples-applications-plot-tomography-l1-reconstruction-py"><span class="std std-ref">Compressive sensing: tomography reconstruction with L1 prior (Lasso)</span></a></span></p> </div> </div>
<div class="sphx-glr-thumbcontainer" tooltip="Fit Ridge and HuberRegressor on a dataset with outliers.">
<div class="figure" id="id3"> <img alt="../../_images/sphx_glr_plot_huber_vs_ridge_thumb.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_huber_vs_ridge_thumb.png"> <p class="caption"><span class="caption-text"><a class="reference internal" href="../../../auto_examples/linear_model/plot_huber_vs_ridge/#sphx-glr-auto-examples-linear-model-plot-huber-vs-ridge-py"><span class="std std-ref">HuberRegressor vs Ridge on dataset with strong outliers</span></a></span></p> </div> </div>
<div class="sphx-glr-thumbcontainer" tooltip="Ridge regression is basically minimizing a penalised version of the least-squared function. The...">
<div class="figure" id="id4"> <img alt="../../_images/sphx_glr_plot_ols_ridge_variance_thumb.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_ols_ridge_variance_thumb.png"> <p class="caption"><span class="caption-text"><a class="reference internal" href="../../../auto_examples/linear_model/plot_ols_ridge_variance/#sphx-glr-auto-examples-linear-model-plot-ols-ridge-variance-py"><span class="std std-ref">Ordinary Least Squares and Ridge Regression Variance</span></a></span></p> </div> </div>
<div class="sphx-glr-thumbcontainer" tooltip="This example demonstrates how to approximate a function with a polynomial of degree n_degree by...">
<div class="figure" id="id5"> <img alt="../../_images/sphx_glr_plot_polynomial_interpolation_thumb.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_polynomial_interpolation_thumb.png"> <p class="caption"><span class="caption-text"><a class="reference internal" href="../../../auto_examples/linear_model/plot_polynomial_interpolation/#sphx-glr-auto-examples-linear-model-plot-polynomial-interpolation-py"><span class="std std-ref">Polynomial interpolation</span></a></span></p> </div> </div>
<div class="sphx-glr-thumbcontainer" tooltip=".. currentmodule:: sklearn.linear_model">
<div class="figure" id="id6"> <img alt="../../_images/sphx_glr_plot_ridge_coeffs_thumb.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_ridge_coeffs_thumb.png"> <p class="caption"><span class="caption-text"><a class="reference internal" href="../../../auto_examples/linear_model/plot_ridge_coeffs/#sphx-glr-auto-examples-linear-model-plot-ridge-coeffs-py"><span class="std std-ref">Plot Ridge coefficients as a function of the L2 regularization</span></a></span></p> </div> </div>
<div class="sphx-glr-thumbcontainer" tooltip="Shows the effect of collinearity in the coefficients of an estimator.">
<div class="figure" id="id7"> <img alt="../../_images/sphx_glr_plot_ridge_path_thumb.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_ridge_path_thumb.png"> <p class="caption"><span class="caption-text"><a class="reference internal" href="../../../auto_examples/linear_model/plot_ridge_path/#sphx-glr-auto-examples-linear-model-plot-ridge-path-py"><span class="std std-ref">Plot Ridge coefficients as a function of the regularization</span></a></span></p> </div> </div>
<div class="_attribution">
  <p class="_attribution-p">
    © 2007–2016 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html" class="_attribution-link" target="_blank">http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html</a>
  </p>
</div>

			</div>
		</div>
	</section>

	</div>
</body>
</html>
