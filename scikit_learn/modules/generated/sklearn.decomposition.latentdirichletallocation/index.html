
<!DOCTYPE HTML>

<html lang="en">

<head>
  <meta charset="utf-8">
  <title>decomposition.LatentDirichletAllocation() - Scikit-learn - W3cubDocs</title>
  
  <meta name="description" content=" Latent Dirichlet Allocation with online variational Bayes algorithm ">
  <meta name="keywords" content="sklearn, decomposition, latentdirichletallocation, -, scikit-learn, scikit_learn">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="http://docs.w3cub.com/scikit_learn/modules/generated/sklearn.decomposition.latentdirichletallocation/">
  <link href="/favicon.png" rel="icon">
  <link type="text/css" rel="stylesheet" href="/assets/application-50364fff564ce3b6327021805f3f00e2957b441cf27f576a7dd4ff63bbc47047.css">
  <script type="text/javascript" src="/assets/application-db64bfd54ceb42be11af7995804cf4902548419ceb79d509b0b7d62c22d98e6f.js"></script>
  <script src="/json/scikit_learn.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body>
	<div class="_app">
	<header class="_header">
  
  <form class="_search">
    <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
    <a class="_search-clear"></a>
    <div class="_search-tag"></div>
  </form>
  
  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/scikit_learn/" class="_nav-link" title="" style="margin-left:0;">scikit-learn</a></span>
  
  <nav class="_nav">
    <a href="/app/" class="_nav-link ">App</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		<div class="_list">
			
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<div class="_page _sphinx">
				
<h1 id="sklearn-decomposition-latentdirichletallocation">sklearn.decomposition.LatentDirichletAllocation</h1> <dl class="class"> <dt id="sklearn.decomposition.LatentDirichletAllocation">
<code>class sklearn.decomposition.LatentDirichletAllocation(n_topics=10, doc_topic_prior=None, topic_word_prior=None, learning_method=None, learning_decay=0.7, learning_offset=10.0, max_iter=10, batch_size=128, evaluate_every=-1, total_samples=1000000.0, perp_tol=0.1, mean_change_tol=0.001, max_doc_update_iter=100, n_jobs=1, verbose=0, random_state=None)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/decomposition/online_lda.py#L137" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Latent Dirichlet Allocation with online variational Bayes algorithm</p> <div class="versionadded"> <p><span class="versionmodified">New in version 0.17.</span></p> </div> <p>Read more in the <a class="reference internal" href="../../decomposition/#latentdirichletallocation"><span class="std std-ref">User Guide</span></a>.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr class="field-odd field">
<th class="field-name">Parameters:</th>
<td class="field-body">
<p class="first"><strong>n_topics</strong> : int, optional (default=10)</p>  <p>Number of topics.</p>  <p><strong>doc_topic_prior</strong> : float, optional (default=None)</p>  <p>Prior of document topic distribution <code>theta</code>. If the value is None, defaults to <code>1 / n_topics</code>. In the literature, this is called <code>alpha</code>.</p>  <p><strong>topic_word_prior</strong> : float, optional (default=None)</p>  <p>Prior of topic word distribution <code>beta</code>. If the value is None, defaults to <code>1 / n_topics</code>. In the literature, this is called <code>eta</code>.</p>  <p><strong>learning_method</strong> : ‘batch’ | ‘online’, default=’online’</p>  <p>Method used to update <code>_component</code>. Only used in <code>fit</code> method. In general, if the data size is large, the online update will be much faster than the batch update. The default learning method is going to be changed to ‘batch’ in the 0.20 release. Valid options:</p> <pre data-language="python">'batch': Batch variational Bayes method. Use all training data in
    each EM update.
    Old `components_` will be overwritten in each iteration.
'online': Online variational Bayes method. In each EM update, use
    mini-batch of training data to update the ``components_``
    variable incrementally. The learning rate is controlled by the
    ``learning_decay`` and the ``learning_offset`` parameters.
</pre>  <p><strong>learning_decay</strong> : float, optional (default=0.7)</p>  <p>It is a parameter that control learning rate in the online learning method. The value should be set between (0.5, 1.0] to guarantee asymptotic convergence. When the value is 0.0 and batch_size is <code>n_samples</code>, the update method is same as batch learning. In the literature, this is called kappa.</p>  <p><strong>learning_offset</strong> : float, optional (default=10.)</p>  <p>A (positive) parameter that downweights early iterations in online learning. It should be greater than 1.0. In the literature, this is called tau_0.</p>  <p><strong>max_iter</strong> : integer, optional (default=10)</p>  <p>The maximum number of iterations.</p>  <p><strong>total_samples</strong> : int, optional (default=1e6)</p>  <p>Total number of documents. Only used in the <code>partial_fit</code> method.</p>  <p><strong>batch_size</strong> : int, optional (default=128)</p>  <p>Number of documents to use in each EM iteration. Only used in online learning.</p>  <p><strong>evaluate_every</strong> : int optional (default=0)</p>  <p>How often to evaluate perplexity. Only used in <code>fit</code> method. set it to 0 or negative number to not evalute perplexity in training at all. Evaluating perplexity can help you check convergence in training process, but it will also increase total training time. Evaluating perplexity in every iteration might increase training time up to two-fold.</p>  <p><strong>perp_tol</strong> : float, optional (default=1e-1)</p>  <p>Perplexity tolerance in batch learning. Only used when <code>evaluate_every</code> is greater than 0.</p>  <p><strong>mean_change_tol</strong> : float, optional (default=1e-3)</p>  <p>Stopping tolerance for updating document topic distribution in E-step.</p>  <p><strong>max_doc_update_iter</strong> : int (default=100)</p>  <p>Max number of iterations for updating document topic distribution in the E-step.</p>  <p><strong>n_jobs</strong> : int, optional (default=1)</p>  <p>The number of jobs to use in the E-step. If -1, all CPUs are used. For <code>n_jobs</code> below -1, (n_cpus + 1 + n_jobs) are used.</p>  <p><strong>verbose</strong> : int, optional (default=0)</p>  <p>Verbosity level.</p>  <p><strong>random_state</strong> : int or RandomState instance or None, optional (default=None)</p>  <p>Pseudo-random number generator seed control.</p>  </td> </tr> <tr class="field-even field">
<th class="field-name">Attributes:</th>
<td class="field-body">
<p class="first"><strong>components_</strong> : array, [n_topics, n_features]</p>  <p>Topic word distribution. <code>components_[i, j]</code> represents word j in topic <code>i</code>.</p>  <p><strong>n_batch_iter_</strong> : int</p>  <p>Number of iterations of the EM step.</p>  <p><strong>n_iter_</strong> : int</p>  <p>Number of passes over the dataset.</p>  </td> </tr>  </table> <h4 class="rubric">References</h4> <dl class="docutils"> <dt>[1] “Online Learning for Latent Dirichlet Allocation”, Matthew D. Hoffman,</dt> <dd>David M. Blei, Francis Bach, 2010</dd> <dt>[2] “Stochastic Variational Inference”, Matthew D. Hoffman, David M. Blei,</dt> <dd>Chong Wang, John Paisley, 2013</dd> <dt>[3] Matthew D. Hoffman’s onlineldavb code. Link:</dt> <dd><a class="reference external" href="http://matthewdhoffman.com//code/onlineldavb.tar" target="_blank">http://matthewdhoffman.com//code/onlineldavb.tar</a></dd> </dl> <h4 class="rubric">Methods</h4> <table class="longtable docutils">   <tr class="row-odd">
<td>
<a class="reference internal" href="#sklearn.decomposition.LatentDirichletAllocation.fit" title="sklearn.decomposition.LatentDirichletAllocation.fit"><code>fit</code></a>(X[, y])</td> <td>Learn model for the data X with variational Bayes method.</td> </tr> <tr class="row-even">
<td>
<a class="reference internal" href="#sklearn.decomposition.LatentDirichletAllocation.fit_transform" title="sklearn.decomposition.LatentDirichletAllocation.fit_transform"><code>fit_transform</code></a>(X[, y])</td> <td>Fit to data, then transform it.</td> </tr> <tr class="row-odd">
<td>
<a class="reference internal" href="#sklearn.decomposition.LatentDirichletAllocation.get_params" title="sklearn.decomposition.LatentDirichletAllocation.get_params"><code>get_params</code></a>([deep])</td> <td>Get parameters for this estimator.</td> </tr> <tr class="row-even">
<td>
<a class="reference internal" href="#sklearn.decomposition.LatentDirichletAllocation.partial_fit" title="sklearn.decomposition.LatentDirichletAllocation.partial_fit"><code>partial_fit</code></a>(X[, y])</td> <td>Online VB with Mini-Batch update.</td> </tr> <tr class="row-odd">
<td>
<a class="reference internal" href="#sklearn.decomposition.LatentDirichletAllocation.perplexity" title="sklearn.decomposition.LatentDirichletAllocation.perplexity"><code>perplexity</code></a>(X[, doc_topic_distr, sub_sampling])</td> <td>Calculate approximate perplexity for data X.</td> </tr> <tr class="row-even">
<td>
<a class="reference internal" href="#sklearn.decomposition.LatentDirichletAllocation.score" title="sklearn.decomposition.LatentDirichletAllocation.score"><code>score</code></a>(X[, y])</td> <td>Calculate approximate log-likelihood as score.</td> </tr> <tr class="row-odd">
<td>
<a class="reference internal" href="#sklearn.decomposition.LatentDirichletAllocation.set_params" title="sklearn.decomposition.LatentDirichletAllocation.set_params"><code>set_params</code></a>(**params)</td> <td>Set the parameters of this estimator.</td> </tr> <tr class="row-even">
<td>
<a class="reference internal" href="#sklearn.decomposition.LatentDirichletAllocation.transform" title="sklearn.decomposition.LatentDirichletAllocation.transform"><code>transform</code></a>(X)</td> <td>Transform data X according to the fitted model.</td> </tr>  </table> <dl class="method"> <dt id="sklearn.decomposition.LatentDirichletAllocation.__init__">
<code>__init__(n_topics=10, doc_topic_prior=None, topic_word_prior=None, learning_method=None, learning_decay=0.7, learning_offset=10.0, max_iter=10, batch_size=128, evaluate_every=-1, total_samples=1000000.0, perp_tol=0.1, mean_change_tol=0.001, max_doc_update_iter=100, n_jobs=1, verbose=0, random_state=None)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/decomposition/online_lda.py#L250" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="sklearn.decomposition.LatentDirichletAllocation.fit">
<code>fit(X, y=None)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/decomposition/online_lda.py#L483" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Learn model for the data X with variational Bayes method.</p> <p>When <code>learning_method</code> is ‘online’, use mini-batch update. Otherwise, use batch update.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr class="field-odd field">
<th class="field-name">Parameters:</th>
<td class="field-body">
<p class="first"><strong>X</strong> : array-like or sparse matrix, shape=(n_samples, n_features)</p>  <p>Document word matrix.</p>  </td> </tr> <tr class="field-even field">
<th class="field-name">Returns:</th>
<td class="field-body">
<p class="first last"><strong>self</strong> :</p> </td> </tr>  </table> </dd>
</dl> <dl class="method"> <dt id="sklearn.decomposition.LatentDirichletAllocation.fit_transform">
<code>fit_transform(X, y=None, **fit_params)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/base.py#L470" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Fit to data, then transform it.</p> <p>Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr class="field-odd field">
<th class="field-name">Parameters:</th>
<td class="field-body">
<p class="first"><strong>X</strong> : numpy array of shape [n_samples, n_features]</p>  <p>Training set.</p>  <p><strong>y</strong> : numpy array of shape [n_samples]</p>  <p>Target values.</p>  </td> </tr> <tr class="field-even field">
<th class="field-name">Returns:</th>
<td class="field-body">
<p class="first"><strong>X_new</strong> : numpy array of shape [n_samples, n_features_new]</p>  <p>Transformed array.</p>  </td> </tr>  </table> </dd>
</dl> <dl class="method"> <dt id="sklearn.decomposition.LatentDirichletAllocation.get_params">
<code>get_params(deep=True)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/base.py#L220" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Get parameters for this estimator.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr class="field-odd field">
<th class="field-name">Parameters:</th>
<td class="field-body">
<p class="first"><strong>deep: boolean, optional</strong> :</p>  <p>If True, will return the parameters for this estimator and contained subobjects that are estimators.</p>  </td> </tr> <tr class="field-even field">
<th class="field-name">Returns:</th>
<td class="field-body">
<p class="first"><strong>params</strong> : mapping of string to any</p>  <p>Parameter names mapped to their values.</p>  </td> </tr>  </table> </dd>
</dl> <dl class="method"> <dt id="sklearn.decomposition.LatentDirichletAllocation.partial_fit">
<code>partial_fit(X, y=None)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/decomposition/online_lda.py#L445" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Online VB with Mini-Batch update.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr class="field-odd field">
<th class="field-name">Parameters:</th>
<td class="field-body">
<p class="first"><strong>X</strong> : array-like or sparse matrix, shape=(n_samples, n_features)</p>  <p>Document word matrix.</p>  </td> </tr> <tr class="field-even field">
<th class="field-name">Returns:</th>
<td class="field-body">
<p class="first last"><strong>self</strong> :</p> </td> </tr>  </table> </dd>
</dl> <dl class="method"> <dt id="sklearn.decomposition.LatentDirichletAllocation.perplexity">
<code>perplexity(X, doc_topic_distr=None, sub_sampling=False)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/decomposition/online_lda.py#L675" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Calculate approximate perplexity for data X.</p> <p>Perplexity is defined as exp(-1. * log-likelihood per word)</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr class="field-odd field">
<th class="field-name">Parameters:</th>
<td class="field-body">
<p class="first"><strong>X</strong> : array-like or sparse matrix, [n_samples, n_features]</p>  <p>Document word matrix.</p>  <p><strong>doc_topic_distr</strong> : None or array, shape=(n_samples, n_topics)</p>  <p>Document topic distribution. If it is None, it will be generated by applying transform on X.</p>  </td> </tr> <tr class="field-even field">
<th class="field-name">Returns:</th>
<td class="field-body">
<p class="first"><strong>score</strong> : float</p>  <p>Perplexity score.</p>  </td> </tr>  </table> </dd>
</dl> <dl class="method"> <dt id="sklearn.decomposition.LatentDirichletAllocation.score">
<code>score(X, y=None)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/decomposition/online_lda.py#L655" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Calculate approximate log-likelihood as score.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr class="field-odd field">
<th class="field-name">Parameters:</th>
<td class="field-body">
<p class="first"><strong>X</strong> : array-like or sparse matrix, shape=(n_samples, n_features)</p>  <p>Document word matrix.</p>  </td> </tr> <tr class="field-even field">
<th class="field-name">Returns:</th>
<td class="field-body">
<p class="first"><strong>score</strong> : float</p>  <p>Use approximate bound as score.</p>  </td> </tr>  </table> </dd>
</dl> <dl class="method"> <dt id="sklearn.decomposition.LatentDirichletAllocation.set_params">
<code>set_params(**params)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/base.py#L257" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Set the parameters of this estimator.</p> <p>The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <code>&lt;component&gt;__&lt;parameter&gt;</code> so that it’s possible to update each component of a nested object.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr class="field-odd field">
<th class="field-name">Returns:</th>
<td class="field-body">
<strong>self</strong> :</td> </tr>  </table> </dd>
</dl> <dl class="method"> <dt id="sklearn.decomposition.LatentDirichletAllocation.transform">
<code>transform(X)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/decomposition/online_lda.py#L546" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Transform data X according to the fitted model.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr class="field-odd field">
<th class="field-name">Parameters:</th>
<td class="field-body">
<p class="first"><strong>X</strong> : array-like or sparse matrix, shape=(n_samples, n_features)</p>  <p>Document word matrix.</p>  </td> </tr> <tr class="field-even field">
<th class="field-name">Returns:</th>
<td class="field-body">
<p class="first"><strong>doc_topic_distr</strong> : shape=(n_samples, n_topics)</p>  <p>Document topic distribution for X.</p>  </td> </tr>  </table> </dd>
</dl> </dd>
</dl>  <h2 id="examples-using-sklearn-decomposition-latentdirichletallocation">Examples using <code>sklearn.decomposition.LatentDirichletAllocation</code>
</h2> <div class="sphx-glr-thumbcontainer" tooltip="This is an example of applying Non-negative Matrix Factorization and Latent Dirichlet Allocatio...">
<div class="figure" id="id1"> <img alt="../../_images/sphx_glr_topics_extraction_with_nmf_lda_thumb.png" src="http://scikit-learn.org/stable/_images/sphx_glr_topics_extraction_with_nmf_lda_thumb.png"> <p class="caption"><span class="caption-text"><a class="reference internal" href="../../../auto_examples/applications/topics_extraction_with_nmf_lda/#sphx-glr-auto-examples-applications-topics-extraction-with-nmf-lda-py"><span class="std std-ref">Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation</span></a></span></p> </div> </div>
<div class="_attribution">
  <p class="_attribution-p">
    © 2007–2016 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html" class="_attribution-link" target="_blank">http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html</a>
  </p>
</div>

			</div>
		</div>
	</section>

	</div>
</body>
</html>
