
<!DOCTYPE HTML>

<html lang="en">

<head>
  <meta charset="utf-8">
  <title>ensemble.BaggingRegressor() - Scikit-learn - W3cubDocs</title>
  
  <meta name="description" content=" A Bagging regressor. ">
  <meta name="keywords" content="sklearn, ensemble, baggingregressor, -, scikit-learn, scikit_learn">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="http://docs.w3cub.com/scikit_learn/modules/generated/sklearn.ensemble.baggingregressor/">
  <link href="/favicon.png" rel="icon">
  <link type="text/css" rel="stylesheet" href="/assets/application-50364fff564ce3b6327021805f3f00e2957b441cf27f576a7dd4ff63bbc47047.css">
  <script type="text/javascript" src="/assets/application-db64bfd54ceb42be11af7995804cf4902548419ceb79d509b0b7d62c22d98e6f.js"></script>
  <script src="/json/scikit_learn.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body>
	<div class="_app">
	<header class="_header">
  
  <form class="_search">
    <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
    <a class="_search-clear"></a>
    <div class="_search-tag"></div>
  </form>
  
  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/scikit_learn/" class="_nav-link" title="" style="margin-left:0;">scikit-learn</a></span>
  
  <nav class="_nav">
    <a href="/app/" class="_nav-link ">App</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		<div class="_list">
			
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<div class="_page _sphinx">
				
<h1 id="sklearn-ensemble-baggingregressor">sklearn.ensemble.BaggingRegressor</h1> <dl class="class"> <dt id="sklearn.ensemble.BaggingRegressor">
<code>class sklearn.ensemble.BaggingRegressor(base_estimator=None, n_estimators=10, max_samples=1.0, max_features=1.0, bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=1, random_state=None, verbose=0)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/ensemble/bagging.py#L796" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>A Bagging regressor.</p> <p>A Bagging regressor is an ensemble meta-estimator that fits base regressors each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it.</p> <p>This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting <a class="reference internal" href="#r155" id="id1">[R155]</a>. If samples are drawn with replacement, then the method is known as Bagging <a class="reference internal" href="#r156" id="id2">[R156]</a>. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces <a class="reference internal" href="#r157" id="id3">[R157]</a>. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches <a class="reference internal" href="#r158" id="id4">[R158]</a>.</p> <p>Read more in the <a class="reference internal" href="../../ensemble/#bagging"><span class="std std-ref">User Guide</span></a>.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr class="field-odd field">
<th class="field-name">Parameters:</th>
<td class="field-body">
<p class="first"><strong>base_estimator</strong> : object or None, optional (default=None)</p>  <p>The base estimator to fit on random subsets of the dataset. If None, then the base estimator is a decision tree.</p>  <p><strong>n_estimators</strong> : int, optional (default=10)</p>  <p>The number of base estimators in the ensemble.</p>  <p><strong>max_samples</strong> : int or float, optional (default=1.0)</p>  <dl class="docutils"> <dt>The number of samples to draw from X to train each base estimator.</dt> <dd>
<ul class="first last simple"> <li>If int, then draw <code>max_samples</code> samples.</li> <li>If float, then draw <code>max_samples * X.shape[0]</code> samples.</li> </ul> </dd> </dl>  <p><strong>max_features</strong> : int or float, optional (default=1.0)</p>  <dl class="docutils"> <dt>The number of features to draw from X to train each base estimator.</dt> <dd>
<ul class="first last simple"> <li>If int, then draw <code>max_features</code> features.</li> <li>If float, then draw <code>max_features * X.shape[1]</code> features.</li> </ul> </dd> </dl>  <p><strong>bootstrap</strong> : boolean, optional (default=True)</p>  <p>Whether samples are drawn with replacement.</p>  <p><strong>bootstrap_features</strong> : boolean, optional (default=False)</p>  <p>Whether features are drawn with replacement.</p>  <p><strong>oob_score</strong> : bool</p>  <p>Whether to use out-of-bag samples to estimate the generalization error.</p>  <p><strong>warm_start</strong> : bool, optional (default=False)</p>  <p>When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new ensemble.</p>  <p><strong>n_jobs</strong> : int, optional (default=1)</p>  <p>The number of jobs to run in parallel for both <code>fit</code> and <code>predict</code>. If -1, then the number of jobs is set to the number of cores.</p>  <p><strong>random_state</strong> : int, RandomState instance or None, optional (default=None)</p>  <p>If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by <code>np.random</code>.</p>  <p><strong>verbose</strong> : int, optional (default=0)</p>  <p>Controls the verbosity of the building process.</p>  </td> </tr> <tr class="field-even field">
<th class="field-name">Attributes:</th>
<td class="field-body">
<p class="first"><strong>estimators_</strong> : list of estimators</p>  <p>The collection of fitted sub-estimators.</p>  <p><strong>estimators_samples_</strong> : list of arrays</p>  <p>The subset of drawn samples (i.e., the in-bag samples) for each base estimator. Each subset is defined by a boolean mask.</p>  <p><strong>estimators_features_</strong> : list of arrays</p>  <p>The subset of drawn features for each base estimator.</p>  <p><strong>oob_score_</strong> : float</p>  <p>Score of the training dataset obtained using an out-of-bag estimate.</p>  <p><strong>oob_prediction_</strong> : array of shape = [n_samples]</p>  <p>Prediction computed with out-of-bag estimate on the training set. If n_estimators is small it might be possible that a data point was never left out during the bootstrap. In this case, <code>oob_prediction_</code> might contain NaN.</p>  </td> </tr>  </table> <h4 class="rubric">References</h4> <table class="docutils citation" frame="void" id="r155" rules="none">   <tr>
<td class="label">[R155]</td>
<td>
<em>(<a class="fn-backref" href="#id1">1</a>, <a class="fn-backref" href="#id5">2</a>)</em> L. Breiman, “Pasting small votes for classification in large databases and on-line”, Machine Learning, 36(1), 85-103, 1999.</td>
</tr>  </table> <table class="docutils citation" frame="void" id="r156" rules="none">   <tr>
<td class="label">[R156]</td>
<td>
<em>(<a class="fn-backref" href="#id2">1</a>, <a class="fn-backref" href="#id6">2</a>)</em> L. Breiman, “Bagging predictors”, Machine Learning, 24(2), 123-140, 1996.</td>
</tr>  </table> <table class="docutils citation" frame="void" id="r157" rules="none">   <tr>
<td class="label">[R157]</td>
<td>
<em>(<a class="fn-backref" href="#id3">1</a>, <a class="fn-backref" href="#id7">2</a>)</em> T. Ho, “The random subspace method for constructing decision forests”, Pattern Analysis and Machine Intelligence, 20(8), 832-844, 1998.</td>
</tr>  </table> <table class="docutils citation" frame="void" id="r158" rules="none">   <tr>
<td class="label">[R158]</td>
<td>
<em>(<a class="fn-backref" href="#id4">1</a>, <a class="fn-backref" href="#id8">2</a>)</em> G. Louppe and P. Geurts, “Ensembles on Random Patches”, Machine Learning and Knowledge Discovery in Databases, 346-361, 2012.</td>
</tr>  </table> <h4 class="rubric">Methods</h4> <table class="longtable docutils">   <tr class="row-odd">
<td>
<a class="reference internal" href="#sklearn.ensemble.BaggingRegressor.fit" title="sklearn.ensemble.BaggingRegressor.fit"><code>fit</code></a>(X, y[, sample_weight])</td> <td>Build a Bagging ensemble of estimators from the training set (X, y).</td> </tr> <tr class="row-even">
<td>
<a class="reference internal" href="#sklearn.ensemble.BaggingRegressor.get_params" title="sklearn.ensemble.BaggingRegressor.get_params"><code>get_params</code></a>([deep])</td> <td>Get parameters for this estimator.</td> </tr> <tr class="row-odd">
<td>
<a class="reference internal" href="#sklearn.ensemble.BaggingRegressor.predict" title="sklearn.ensemble.BaggingRegressor.predict"><code>predict</code></a>(X)</td> <td>Predict regression target for X.</td> </tr> <tr class="row-even">
<td>
<a class="reference internal" href="#sklearn.ensemble.BaggingRegressor.score" title="sklearn.ensemble.BaggingRegressor.score"><code>score</code></a>(X, y[, sample_weight])</td> <td>Returns the coefficient of determination R^2 of the prediction.</td> </tr> <tr class="row-odd">
<td>
<a class="reference internal" href="#sklearn.ensemble.BaggingRegressor.set_params" title="sklearn.ensemble.BaggingRegressor.set_params"><code>set_params</code></a>(**params)</td> <td>Set the parameters of this estimator.</td> </tr>  </table> <dl class="method"> <dt id="sklearn.ensemble.BaggingRegressor.__init__">
<code>__init__(base_estimator=None, n_estimators=10, max_samples=1.0, max_features=1.0, bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=1, random_state=None, verbose=0)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/ensemble/bagging.py#L903" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="attribute"> <dt id="sklearn.ensemble.BaggingRegressor.estimators_samples_">
<code>estimators_samples_</code> </dt> <dd>
<p>The subset of drawn samples for each base estimator.</p> <p>Returns a dynamically generated list of boolean masks identifying the samples used for for fitting each member of the ensemble, i.e., the in-bag samples.</p> <p>Note: the list is re-created at each call to the property in order to reduce the object memory footprint by not storing the sampling data. Thus fetching the property may be slower than expected.</p> </dd>
</dl> <dl class="method"> <dt id="sklearn.ensemble.BaggingRegressor.fit">
<code>fit(X, y, sample_weight=None)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/ensemble/bagging.py#L224" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<dl class="docutils"> <dt>Build a Bagging ensemble of estimators from the training</dt> <dd>set (X, y).</dd> </dl> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr class="field-odd field">
<th class="field-name">Parameters:</th>
<td class="field-body">
<p class="first"><strong>X</strong> : {array-like, sparse matrix} of shape = [n_samples, n_features]</p>  <p>The training input samples. Sparse matrices are accepted only if they are supported by the base estimator.</p>  <p><strong>y</strong> : array-like, shape = [n_samples]</p>  <p>The target values (class labels in classification, real numbers in regression).</p>  <p><strong>sample_weight</strong> : array-like, shape = [n_samples] or None</p>  <p>Sample weights. If None, then samples are equally weighted. Note that this is supported only if the base estimator supports sample weighting.</p>  </td> </tr> <tr class="field-even field">
<th class="field-name">Returns:</th>
<td class="field-body">
<p class="first"><strong>self</strong> : object</p>  <p>Returns self.</p>  </td> </tr>  </table> </dd>
</dl> <dl class="method"> <dt id="sklearn.ensemble.BaggingRegressor.get_params">
<code>get_params(deep=True)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/base.py#L220" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Get parameters for this estimator.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr class="field-odd field">
<th class="field-name">Parameters:</th>
<td class="field-body">
<p class="first"><strong>deep: boolean, optional</strong> :</p>  <p>If True, will return the parameters for this estimator and contained subobjects that are estimators.</p>  </td> </tr> <tr class="field-even field">
<th class="field-name">Returns:</th>
<td class="field-body">
<p class="first"><strong>params</strong> : mapping of string to any</p>  <p>Parameter names mapped to their values.</p>  </td> </tr>  </table> </dd>
</dl> <dl class="method"> <dt id="sklearn.ensemble.BaggingRegressor.predict">
<code>predict(X)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/ensemble/bagging.py#L928" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Predict regression target for X.</p> <p>The predicted regression target of an input sample is computed as the mean predicted regression targets of the estimators in the ensemble.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr class="field-odd field">
<th class="field-name">Parameters:</th>
<td class="field-body">
<p class="first"><strong>X</strong> : {array-like, sparse matrix} of shape = [n_samples, n_features]</p>  <p>The training input samples. Sparse matrices are accepted only if they are supported by the base estimator.</p>  </td> </tr> <tr class="field-even field">
<th class="field-name">Returns:</th>
<td class="field-body">
<p class="first"><strong>y</strong> : array of shape = [n_samples]</p>  <p>The predicted values.</p>  </td> </tr>  </table> </dd>
</dl> <dl class="method"> <dt id="sklearn.ensemble.BaggingRegressor.score">
<code>score(X, y, sample_weight=None)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/base.py#L357" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns the coefficient of determination R^2 of the prediction.</p> <p>The coefficient R^2 is defined as (1 - u/v), where u is the regression sum of squares ((y_true - y_pred) ** 2).sum() and v is the residual sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr class="field-odd field">
<th class="field-name">Parameters:</th>
<td class="field-body">
<p class="first"><strong>X</strong> : array-like, shape = (n_samples, n_features)</p>  <p>Test samples.</p>  <p><strong>y</strong> : array-like, shape = (n_samples) or (n_samples, n_outputs)</p>  <p>True values for X.</p>  <p><strong>sample_weight</strong> : array-like, shape = [n_samples], optional</p>  <p>Sample weights.</p>  </td> </tr> <tr class="field-even field">
<th class="field-name">Returns:</th>
<td class="field-body">
<p class="first"><strong>score</strong> : float</p>  <p>R^2 of self.predict(X) wrt. y.</p>  </td> </tr>  </table> </dd>
</dl> <dl class="method"> <dt id="sklearn.ensemble.BaggingRegressor.set_params">
<code>set_params(**params)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/base.py#L257" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Set the parameters of this estimator.</p> <p>The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <code>&lt;component&gt;__&lt;parameter&gt;</code> so that it’s possible to update each component of a nested object.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr class="field-odd field">
<th class="field-name">Returns:</th>
<td class="field-body">
<strong>self</strong> :</td> </tr>  </table> </dd>
</dl> </dd>
</dl>  <h2 id="examples-using-sklearn-ensemble-baggingregressor">Examples using <code>sklearn.ensemble.BaggingRegressor</code>
</h2> <div class="sphx-glr-thumbcontainer" tooltip="This example illustrates and compares the bias-variance decomposition of the expected mean squa...">
<div class="figure" id="id9"> <img alt="../../_images/sphx_glr_plot_bias_variance_thumb.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_bias_variance_thumb.png"> <p class="caption"><span class="caption-text"><a class="reference internal" href="../../../auto_examples/ensemble/plot_bias_variance/#sphx-glr-auto-examples-ensemble-plot-bias-variance-py"><span class="std std-ref">Single estimator versus bagging: bias-variance decomposition</span></a></span></p> </div> </div>
<div class="_attribution">
  <p class="_attribution-p">
    © 2007–2016 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html" class="_attribution-link" target="_blank">http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html</a>
  </p>
</div>

			</div>
		</div>
	</section>

	</div>
</body>
</html>
