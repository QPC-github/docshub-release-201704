
<!DOCTYPE HTML>

<html lang="en">

<head>
  <meta charset="utf-8">
  <title>decomposition.PCA() - Scikit-learn - W3cubDocs</title>
  
  <meta name="description" content=" Principal component analysis (PCA) ">
  <meta name="keywords" content="sklearn, decomposition, pca, -, scikit-learn, scikit_learn">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="http://docs.w3cub.com/scikit_learn/modules/generated/sklearn.decomposition.pca/">
  <link href="/favicon.png" rel="icon">
  <link type="text/css" rel="stylesheet" href="/assets/application-50364fff564ce3b6327021805f3f00e2957b441cf27f576a7dd4ff63bbc47047.css">
  <script type="text/javascript" src="/assets/application-db64bfd54ceb42be11af7995804cf4902548419ceb79d509b0b7d62c22d98e6f.js"></script>
  <script src="/json/scikit_learn.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body>
	<div class="_app">
	<header class="_header">
  
  <form class="_search">
    <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
    <a class="_search-clear"></a>
    <div class="_search-tag"></div>
  </form>
  
  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/scikit_learn/" class="_nav-link" title="" style="margin-left:0;">scikit-learn</a></span>
  
  <nav class="_nav">
    <a href="/app/" class="_nav-link ">App</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		<div class="_list">
			
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<div class="_page _sphinx">
				
<h1 id="sklearn-decomposition-pca">sklearn.decomposition.PCA</h1> <dl class="class"> <dt id="sklearn.decomposition.PCA">
<code>class sklearn.decomposition.PCA(n_components=None, copy=True, whiten=False, svd_solver='auto', tol=0.0, iterated_power='auto', random_state=None)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/decomposition/pca.py#L105" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Principal component analysis (PCA)</p> <p>Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space.</p> <p>It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.</p> <p>It can also use the scipy.sparse.linalg ARPACK implementation of the truncated SVD.</p> <p>Read more in the <a class="reference internal" href="../../decomposition/#pca"><span class="std std-ref">User Guide</span></a>.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr class="field-odd field">
<th class="field-name">Parameters:</th>
<td class="field-body">
<p class="first"><strong>n_components</strong> : int, float, None or string</p>  <p>Number of components to keep. if n_components is not set all components are kept:</p> <pre data-language="python">n_components == min(n_samples, n_features)
</pre> <p>if n_components == ‘mle’ and svd_solver == ‘full’, Minka’s MLE is used to guess the dimension if <code>0 &lt; n_components &lt; 1</code> and svd_solver == ‘full’, select the number of components such that the amount of variance that needs to be explained is greater than the percentage specified by n_components n_components cannot be equal to n_features for svd_solver == ‘arpack’.</p>  <p><strong>copy</strong> : bool (default True)</p>  <p>If False, data passed to fit are overwritten and running fit(X).transform(X) will not yield the expected results, use fit_transform(X) instead.</p>  <p><strong>whiten</strong> : bool, optional (default False)</p>  <p>When True (False by default) the <code>components_</code> vectors are multiplied by the square root of n_samples and then divided by the singular values to ensure uncorrelated outputs with unit component-wise variances.</p> <p>Whitening will remove some information from the transformed signal (the relative variance scales of the components) but can sometime improve the predictive accuracy of the downstream estimators by making their data respect some hard-wired assumptions.</p>  <p><strong>svd_solver</strong> : string {‘auto’, ‘full’, ‘arpack’, ‘randomized’}</p>  <dl class="docutils"> <dt>auto :</dt> <dd>
<p class="first last">the solver is selected by a default policy based on <code>X.shape</code> and <code>n_components</code>: if the input data is larger than 500x500 and the number of components to extract is lower than 80% of the smallest dimension of the data, then then more efficient ‘randomized’ method is enabled. Otherwise the exact full SVD is computed and optionally truncated afterwards.</p> </dd> <dt>full :</dt> <dd>
<p class="first last">run exact full SVD calling the standard LAPACK solver via <code>scipy.linalg.svd</code> and select the components by postprocessing</p> </dd> <dt>arpack :</dt> <dd>
<p class="first last">run SVD truncated to n_components calling ARPACK solver via <code>scipy.sparse.linalg.svds</code>. It requires strictly 0 &lt; n_components &lt; X.shape[1]</p> </dd> <dt>randomized :</dt> <dd>
<p class="first last">run randomized SVD by the method of Halko et al.</p> </dd> </dl> <div class="versionadded"> <p><span class="versionmodified">New in version 0.18.0.</span></p> </div>  <p><strong>tol</strong> : float &gt;= 0, optional (default .0)</p>  <p>Tolerance for singular values computed by svd_solver == ‘arpack’.</p> <div class="versionadded"> <p><span class="versionmodified">New in version 0.18.0.</span></p> </div>  <p><strong>iterated_power</strong> : int &gt;= 0, or ‘auto’, (default ‘auto’)</p>  <p>Number of iterations for the power method computed by svd_solver == ‘randomized’.</p> <div class="versionadded"> <p><span class="versionmodified">New in version 0.18.0.</span></p> </div>  <p><strong>random_state</strong> : int or RandomState instance or None (default None)</p>  <p>Pseudo Random Number generator seed control. If None, use the numpy.random singleton. Used by svd_solver == ‘arpack’ or ‘randomized’.</p> <div class="versionadded"> <p><span class="versionmodified">New in version 0.18.0.</span></p> </div>  </td> </tr> <tr class="field-even field">
<th class="field-name">Attributes:</th>
<td class="field-body">
<p class="first"><strong>components_</strong> : array, [n_components, n_features]</p>  <p>Principal axes in feature space, representing the directions of maximum variance in the data. The components are sorted by <code>explained_variance_</code>.</p>  <p><strong>explained_variance_</strong> : array, [n_components]</p>  <p>The amount of variance explained by each of the selected components.</p> <div class="versionadded"> <p><span class="versionmodified">New in version 0.18.</span></p> </div>  <p><strong>explained_variance_ratio_</strong> : array, [n_components]</p>  <p>Percentage of variance explained by each of the selected components.</p> <p>If <code>n_components</code> is not set then all components are stored and the sum of explained variances is equal to 1.0.</p>  <p><strong>mean_</strong> : array, [n_features]</p>  <p>Per-feature empirical mean, estimated from the training set.</p> <p>Equal to <code>X.mean(axis=1)</code>.</p>  <p><strong>n_components_</strong> : int</p>  <p>The estimated number of components. When n_components is set to ‘mle’ or a number between 0 and 1 (with svd_solver == ‘full’) this number is estimated from input data. Otherwise it equals the parameter n_components, or n_features if n_components is None.</p>  <p><strong>noise_variance_</strong> : float</p>  <p>The estimated noise covariance following the Probabilistic PCA model from Tipping and Bishop 1999. See “Pattern Recognition and Machine Learning” by C. Bishop, 12.2.1 p. 574 or <a class="reference external" href="http://www.miketipping.com/papers/met-mppca.pdf" target="_blank">http://www.miketipping.com/papers/met-mppca.pdf</a>. It is required to computed the estimated data covariance and score samples.</p>  </td> </tr>  </table> <div class="admonition seealso"> <p class="first admonition-title">See also</p> <p class="last"><a class="reference internal" href="../sklearn.decomposition.kernelpca/#sklearn.decomposition.KernelPCA" title="sklearn.decomposition.KernelPCA"><code>KernelPCA</code></a>, <a class="reference internal" href="../sklearn.decomposition.sparsepca/#sklearn.decomposition.SparsePCA" title="sklearn.decomposition.SparsePCA"><code>SparsePCA</code></a>, <a class="reference internal" href="../sklearn.decomposition.truncatedsvd/#sklearn.decomposition.TruncatedSVD" title="sklearn.decomposition.TruncatedSVD"><code>TruncatedSVD</code></a>, <a class="reference internal" href="../sklearn.decomposition.incrementalpca/#sklearn.decomposition.IncrementalPCA" title="sklearn.decomposition.IncrementalPCA"><code>IncrementalPCA</code></a></p> </div> <h4 class="rubric">References</h4> <p>For n_components == ‘mle’, this class uses the method of <code>Thomas P. Minka: Automatic Choice of Dimensionality for PCA. NIPS 2000: 598-604</code></p> <p>Implements the probabilistic PCA model from: M. Tipping and C. Bishop, Probabilistic Principal Component Analysis, Journal of the Royal Statistical Society, Series B, 61, Part 3, pp. 611-622 via the score and score_samples methods. See <a class="reference external" href="http://www.miketipping.com/papers/met-mppca.pdf" target="_blank">http://www.miketipping.com/papers/met-mppca.pdf</a></p> <p>For svd_solver == ‘arpack’, refer to <code>scipy.sparse.linalg.svds</code>.</p> <p>For svd_solver == ‘randomized’, see: <code>Finding structure with randomness: Stochastic algorithms for constructing approximate matrix decompositions Halko, et al., 2009 (arXiv:909)</code> <code>A randomized algorithm for the decomposition of matrices Per-Gunnar Martinsson, Vladimir Rokhlin and Mark Tygert</code></p> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from sklearn.decomposition import PCA
&gt;&gt;&gt; X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
&gt;&gt;&gt; pca = PCA(n_components=2)
&gt;&gt;&gt; pca.fit(X)
PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,
  svd_solver='auto', tol=0.0, whiten=False)
&gt;&gt;&gt; print(pca.explained_variance_ratio_) 
[ 0.99244...  0.00755...]
</pre> <pre data-language="python">&gt;&gt;&gt; pca = PCA(n_components=2, svd_solver='full')
&gt;&gt;&gt; pca.fit(X)                 
PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,
  svd_solver='full', tol=0.0, whiten=False)
&gt;&gt;&gt; print(pca.explained_variance_ratio_) 
[ 0.99244...  0.00755...]
</pre> <pre data-language="python">&gt;&gt;&gt; pca = PCA(n_components=1, svd_solver='arpack')
&gt;&gt;&gt; pca.fit(X)
PCA(copy=True, iterated_power='auto', n_components=1, random_state=None,
  svd_solver='arpack', tol=0.0, whiten=False)
&gt;&gt;&gt; print(pca.explained_variance_ratio_) 
[ 0.99244...]
</pre> <h4 class="rubric">Methods</h4> <table class="longtable docutils">   <tr class="row-odd">
<td>
<a class="reference internal" href="#sklearn.decomposition.PCA.fit" title="sklearn.decomposition.PCA.fit"><code>fit</code></a>(X[, y])</td> <td>Fit the model with X.</td> </tr> <tr class="row-even">
<td>
<a class="reference internal" href="#sklearn.decomposition.PCA.fit_transform" title="sklearn.decomposition.PCA.fit_transform"><code>fit_transform</code></a>(X[, y])</td> <td>Fit the model with X and apply the dimensionality reduction on X.</td> </tr> <tr class="row-odd">
<td>
<a class="reference internal" href="#sklearn.decomposition.PCA.get_covariance" title="sklearn.decomposition.PCA.get_covariance"><code>get_covariance</code></a>()</td> <td>Compute data covariance with the generative model.</td> </tr> <tr class="row-even">
<td>
<a class="reference internal" href="#sklearn.decomposition.PCA.get_params" title="sklearn.decomposition.PCA.get_params"><code>get_params</code></a>([deep])</td> <td>Get parameters for this estimator.</td> </tr> <tr class="row-odd">
<td>
<a class="reference internal" href="#sklearn.decomposition.PCA.get_precision" title="sklearn.decomposition.PCA.get_precision"><code>get_precision</code></a>()</td> <td>Compute data precision matrix with the generative model.</td> </tr> <tr class="row-even">
<td>
<a class="reference internal" href="#sklearn.decomposition.PCA.inverse_transform" title="sklearn.decomposition.PCA.inverse_transform"><code>inverse_transform</code></a>(X[, y])</td> <td>Transform data back to its original space.</td> </tr> <tr class="row-odd">
<td>
<a class="reference internal" href="#sklearn.decomposition.PCA.score" title="sklearn.decomposition.PCA.score"><code>score</code></a>(X[, y])</td> <td>Return the average log-likelihood of all samples.</td> </tr> <tr class="row-even">
<td>
<a class="reference internal" href="#sklearn.decomposition.PCA.score_samples" title="sklearn.decomposition.PCA.score_samples"><code>score_samples</code></a>(X)</td> <td>Return the log-likelihood of each sample.</td> </tr> <tr class="row-odd">
<td>
<a class="reference internal" href="#sklearn.decomposition.PCA.set_params" title="sklearn.decomposition.PCA.set_params"><code>set_params</code></a>(**params)</td> <td>Set the parameters of this estimator.</td> </tr> <tr class="row-even">
<td>
<a class="reference internal" href="#sklearn.decomposition.PCA.transform" title="sklearn.decomposition.PCA.transform"><code>transform</code></a>(X[, y])</td> <td>Apply dimensionality reduction to X.</td> </tr>  </table> <dl class="method"> <dt id="sklearn.decomposition.PCA.__init__">
<code>__init__(n_components=None, copy=True, whiten=False, svd_solver='auto', tol=0.0, iterated_power='auto', random_state=None)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/decomposition/pca.py#L278" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="sklearn.decomposition.PCA.fit">
<code>fit(X, y=None)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/decomposition/pca.py#L289" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Fit the model with X.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr class="field-odd field">
<th class="field-name">Parameters:</th>
<td class="field-body">
<p class="first"><strong>X: array-like, shape (n_samples, n_features)</strong> :</p>  <p>Training data, where n_samples in the number of samples and n_features is the number of features.</p>  </td> </tr> <tr class="field-even field">
<th class="field-name">Returns:</th>
<td class="field-body">
<p class="first"><strong>self</strong> : object</p>  <p>Returns the instance itself.</p>  </td> </tr>  </table> </dd>
</dl> <dl class="method"> <dt id="sklearn.decomposition.PCA.fit_transform">
<code>fit_transform(X, y=None)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/decomposition/pca.py#L306" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Fit the model with X and apply the dimensionality reduction on X.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr class="field-odd field">
<th class="field-name">Parameters:</th>
<td class="field-body">
<p class="first"><strong>X</strong> : array-like, shape (n_samples, n_features)</p>  <p>Training data, where n_samples is the number of samples and n_features is the number of features.</p>  </td> </tr> <tr class="field-even field">
<th class="field-name">Returns:</th>
<td class="field-body">
<p class="first last"><strong>X_new</strong> : array-like, shape (n_samples, n_components)</p> </td> </tr>  </table> </dd>
</dl> <dl class="method"> <dt id="sklearn.decomposition.PCA.get_covariance">
<code>get_covariance()</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/decomposition/base.py#L28" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Compute data covariance with the generative model.</p> <p><code>cov = components_.T * S**2 * components_ + sigma2 * eye(n_features)</code> where S**2 contains the explained variances, and sigma2 contains the noise variances.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr class="field-odd field">
<th class="field-name">Returns:</th>
<td class="field-body">
<p class="first"><strong>cov</strong> : array, shape=(n_features, n_features)</p>  <p>Estimated covariance of data.</p>  </td> </tr>  </table> </dd>
</dl> <dl class="method"> <dt id="sklearn.decomposition.PCA.get_params">
<code>get_params(deep=True)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/base.py#L220" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Get parameters for this estimator.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr class="field-odd field">
<th class="field-name">Parameters:</th>
<td class="field-body">
<p class="first"><strong>deep: boolean, optional</strong> :</p>  <p>If True, will return the parameters for this estimator and contained subobjects that are estimators.</p>  </td> </tr> <tr class="field-even field">
<th class="field-name">Returns:</th>
<td class="field-body">
<p class="first"><strong>params</strong> : mapping of string to any</p>  <p>Parameter names mapped to their values.</p>  </td> </tr>  </table> </dd>
</dl> <dl class="method"> <dt id="sklearn.decomposition.PCA.get_precision">
<code>get_precision()</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/decomposition/base.py#L49" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Compute data precision matrix with the generative model.</p> <p>Equals the inverse of the covariance but computed with the matrix inversion lemma for efficiency.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr class="field-odd field">
<th class="field-name">Returns:</th>
<td class="field-body">
<p class="first"><strong>precision</strong> : array, shape=(n_features, n_features)</p>  <p>Estimated precision of data.</p>  </td> </tr>  </table> </dd>
</dl> <dl class="method"> <dt id="sklearn.decomposition.PCA.inverse_transform">
<code>inverse_transform(X, y=None)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/decomposition/base.py#L138" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Transform data back to its original space.</p> <p>In other words, return an input X_original whose transform would be X.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr class="field-odd field">
<th class="field-name">Parameters:</th>
<td class="field-body">
<p class="first"><strong>X</strong> : array-like, shape (n_samples, n_components)</p>  <p>New data, where n_samples is the number of samples and n_components is the number of components.</p>  </td> </tr> <tr class="field-even field">
<th class="field-name">Returns:</th>
<td class="field-body">
<p class="first last"><strong>X_original array-like, shape (n_samples, n_features)</strong> :</p> </td> </tr>  </table> <h4 class="rubric">Notes</h4> <p>If whitening is enabled, inverse_transform will compute the exact inverse operation, which includes reversing whitening.</p> </dd>
</dl> <dl class="method"> <dt id="sklearn.decomposition.PCA.score">
<code>score(X, y=None)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/decomposition/pca.py#L503" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Return the average log-likelihood of all samples.</p> <p>See. “Pattern Recognition and Machine Learning” by C. Bishop, 12.2.1 p. 574 or <a class="reference external" href="http://www.miketipping.com/papers/met-mppca.pdf" target="_blank">http://www.miketipping.com/papers/met-mppca.pdf</a></p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr class="field-odd field">
<th class="field-name">Parameters:</th>
<td class="field-body">
<p class="first"><strong>X: array, shape(n_samples, n_features)</strong> :</p>  <p>The data.</p>  </td> </tr> <tr class="field-even field">
<th class="field-name">Returns:</th>
<td class="field-body">
<p class="first"><strong>ll: float</strong> :</p>  <p>Average log-likelihood of the samples under the current model</p>  </td> </tr>  </table> </dd>
</dl> <dl class="method"> <dt id="sklearn.decomposition.PCA.score_samples">
<code>score_samples(X)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/decomposition/pca.py#L474" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Return the log-likelihood of each sample.</p> <p>See. “Pattern Recognition and Machine Learning” by C. Bishop, 12.2.1 p. 574 or <a class="reference external" href="http://www.miketipping.com/papers/met-mppca.pdf" target="_blank">http://www.miketipping.com/papers/met-mppca.pdf</a></p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr class="field-odd field">
<th class="field-name">Parameters:</th>
<td class="field-body">
<p class="first"><strong>X: array, shape(n_samples, n_features)</strong> :</p>  <p>The data.</p>  </td> </tr> <tr class="field-even field">
<th class="field-name">Returns:</th>
<td class="field-body">
<p class="first"><strong>ll: array, shape (n_samples,)</strong> :</p>  <p>Log-likelihood of each sample under the current model</p>  </td> </tr>  </table> </dd>
</dl> <dl class="method"> <dt id="sklearn.decomposition.PCA.set_params">
<code>set_params(**params)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/base.py#L257" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Set the parameters of this estimator.</p> <p>The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <code>&lt;component&gt;__&lt;parameter&gt;</code> so that it’s possible to update each component of a nested object.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr class="field-odd field">
<th class="field-name">Returns:</th>
<td class="field-body">
<strong>self</strong> :</td> </tr>  </table> </dd>
</dl> <dl class="method"> <dt id="sklearn.decomposition.PCA.transform">
<code>transform(X, y=None)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/decomposition/base.py#L101" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Apply dimensionality reduction to X.</p> <p>X is projected on the first principal components previously extracted from a training set.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr class="field-odd field">
<th class="field-name">Parameters:</th>
<td class="field-body">
<p class="first"><strong>X</strong> : array-like, shape (n_samples, n_features)</p>  <p>New data, where n_samples is the number of samples and n_features is the number of features.</p>  </td> </tr> <tr class="field-even field">
<th class="field-name">Returns:</th>
<td class="field-body">
<p class="first last"><strong>X_new</strong> : array-like, shape (n_samples, n_components)</p> </td> </tr>  </table> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from sklearn.decomposition import IncrementalPCA
&gt;&gt;&gt; X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
&gt;&gt;&gt; ipca = IncrementalPCA(n_components=2, batch_size=3)
&gt;&gt;&gt; ipca.fit(X)
IncrementalPCA(batch_size=3, copy=True, n_components=2, whiten=False)
&gt;&gt;&gt; ipca.transform(X) 
</pre> </dd>
</dl> </dd>
</dl>  <h2 id="examples-using-sklearn-decomposition-pca">Examples using <code>sklearn.decomposition.PCA</code>
</h2> <div class="sphx-glr-thumbcontainer" tooltip="In many real-world examples, there are many ways to extract features from a dataset. Often it i...">
<div class="figure" id="id1"> <img alt="../../_images/sphx_glr_feature_stacker_thumb.png" src="http://scikit-learn.org/stable/_images/sphx_glr_feature_stacker_thumb.png"> <p class="caption"><span class="caption-text"><a class="reference internal" href="../../../auto_examples/feature_stacker/#sphx-glr-auto-examples-feature-stacker-py"><span class="std std-ref">Concatenating multiple feature extraction methods</span></a></span></p> </div> </div>
<div class="sphx-glr-thumbcontainer" tooltip="This example constructs a pipeline that does dimensionality reduction followed by prediction wi...">
<div class="figure" id="id2"> <img alt="../../_images/sphx_glr_plot_compare_reduction_thumb.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_compare_reduction_thumb.png"> <p class="caption"><span class="caption-text"><a class="reference internal" href="../../../auto_examples/plot_compare_reduction/#sphx-glr-auto-examples-plot-compare-reduction-py"><span class="std std-ref">Selecting dimensionality reduction with Pipeline and GridSearchCV</span></a></span></p> </div> </div>
<div class="sphx-glr-thumbcontainer" tooltip="The PCA does an unsupervised dimensionality reduction, while the logistic regression does the p...">
<div class="figure" id="id3"> <img alt="../../_images/sphx_glr_plot_digits_pipe_thumb.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_digits_pipe_thumb.png"> <p class="caption"><span class="caption-text"><a class="reference internal" href="../../../auto_examples/plot_digits_pipe/#sphx-glr-auto-examples-plot-digits-pipe-py"><span class="std std-ref">Pipelining: chaining a PCA and a logistic regression</span></a></span></p> </div> </div>
<div class="sphx-glr-thumbcontainer" tooltip="An example illustrating the approximation of the feature map of an RBF kernel.">
<div class="figure" id="id4"> <img alt="../../_images/sphx_glr_plot_kernel_approximation_thumb.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_kernel_approximation_thumb.png"> <p class="caption"><span class="caption-text"><a class="reference internal" href="../../../auto_examples/plot_kernel_approximation/#sphx-glr-auto-examples-plot-kernel-approximation-py"><span class="std std-ref">Explicit feature map approximation for RBF kernels</span></a></span></p> </div> </div>
<div class="sphx-glr-thumbcontainer" tooltip="This example simulates a multi-label document classification problem. The dataset is generated ...">
<div class="figure" id="id5"> <img alt="../../_images/sphx_glr_plot_multilabel_thumb.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_multilabel_thumb.png"> <p class="caption"><span class="caption-text"><a class="reference internal" href="../../../auto_examples/plot_multilabel/#sphx-glr-auto-examples-plot-multilabel-py"><span class="std std-ref">Multilabel classification</span></a></span></p> </div> </div>
<div class="sphx-glr-thumbcontainer" tooltip="The dataset used in this example is a preprocessed excerpt of the " labeled faces in the wild ...>
<div class="figure" id="id6"> <img alt="../../_images/sphx_glr_face_recognition_thumb.png" src="http://scikit-learn.org/stable/_images/sphx_glr_face_recognition_thumb.png"> <p class="caption"><span class="caption-text"><a class="reference internal" href="../../../auto_examples/applications/face_recognition/#sphx-glr-auto-examples-applications-face-recognition-py"><span class="std std-ref">Faces recognition example using eigenfaces and SVMs</span></a></span></p> </div> </div>
<div class="sphx-glr-thumbcontainer" tooltip="In this example we compare the various initialization strategies for K-means in terms of runtim...">
<div class="figure" id="id7"> <img alt="../../_images/sphx_glr_plot_kmeans_digits_thumb.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_kmeans_digits_thumb.png"> <p class="caption"><span class="caption-text"><a class="reference internal" href="../../../auto_examples/cluster/plot_kmeans_digits/#sphx-glr-auto-examples-cluster-plot-kmeans-digits-py"><span class="std std-ref">A demo of K-Means clustering on the handwritten digits data</span></a></span></p> </div> </div>
<div class="sphx-glr-thumbcontainer" tooltip="The rows being the samples and the columns being: Sepal Length, Sepal Width, Petal Length     and P...">
<div class="figure" id="id8"> <img alt="../../_images/sphx_glr_plot_iris_dataset_thumb.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_iris_dataset_thumb.png"> <p class="caption"><span class="caption-text"><a class="reference internal" href="../../../auto_examples/datasets/plot_iris_dataset/#sphx-glr-auto-examples-datasets-plot-iris-dataset-py"><span class="std std-ref">The Iris Dataset</span></a></span></p> </div> </div>
<div class="sphx-glr-thumbcontainer" tooltip="This example applies to :ref:`olivetti_faces` different unsupervised matrix decomposition (dime...">
<div class="figure" id="id9"> <img alt="../../_images/sphx_glr_plot_faces_decomposition_thumb.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_faces_decomposition_thumb.png"> <p class="caption"><span class="caption-text"><a class="reference internal" href="../../../auto_examples/decomposition/plot_faces_decomposition/#sphx-glr-auto-examples-decomposition-plot-faces-decomposition-py"><span class="std std-ref">Faces dataset decompositions</span></a></span></p> </div> </div>
<div class="sphx-glr-thumbcontainer" tooltip="An example of estimating sources from noisy data.">
<div class="figure" id="id10"> <img alt="../../_images/sphx_glr_plot_ica_blind_source_separation_thumb.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_ica_blind_source_separation_thumb.png"> <p class="caption"><span class="caption-text"><a class="reference internal" href="../../../auto_examples/decomposition/plot_ica_blind_source_separation/#sphx-glr-auto-examples-decomposition-plot-ica-blind-source-separation-py"><span class="std std-ref">Blind source separation using FastICA</span></a></span></p> </div> </div>
<div class="sphx-glr-thumbcontainer" tooltip="This example illustrates visually in the feature space a comparison by results using two differ...">
<div class="figure" id="id11"> <img alt="../../_images/sphx_glr_plot_ica_vs_pca_thumb.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_ica_vs_pca_thumb.png"> <p class="caption"><span class="caption-text"><a class="reference internal" href="../../../auto_examples/decomposition/plot_ica_vs_pca/#sphx-glr-auto-examples-decomposition-plot-ica-vs-pca-py"><span class="std std-ref">FastICA on 2D point clouds</span></a></span></p> </div> </div>
<div class="sphx-glr-thumbcontainer" tooltip="Incremental principal component analysis (IPCA) is typically used as a replacement for principa...">
<div class="figure" id="id12"> <img alt="../../_images/sphx_glr_plot_incremental_pca_thumb.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_incremental_pca_thumb.png"> <p class="caption"><span class="caption-text"><a class="reference internal" href="../../../auto_examples/decomposition/plot_incremental_pca/#sphx-glr-auto-examples-decomposition-plot-incremental-pca-py"><span class="std std-ref">Incremental PCA</span></a></span></p> </div> </div>
<div class="sphx-glr-thumbcontainer" tooltip="This example shows that Kernel PCA is able to find a projection of the data that makes data lin...">
<div class="figure" id="id13"> <img alt="../../_images/sphx_glr_plot_kernel_pca_thumb.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_kernel_pca_thumb.png"> <p class="caption"><span class="caption-text"><a class="reference internal" href="../../../auto_examples/decomposition/plot_kernel_pca/#sphx-glr-auto-examples-decomposition-plot-kernel-pca-py"><span class="std std-ref">Kernel PCA</span></a></span></p> </div> </div>
<div class="sphx-glr-thumbcontainer" tooltip="These figures aid in illustrating how a point cloud can be very flat in one direction--which is...">
<div class="figure" id="id14"> <img alt="../../_images/sphx_glr_plot_pca_3d_thumb.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_pca_3d_thumb.png"> <p class="caption"><span class="caption-text"><a class="reference internal" href="../../../auto_examples/decomposition/plot_pca_3d/#sphx-glr-auto-examples-decomposition-plot-pca-3d-py"><span class="std std-ref">Principal components analysis (PCA)</span></a></span></p> </div> </div>
<div class="sphx-glr-thumbcontainer" tooltip="Principal Component Analysis applied to the Iris dataset.">
<div class="figure" id="id15"> <img alt="../../_images/sphx_glr_plot_pca_iris_thumb.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_pca_iris_thumb.png"> <p class="caption"><span class="caption-text"><a class="reference internal" href="../../../auto_examples/decomposition/plot_pca_iris/#sphx-glr-auto-examples-decomposition-plot-pca-iris-py"><span class="std std-ref">PCA example with Iris Data-set</span></a></span></p> </div> </div>
<div class="sphx-glr-thumbcontainer" tooltip="Probabilistic PCA and Factor Analysis are probabilistic models. The consequence is that the lik...">
<div class="figure" id="id16"> <img alt="../../_images/sphx_glr_plot_pca_vs_fa_model_selection_thumb.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_pca_vs_fa_model_selection_thumb.png"> <p class="caption"><span class="caption-text"><a class="reference internal" href="../../../auto_examples/decomposition/plot_pca_vs_fa_model_selection/#sphx-glr-auto-examples-decomposition-plot-pca-vs-fa-model-selection-py"><span class="std std-ref">Model selection with Probabilistic PCA and Factor Analysis (FA)</span></a></span></p> </div> </div>
<div class="sphx-glr-thumbcontainer" tooltip="The Iris dataset represents 3 kind of Iris flowers (Setosa, Versicolour and Virginica) with 4 a...">
<div class="figure" id="id17"> <img alt="../../_images/sphx_glr_plot_pca_vs_lda_thumb.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_pca_vs_lda_thumb.png"> <p class="caption"><span class="caption-text"><a class="reference internal" href="../../../auto_examples/decomposition/plot_pca_vs_lda/#sphx-glr-auto-examples-decomposition-plot-pca-vs-lda-py"><span class="std std-ref">Comparison of LDA and PCA 2D projection of Iris dataset</span></a></span></p> </div> </div>
<div class="sphx-glr-thumbcontainer" tooltip="An illustration of the metric and non-metric MDS on generated noisy data.">
<div class="figure" id="id18"> <img alt="../../_images/sphx_glr_plot_mds_thumb.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_mds_thumb.png"> <p class="caption"><span class="caption-text"><a class="reference internal" href="../../../auto_examples/manifold/plot_mds/#sphx-glr-auto-examples-manifold-plot-mds-py"><span class="std std-ref">Multi-dimensional scaling</span></a></span></p> </div> </div>
<div class="sphx-glr-thumbcontainer" tooltip="This example shows how kernel density estimation (KDE), a powerful non-parametric density estim...">
<div class="figure" id="id19"> <img alt="../../_images/sphx_glr_plot_digits_kde_sampling_thumb.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_digits_kde_sampling_thumb.png"> <p class="caption"><span class="caption-text"><a class="reference internal" href="../../../auto_examples/neighbors/plot_digits_kde_sampling/#sphx-glr-auto-examples-neighbors-plot-digits-kde-sampling-py"><span class="std std-ref">Kernel Density Estimation</span></a></span></p> </div> </div>
<div class="sphx-glr-thumbcontainer" tooltip="Shows how to use a function transformer in a pipeline. If you know your dataset's first princip...">
<div class="figure" id="id20"> <img alt="../../_images/sphx_glr_plot_function_transformer_thumb.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_function_transformer_thumb.png"> <p class="caption"><span class="caption-text"><a class="reference internal" href="../../../auto_examples/preprocessing/plot_function_transformer/#sphx-glr-auto-examples-preprocessing-plot-function-transformer-py"><span class="std std-ref">Using FunctionTransformer to select columns</span></a></span></p> </div> </div>
<div class="_attribution">
  <p class="_attribution-p">
    © 2007–2016 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html" class="_attribution-link" target="_blank">http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html</a>
  </p>
</div>

			</div>
		</div>
	</section>

	</div>
</body>
</html>
