
<!DOCTYPE HTML>

<html lang="en">

<head>
  <meta charset="utf-8">
  <title>feature_extraction.text.HashingVectorizer() - Scikit-learn - W3cubDocs</title>
  
  <meta name="description" content=" Convert a collection of text documents to a matrix of token occurrences ">
  <meta name="keywords" content="sklearn, feature, extraction, text, hashingvectorizer, -, scikit-learn, scikit_learn">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="http://docs.w3cub.com/scikit_learn/modules/generated/sklearn.feature_extraction.text.hashingvectorizer/">
  <link href="/favicon.png" rel="icon">
  <link type="text/css" rel="stylesheet" href="/assets/application-50364fff564ce3b6327021805f3f00e2957b441cf27f576a7dd4ff63bbc47047.css">
  <script type="text/javascript" src="/assets/application-db64bfd54ceb42be11af7995804cf4902548419ceb79d509b0b7d62c22d98e6f.js"></script>
  <script src="/json/scikit_learn.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body>
	<div class="_app">
	<header class="_header">
  
  <form class="_search">
    <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
    <a class="_search-clear"></a>
    <div class="_search-tag"></div>
  </form>
  
  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/scikit_learn/" class="_nav-link" title="" style="margin-left:0;">scikit-learn</a></span>
  
  <nav class="_nav">
    <a href="/app/" class="_nav-link ">App</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		<div class="_list">
			
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<div class="_page _sphinx">
				
<h1 id="sklearn-feature-extraction-text-hashingvectorizer">sklearn.feature_extraction.text.HashingVectorizer</h1> <dl class="class"> <dt id="sklearn.feature_extraction.text.HashingVectorizer">
<code>class sklearn.feature_extraction.text.HashingVectorizer(input=u'content', encoding=u'utf-8', decode_error=u'strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern=u'(?u)\b\w\w+\b', ngram_range=(1, 1), analyzer=u'word', n_features=1048576, binary=False, norm=u'l2', non_negative=False, dtype=<type>)</type></code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/feature_extraction/text.py#L284" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Convert a collection of text documents to a matrix of token occurrences</p> <p>It turns a collection of text documents into a scipy.sparse matrix holding token occurrence counts (or binary occurrence information), possibly normalized as token frequencies if norm=’l1’ or projected on the euclidean unit sphere if norm=’l2’.</p> <p>This text vectorizer implementation uses the hashing trick to find the token string name to feature integer index mapping.</p> <p>This strategy has several advantages:</p> <ul class="simple"> <li>it is very low memory scalable to large datasets as there is no need to store a vocabulary dictionary in memory</li> <li>it is fast to pickle and un-pickle as it holds no state besides the constructor parameters</li> <li>it can be used in a streaming (partial fit) or parallel pipeline as there is no state computed during fit.</li> </ul> <p>There are also a couple of cons (vs using a CountVectorizer with an in-memory vocabulary):</p> <ul class="simple"> <li>there is no way to compute the inverse transform (from feature indices to string feature names) which can be a problem when trying to introspect which features are most important to a model.</li> <li>there can be collisions: distinct tokens can be mapped to the same feature index. However in practice this is rarely an issue if n_features is large enough (e.g. 2 ** 18 for text classification problems).</li> <li>no IDF weighting as this would render the transformer stateful.</li> </ul> <p>The hash function employed is the signed 32-bit version of Murmurhash3.</p> <p>Read more in the <a class="reference internal" href="../../feature_extraction/#text-feature-extraction"><span class="std std-ref">User Guide</span></a>.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr class="field-odd field">
<th class="field-name">Parameters:</th>
<td class="field-body">
<p class="first"><strong>input</strong> : string {‘filename’, ‘file’, ‘content’}</p>  <p>If ‘filename’, the sequence passed as an argument to fit is expected to be a list of filenames that need reading to fetch the raw content to analyze.</p> <p>If ‘file’, the sequence items must have a ‘read’ method (file-like object) that is called to fetch the bytes in memory.</p> <p>Otherwise the input is expected to be the sequence strings or bytes items are expected to be analyzed directly.</p>  <p><strong>encoding</strong> : string, default=’utf-8’</p>  <p>If bytes or files are given to analyze, this encoding is used to decode.</p>  <p><strong>decode_error</strong> : {‘strict’, ‘ignore’, ‘replace’}</p>  <p>Instruction on what to do if a byte sequence is given to analyze that contains characters not of the given <code>encoding</code>. By default, it is ‘strict’, meaning that a UnicodeDecodeError will be raised. Other values are ‘ignore’ and ‘replace’.</p>  <p><strong>strip_accents</strong> : {‘ascii’, ‘unicode’, None}</p>  <p>Remove accents during the preprocessing step. ‘ascii’ is a fast method that only works on characters that have an direct ASCII mapping. ‘unicode’ is a slightly slower method that works on any characters. None (default) does nothing.</p>  <p><strong>analyzer</strong> : string, {‘word’, ‘char’, ‘char_wb’} or callable</p>  <p>Whether the feature should be made of word or character n-grams. Option ‘char_wb’ creates character n-grams only from text inside word boundaries.</p> <p>If a callable is passed it is used to extract the sequence of features out of the raw, unprocessed input.</p>  <p><strong>preprocessor</strong> : callable or None (default)</p>  <p>Override the preprocessing (string transformation) stage while preserving the tokenizing and n-grams generation steps.</p>  <p><strong>tokenizer</strong> : callable or None (default)</p>  <p>Override the string tokenization step while preserving the preprocessing and n-grams generation steps. Only applies if <code>analyzer == 'word'</code>.</p>  <p><strong>ngram_range</strong> : tuple (min_n, max_n), default=(1, 1)</p>  <p>The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n &lt;= n &lt;= max_n will be used.</p>  <p><strong>stop_words</strong> : string {‘english’}, list, or None (default)</p>  <p>If ‘english’, a built-in stop word list for English is used.</p> <p>If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens. Only applies if <code>analyzer == 'word'</code>.</p>  <p><strong>lowercase</strong> : boolean, default=True</p>  <p>Convert all characters to lowercase before tokenizing.</p>  <p><strong>token_pattern</strong> : string</p>  <p>Regular expression denoting what constitutes a “token”, only used if <code>analyzer == 'word'</code>. The default regexp selects tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator).</p>  <p><strong>n_features</strong> : integer, default=(2 ** 20)</p>  <p>The number of features (columns) in the output matrices. Small numbers of features are likely to cause hash collisions, but large numbers will cause larger coefficient dimensions in linear learners.</p>  <p><strong>norm</strong> : ‘l1’, ‘l2’ or None, optional</p>  <p>Norm used to normalize term vectors. None for no normalization.</p>  <p><strong>binary: boolean, default=False.</strong> :</p>  <p>If True, all non zero counts are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts.</p>  <p><strong>dtype: type, optional</strong> :</p>  <p>Type of the matrix returned by fit_transform() or transform().</p>  <p><strong>non_negative</strong> : boolean, default=False</p>  <p>Whether output matrices should contain non-negative values only; effectively calls abs on the matrix prior to returning it. When True, output values can be interpreted as frequencies. When False, output values will have expected value zero.</p>  </td> </tr>  </table> <div class="admonition seealso"> <p class="first admonition-title">See also</p> <p class="last"><a class="reference internal" href="../sklearn.feature_extraction.text.countvectorizer/#sklearn.feature_extraction.text.CountVectorizer" title="sklearn.feature_extraction.text.CountVectorizer"><code>CountVectorizer</code></a>, <a class="reference internal" href="../sklearn.feature_extraction.text.tfidfvectorizer/#sklearn.feature_extraction.text.TfidfVectorizer" title="sklearn.feature_extraction.text.TfidfVectorizer"><code>TfidfVectorizer</code></a></p> </div> <h4 class="rubric">Methods</h4> <table class="longtable docutils">   <tr class="row-odd">
<td>
<a class="reference internal" href="#sklearn.feature_extraction.text.HashingVectorizer.build_analyzer" title="sklearn.feature_extraction.text.HashingVectorizer.build_analyzer"><code>build_analyzer</code></a>()</td> <td>Return a callable that handles preprocessing and tokenization</td> </tr> <tr class="row-even">
<td>
<a class="reference internal" href="#sklearn.feature_extraction.text.HashingVectorizer.build_preprocessor" title="sklearn.feature_extraction.text.HashingVectorizer.build_preprocessor"><code>build_preprocessor</code></a>()</td> <td>Return a function to preprocess the text before tokenization</td> </tr> <tr class="row-odd">
<td>
<a class="reference internal" href="#sklearn.feature_extraction.text.HashingVectorizer.build_tokenizer" title="sklearn.feature_extraction.text.HashingVectorizer.build_tokenizer"><code>build_tokenizer</code></a>()</td> <td>Return a function that splits a string into a sequence of tokens</td> </tr> <tr class="row-even">
<td>
<a class="reference internal" href="#sklearn.feature_extraction.text.HashingVectorizer.decode" title="sklearn.feature_extraction.text.HashingVectorizer.decode"><code>decode</code></a>(doc)</td> <td>Decode the input into a string of unicode symbols</td> </tr> <tr class="row-odd">
<td>
<a class="reference internal" href="#sklearn.feature_extraction.text.HashingVectorizer.fit" title="sklearn.feature_extraction.text.HashingVectorizer.fit"><code>fit</code></a>(X[, y])</td> <td>Does nothing: this transformer is stateless.</td> </tr> <tr class="row-even">
<td>
<a class="reference internal" href="#sklearn.feature_extraction.text.HashingVectorizer.fit_transform" title="sklearn.feature_extraction.text.HashingVectorizer.fit_transform"><code>fit_transform</code></a>(X[, y])</td> <td>Transform a sequence of documents to a document-term matrix.</td> </tr> <tr class="row-odd">
<td>
<a class="reference internal" href="#sklearn.feature_extraction.text.HashingVectorizer.get_params" title="sklearn.feature_extraction.text.HashingVectorizer.get_params"><code>get_params</code></a>([deep])</td> <td>Get parameters for this estimator.</td> </tr> <tr class="row-even">
<td>
<a class="reference internal" href="#sklearn.feature_extraction.text.HashingVectorizer.get_stop_words" title="sklearn.feature_extraction.text.HashingVectorizer.get_stop_words"><code>get_stop_words</code></a>()</td> <td>Build or fetch the effective stop words list</td> </tr> <tr class="row-odd">
<td>
<a class="reference internal" href="#sklearn.feature_extraction.text.HashingVectorizer.partial_fit" title="sklearn.feature_extraction.text.HashingVectorizer.partial_fit"><code>partial_fit</code></a>(X[, y])</td> <td>Does nothing: this transformer is stateless.</td> </tr> <tr class="row-even">
<td>
<a class="reference internal" href="#sklearn.feature_extraction.text.HashingVectorizer.set_params" title="sklearn.feature_extraction.text.HashingVectorizer.set_params"><code>set_params</code></a>(**params)</td> <td>Set the parameters of this estimator.</td> </tr> <tr class="row-odd">
<td>
<a class="reference internal" href="#sklearn.feature_extraction.text.HashingVectorizer.transform" title="sklearn.feature_extraction.text.HashingVectorizer.transform"><code>transform</code></a>(X[, y])</td> <td>Transform a sequence of documents to a document-term matrix.</td> </tr>  </table> <dl class="method"> <dt id="sklearn.feature_extraction.text.HashingVectorizer.__init__">
<code>__init__(input=u'content', encoding=u'utf-8', decode_error=u'strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern=u'(?u)\\b\\w\\w+\\b', ngram_range=(1, 1), analyzer=u'word', n_features=1048576, binary=False, norm=u'l2', non_negative=False, dtype=<type>)</type></code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/feature_extraction/text.py#L419" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="sklearn.feature_extraction.text.HashingVectorizer.build_analyzer">
<code>build_analyzer()</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/feature_extraction/text.py#L222" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Return a callable that handles preprocessing and tokenization</p> </dd>
</dl> <dl class="method"> <dt id="sklearn.feature_extraction.text.HashingVectorizer.build_preprocessor">
<code>build_preprocessor()</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/feature_extraction/text.py#L181" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Return a function to preprocess the text before tokenization</p> </dd>
</dl> <dl class="method"> <dt id="sklearn.feature_extraction.text.HashingVectorizer.build_tokenizer">
<code>build_tokenizer()</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/feature_extraction/text.py#L211" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Return a function that splits a string into a sequence of tokens</p> </dd>
</dl> <dl class="method"> <dt id="sklearn.feature_extraction.text.HashingVectorizer.decode">
<code>decode(doc)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/feature_extraction/text.py#L105" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Decode the input into a string of unicode symbols</p> <p>The decoding strategy depends on the vectorizer parameters.</p> </dd>
</dl> <dl class="method"> <dt id="sklearn.feature_extraction.text.HashingVectorizer.fit">
<code>fit(X, y=None)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/feature_extraction/text.py#L452" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Does nothing: this transformer is stateless.</p> </dd>
</dl> <dl class="method"> <dt id="sklearn.feature_extraction.text.HashingVectorizer.fit_transform">
<code>fit_transform(X, y=None)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/feature_extraction/text.py#L458" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Transform a sequence of documents to a document-term matrix.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr class="field-odd field">
<th class="field-name">Parameters:</th>
<td class="field-body">
<p class="first"><strong>X</strong> : iterable over raw text documents, length = n_samples</p>  <p>Samples. Each sample must be a text document (either bytes or unicode strings, file name or file object depending on the constructor argument) which will be tokenized and hashed.</p>  <p><strong>y</strong> : (ignored)</p> </td> </tr> <tr class="field-even field">
<th class="field-name">Returns:</th>
<td class="field-body">
<p class="first"><strong>X</strong> : scipy.sparse matrix, shape = (n_samples, self.n_features)</p>  <p>Document-term matrix.</p>  </td> </tr>  </table> </dd>
</dl> <dl class="method"> <dt id="sklearn.feature_extraction.text.HashingVectorizer.get_params">
<code>get_params(deep=True)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/base.py#L220" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Get parameters for this estimator.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr class="field-odd field">
<th class="field-name">Parameters:</th>
<td class="field-body">
<p class="first"><strong>deep: boolean, optional</strong> :</p>  <p>If True, will return the parameters for this estimator and contained subobjects that are estimators.</p>  </td> </tr> <tr class="field-even field">
<th class="field-name">Returns:</th>
<td class="field-body">
<p class="first"><strong>params</strong> : mapping of string to any</p>  <p>Parameter names mapped to their values.</p>  </td> </tr>  </table> </dd>
</dl> <dl class="method"> <dt id="sklearn.feature_extraction.text.HashingVectorizer.get_stop_words">
<code>get_stop_words()</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/feature_extraction/text.py#L218" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Build or fetch the effective stop words list</p> </dd>
</dl> <dl class="method"> <dt id="sklearn.feature_extraction.text.HashingVectorizer.partial_fit">
<code>partial_fit(X, y=None)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/feature_extraction/text.py#L443" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Does nothing: this transformer is stateless.</p> <p>This method is just there to mark the fact that this transformer can work in a streaming setup.</p> </dd>
</dl> <dl class="method"> <dt id="sklearn.feature_extraction.text.HashingVectorizer.set_params">
<code>set_params(**params)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/base.py#L257" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Set the parameters of this estimator.</p> <p>The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <code>&lt;component&gt;__&lt;parameter&gt;</code> so that it’s possible to update each component of a nested object.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr class="field-odd field">
<th class="field-name">Returns:</th>
<td class="field-body">
<strong>self</strong> :</td> </tr>  </table> </dd>
</dl> <dl class="method"> <dt id="sklearn.feature_extraction.text.HashingVectorizer.transform">
<code>transform(X, y=None)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/feature_extraction/text.py#L458" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Transform a sequence of documents to a document-term matrix.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr class="field-odd field">
<th class="field-name">Parameters:</th>
<td class="field-body">
<p class="first"><strong>X</strong> : iterable over raw text documents, length = n_samples</p>  <p>Samples. Each sample must be a text document (either bytes or unicode strings, file name or file object depending on the constructor argument) which will be tokenized and hashed.</p>  <p><strong>y</strong> : (ignored)</p> </td> </tr> <tr class="field-even field">
<th class="field-name">Returns:</th>
<td class="field-body">
<p class="first"><strong>X</strong> : scipy.sparse matrix, shape = (n_samples, self.n_features)</p>  <p>Document-term matrix.</p>  </td> </tr>  </table> </dd>
</dl> </dd>
</dl>  <h2 id="examples-using-sklearn-feature-extraction-text-hashingvectorizer">Examples using <code>sklearn.feature_extraction.text.HashingVectorizer</code>
</h2> <div class="sphx-glr-thumbcontainer" tooltip="This is an example showing how scikit-learn can be used for classification using an out-of-core...">
<div class="figure" id="id1"> <img alt="../../_images/sphx_glr_plot_out_of_core_classification_thumb.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_out_of_core_classification_thumb.png"> <p class="caption"><span class="caption-text"><a class="reference internal" href="../../../auto_examples/applications/plot_out_of_core_classification/#sphx-glr-auto-examples-applications-plot-out-of-core-classification-py"><span class="std std-ref">Out-of-core classification of text documents</span></a></span></p> </div> </div>
<div class="sphx-glr-thumbcontainer" tooltip="This is an example showing how scikit-learn can be used to classify documents by topics using a...">
<div class="figure" id="id2"> <img alt="../../_images/sphx_glr_document_classification_20newsgroups_thumb.png" src="http://scikit-learn.org/stable/_images/sphx_glr_document_classification_20newsgroups_thumb.png"> <p class="caption"><span class="caption-text"><a class="reference internal" href="../../../auto_examples/text/document_classification_20newsgroups/#sphx-glr-auto-examples-text-document-classification-20newsgroups-py"><span class="std std-ref">Classification of text documents using sparse features</span></a></span></p> </div> </div>
<div class="sphx-glr-thumbcontainer" tooltip="This is an example showing how the scikit-learn can be used to cluster documents by topics usin...">
<div class="figure" id="id3"> <img alt="../../_images/sphx_glr_document_clustering_thumb.png" src="http://scikit-learn.org/stable/_images/sphx_glr_document_clustering_thumb.png"> <p class="caption"><span class="caption-text"><a class="reference internal" href="../../../auto_examples/text/document_clustering/#sphx-glr-auto-examples-text-document-clustering-py"><span class="std std-ref">Clustering text documents using k-means</span></a></span></p> </div> </div>
<div class="_attribution">
  <p class="_attribution-p">
    © 2007–2016 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html" class="_attribution-link" target="_blank">http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html</a>
  </p>
</div>

			</div>
		</div>
	</section>

	</div>
</body>
</html>
