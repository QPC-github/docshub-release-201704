
<!DOCTYPE HTML>

<html lang="en">

<head>
  <meta charset="utf-8">
  <title>manifold.TSNE() - Scikit-learn - W3cubDocs</title>
  
  <meta name="description" content=" t-distributed Stochastic Neighbor Embedding. ">
  <meta name="keywords" content="sklearn, manifold, tsne, -, scikit-learn, scikit_learn">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="http://docs.w3cub.com/scikit_learn/modules/generated/sklearn.manifold.tsne/">
  <link href="/favicon.png" rel="icon">
  <link type="text/css" rel="stylesheet" href="/assets/application-50364fff564ce3b6327021805f3f00e2957b441cf27f576a7dd4ff63bbc47047.css">
  <script type="text/javascript" src="/assets/application-db64bfd54ceb42be11af7995804cf4902548419ceb79d509b0b7d62c22d98e6f.js"></script>
  <script src="/json/scikit_learn.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body>
	<div class="_app">
	<header class="_header">
  
  <form class="_search">
    <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
    <a class="_search-clear"></a>
    <div class="_search-tag"></div>
  </form>
  
  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/scikit_learn/" class="_nav-link" title="" style="margin-left:0;">scikit-learn</a></span>
  
  <nav class="_nav">
    <a href="/app/" class="_nav-link ">App</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		<div class="_list">
			
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<div class="_page _sphinx">
				
<h1 id="sklearn-manifold-tsne">sklearn.manifold.TSNE</h1> <dl class="class"> <dt id="sklearn.manifold.TSNE">
<code>class sklearn.manifold.TSNE(n_components=2, perplexity=30.0, early_exaggeration=4.0, learning_rate=1000.0, n_iter=1000, n_iter_without_progress=30, min_grad_norm=1e-07, metric='euclidean', init='random', verbose=0, random_state=None, method='barnes_hut', angle=0.5)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/manifold/t_sne.py#L497" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>t-distributed Stochastic Neighbor Embedding.</p> <p>t-SNE [1] is a tool to visualize high-dimensional data. It converts similarities between data points to joint probabilities and tries to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data. t-SNE has a cost function that is not convex, i.e. with different initializations we can get different results.</p> <p>It is highly recommended to use another dimensionality reduction method (e.g. PCA for dense data or TruncatedSVD for sparse data) to reduce the number of dimensions to a reasonable amount (e.g. 50) if the number of features is very high. This will suppress some noise and speed up the computation of pairwise distances between samples. For more tips see Laurens van der Maaten’s FAQ [2].</p> <p>Read more in the <a class="reference internal" href="../../manifold/#t-sne"><span class="std std-ref">User Guide</span></a>.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr class="field-odd field">
<th class="field-name">Parameters:</th>
<td class="field-body">
<p class="first"><strong>n_components</strong> : int, optional (default: 2)</p>  <p>Dimension of the embedded space.</p>  <p><strong>perplexity</strong> : float, optional (default: 30)</p>  <p>The perplexity is related to the number of nearest neighbors that is used in other manifold learning algorithms. Larger datasets usually require a larger perplexity. Consider selecting a value between 5 and 50. The choice is not extremely critical since t-SNE is quite insensitive to this parameter.</p>  <p><strong>early_exaggeration</strong> : float, optional (default: 4.0)</p>  <p>Controls how tight natural clusters in the original space are in the embedded space and how much space will be between them. For larger values, the space between natural clusters will be larger in the embedded space. Again, the choice of this parameter is not very critical. If the cost function increases during initial optimization, the early exaggeration factor or the learning rate might be too high.</p>  <p><strong>learning_rate</strong> : float, optional (default: 1000)</p>  <p>The learning rate can be a critical parameter. It should be between 100 and 1000. If the cost function increases during initial optimization, the early exaggeration factor or the learning rate might be too high. If the cost function gets stuck in a bad local minimum increasing the learning rate helps sometimes.</p>  <p><strong>n_iter</strong> : int, optional (default: 1000)</p>  <p>Maximum number of iterations for the optimization. Should be at least 200.</p>  <p><strong>n_iter_without_progress</strong> : int, optional (default: 30)</p>  <p>Maximum number of iterations without progress before we abort the optimization.</p> <div class="versionadded"> <p><span class="versionmodified">New in version 0.17: </span>parameter <em>n_iter_without_progress</em> to control stopping criteria.</p> </div>  <p><strong>min_grad_norm</strong> : float, optional (default: 1E-7)</p>  <p>If the gradient norm is below this threshold, the optimization will be aborted.</p>  <p><strong>metric</strong> : string or callable, optional</p>  <p>The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options allowed by scipy.spatial.distance.pdist for its metric parameter, or a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS. If metric is “precomputed”, X is assumed to be a distance matrix. Alternatively, if metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays from X as input and return a value indicating the distance between them. The default is “euclidean” which is interpreted as squared euclidean distance.</p>  <p><strong>init</strong> : string or numpy array, optional (default: “random”)</p>  <p>Initialization of embedding. Possible options are ‘random’, ‘pca’, and a numpy array of shape (n_samples, n_components). PCA initialization cannot be used with precomputed distances and is usually more globally stable than random initialization.</p>  <p><strong>verbose</strong> : int, optional (default: 0)</p>  <p>Verbosity level.</p>  <p><strong>random_state</strong> : int or RandomState instance or None (default)</p>  <p>Pseudo Random Number generator seed control. If None, use the numpy.random singleton. Note that different initializations might result in different local minima of the cost function.</p>  <p><strong>method</strong> : string (default: ‘barnes_hut’)</p>  <p>By default the gradient calculation algorithm uses Barnes-Hut approximation running in O(NlogN) time. method=’exact’ will run on the slower, but exact, algorithm in O(N^2) time. The exact algorithm should be used when nearest-neighbor errors need to be better than 3%. However, the exact method cannot scale to millions of examples.</p> <div class="versionadded"> <p><span class="versionmodified">New in version 0.17: </span>Approximate optimization <em>method</em> via the Barnes-Hut.</p> </div>  <p><strong>angle</strong> : float (default: 0.5)</p>  <p>Only used if method=’barnes_hut’ This is the trade-off between speed and accuracy for Barnes-Hut T-SNE. ‘angle’ is the angular size (referred to as theta in [3]) of a distant node as measured from a point. If this size is below ‘angle’ then it is used as a summary node of all points contained within it. This method is not very sensitive to changes in this parameter in the range of 0.2 - 0.8. Angle less than 0.2 has quickly increasing computation time and angle greater 0.8 has quickly increasing error.</p>  </td> </tr> <tr class="field-even field">
<th class="field-name">Attributes:</th>
<td class="field-body">
<p class="first"><strong>embedding_</strong> : array-like, shape (n_samples, n_components)</p>  <p>Stores the embedding vectors.</p>  <p><strong>kl_divergence_</strong> : float</p>  <p>Kullback-Leibler divergence after optimization.</p>  </td> </tr>  </table> <h4 class="rubric">References</h4> <dl class="docutils"> <dt>[1] van der Maaten, L.J.P.; Hinton, G.E. Visualizing High-Dimensional Data</dt> <dd>Using t-SNE. Journal of Machine Learning Research 9:2579-2605, 2008.</dd> <dt>[2] van der Maaten, L.J.P. t-Distributed Stochastic Neighbor Embedding</dt> <dd><a class="reference external" href="http://homepage.tudelft.nl/19j49/t-SNE.html" target="_blank">http://homepage.tudelft.nl/19j49/t-SNE.html</a></dd> <dt>[3] L.J.P. van der Maaten. Accelerating t-SNE using Tree-Based Algorithms.</dt> <dd>Journal of Machine Learning Research 15(Oct):3221-3245, 2014. <a class="reference external" href="http://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf" target="_blank">http://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf</a>
</dd> </dl> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from sklearn.manifold import TSNE
&gt;&gt;&gt; X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])
&gt;&gt;&gt; model = TSNE(n_components=2, random_state=0)
&gt;&gt;&gt; np.set_printoptions(suppress=True)
&gt;&gt;&gt; model.fit_transform(X) 
array([[ 0.00017599,  0.00003993],
       [ 0.00009891,  0.00021913],
       [ 0.00018554, -0.00009357],
       [ 0.00009528, -0.00001407]])
</pre> <h4 class="rubric">Methods</h4> <table class="longtable docutils">   <tr class="row-odd">
<td>
<a class="reference internal" href="#sklearn.manifold.TSNE.fit" title="sklearn.manifold.TSNE.fit"><code>fit</code></a>(X[, y])</td> <td>Fit X into an embedded space.</td> </tr> <tr class="row-even">
<td>
<a class="reference internal" href="#sklearn.manifold.TSNE.fit_transform" title="sklearn.manifold.TSNE.fit_transform"><code>fit_transform</code></a>(X[, y])</td> <td>Fit X into an embedded space and return that transformed output.</td> </tr> <tr class="row-odd">
<td>
<a class="reference internal" href="#sklearn.manifold.TSNE.get_params" title="sklearn.manifold.TSNE.get_params"><code>get_params</code></a>([deep])</td> <td>Get parameters for this estimator.</td> </tr> <tr class="row-even">
<td>
<a class="reference internal" href="#sklearn.manifold.TSNE.set_params" title="sklearn.manifold.TSNE.set_params"><code>set_params</code></a>(**params)</td> <td>Set the parameters of this estimator.</td> </tr>  </table> <dl class="method"> <dt id="sklearn.manifold.TSNE.__init__">
<code>__init__(n_components=2, perplexity=30.0, early_exaggeration=4.0, learning_rate=1000.0, n_iter=1000, n_iter_without_progress=30, min_grad_norm=1e-07, metric='euclidean', init='random', verbose=0, random_state=None, method='barnes_hut', angle=0.5)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/manifold/t_sne.py#L643" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="sklearn.manifold.TSNE.fit">
<code>fit(X, y=None)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/manifold/t_sne.py#L884" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Fit X into an embedded space.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr class="field-odd field">
<th class="field-name">Parameters:</th>
<td class="field-body">
<p class="first"><strong>X</strong> : array, shape (n_samples, n_features) or (n_samples, n_samples)</p>  <p>If the metric is ‘precomputed’ X must be a square distance matrix. Otherwise it contains a sample per row. If the method is ‘exact’, X may be a sparse matrix of type ‘csr’, ‘csc’ or ‘coo’.</p>  </td> </tr>  </table> </dd>
</dl> <dl class="method"> <dt id="sklearn.manifold.TSNE.fit_transform">
<code>fit_transform(X, y=None)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/manifold/t_sne.py#L865" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Fit X into an embedded space and return that transformed output.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr class="field-odd field">
<th class="field-name">Parameters:</th>
<td class="field-body">
<p class="first"><strong>X</strong> : array, shape (n_samples, n_features) or (n_samples, n_samples)</p>  <p>If the metric is ‘precomputed’ X must be a square distance matrix. Otherwise it contains a sample per row.</p>  </td> </tr> <tr class="field-even field">
<th class="field-name">Returns:</th>
<td class="field-body">
<p class="first"><strong>X_new</strong> : array, shape (n_samples, n_components)</p>  <p>Embedding of the training data in low-dimensional space.</p>  </td> </tr>  </table> </dd>
</dl> <dl class="method"> <dt id="sklearn.manifold.TSNE.get_params">
<code>get_params(deep=True)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/base.py#L220" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Get parameters for this estimator.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr class="field-odd field">
<th class="field-name">Parameters:</th>
<td class="field-body">
<p class="first"><strong>deep: boolean, optional</strong> :</p>  <p>If True, will return the parameters for this estimator and contained subobjects that are estimators.</p>  </td> </tr> <tr class="field-even field">
<th class="field-name">Returns:</th>
<td class="field-body">
<p class="first"><strong>params</strong> : mapping of string to any</p>  <p>Parameter names mapped to their values.</p>  </td> </tr>  </table> </dd>
</dl> <dl class="method"> <dt id="sklearn.manifold.TSNE.set_params">
<code>set_params(**params)</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/412996f/sklearn/base.py#L257" target="_blank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Set the parameters of this estimator.</p> <p>The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <code>&lt;component&gt;__&lt;parameter&gt;</code> so that it’s possible to update each component of a nested object.</p> <table class="docutils field-list" frame="void" rules="none"> <col class="field-name"> <col class="field-body">  <tr class="field-odd field">
<th class="field-name">Returns:</th>
<td class="field-body">
<strong>self</strong> :</td> </tr>  </table> </dd>
</dl> </dd>
</dl>  <h2 id="examples-using-sklearn-manifold-tsne">Examples using <code>sklearn.manifold.TSNE</code>
</h2> <div class="sphx-glr-thumbcontainer" tooltip="An illustration of dimensionality reduction on the S-curve dataset with various manifold learni...">
<div class="figure" id="id1"> <img alt="../../_images/sphx_glr_plot_compare_methods_thumb.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_compare_methods_thumb.png"> <p class="caption"><span class="caption-text"><a class="reference internal" href="../../../auto_examples/manifold/plot_compare_methods/#sphx-glr-auto-examples-manifold-plot-compare-methods-py"><span class="std std-ref">Comparison of Manifold Learning methods</span></a></span></p> </div> </div>
<div class="sphx-glr-thumbcontainer" tooltip="An illustration of various embeddings on the digits dataset.">
<div class="figure" id="id2"> <img alt="../../_images/sphx_glr_plot_lle_digits_thumb.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_lle_digits_thumb.png"> <p class="caption"><span class="caption-text"><a class="reference internal" href="../../../auto_examples/manifold/plot_lle_digits/#sphx-glr-auto-examples-manifold-plot-lle-digits-py"><span class="std std-ref">Manifold learning on handwritten digits: Locally Linear Embedding, Isomap...</span></a></span></p> </div> </div>
<div class="sphx-glr-thumbcontainer" tooltip="An application of the different :ref:`manifold` techniques on a spherical data-set. Here one ca...">
<div class="figure" id="id3"> <img alt="../../_images/sphx_glr_plot_manifold_sphere_thumb.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_manifold_sphere_thumb.png"> <p class="caption"><span class="caption-text"><a class="reference internal" href="../../../auto_examples/manifold/plot_manifold_sphere/#sphx-glr-auto-examples-manifold-plot-manifold-sphere-py"><span class="std std-ref">Manifold Learning methods on a severed sphere</span></a></span></p> </div> </div>
<div class="_attribution">
  <p class="_attribution-p">
    © 2007–2016 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html" class="_attribution-link" target="_blank">http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html</a>
  </p>
</div>

			</div>
		</div>
	</section>

	</div>
</body>
</html>
