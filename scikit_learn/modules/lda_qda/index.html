
<!DOCTYPE HTML>

<html lang="en">

<head>
  <meta charset="utf-8">
  <title>1.2. Linear and Quadratic Discriminant Analysis - Scikit-learn - W3cubDocs</title>
  
  <meta name="description" content="Linear Discriminant Analysis (discriminant_analysis.LinearDiscriminantAnalysis) and Quadratic Discriminant Analysis (discriminant_analysis. &hellip;">
  <meta name="keywords" content="linear, and, quadratic, discriminant, analysis, -, scikit-learn, scikit_learn">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="http://docs.w3cub.com/scikit_learn/modules/lda_qda/">
  <link href="/favicon.png" rel="icon">
  <link type="text/css" rel="stylesheet" href="/assets/application-50364fff564ce3b6327021805f3f00e2957b441cf27f576a7dd4ff63bbc47047.css">
  <script type="text/javascript" src="/assets/application-db64bfd54ceb42be11af7995804cf4902548419ceb79d509b0b7d62c22d98e6f.js"></script>
  <script src="/json/scikit_learn.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body>
	<div class="_app">
	<header class="_header">
  
  <form class="_search">
    <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
    <a class="_search-clear"></a>
    <div class="_search-tag"></div>
  </form>
  
  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/scikit_learn/" class="_nav-link" title="" style="margin-left:0;">scikit-learn</a></span>
  
  <nav class="_nav">
    <a href="/app/" class="_nav-link ">App</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		<div class="_list">
			
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<div class="_page _sphinx">
				
<h1 id="lda-qda">1.2. Linear and Quadratic Discriminant Analysis</h1> <p id="linear-and-quadratic-discriminant-analysis">Linear Discriminant Analysis (<a class="reference internal" href="../generated/sklearn.discriminant_analysis.lineardiscriminantanalysis/#sklearn.discriminant_analysis.LinearDiscriminantAnalysis" title="sklearn.discriminant_analysis.LinearDiscriminantAnalysis"><code>discriminant_analysis.LinearDiscriminantAnalysis</code></a>) and Quadratic Discriminant Analysis (<a class="reference internal" href="../generated/sklearn.discriminant_analysis.quadraticdiscriminantanalysis/#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis" title="sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis"><code>discriminant_analysis.QuadraticDiscriminantAnalysis</code></a>) are two classic classifiers, with, as their names suggest, a linear and a quadratic decision surface, respectively.</p> <p>These classifiers are attractive because they have closed-form solutions that can be easily computed, are inherently multiclass, have proven to work well in practice and have no hyperparameters to tune.</p> <p class="centered"> <strong><a class="reference external" href="../../auto_examples/classification/plot_lda_qda/"><img alt="ldaqda" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_lda_qda_0011.png" style="width: 640.0px; height: 480.0px;"></a></strong></p>
<p>The plot shows decision boundaries for Linear Discriminant Analysis and Quadratic Discriminant Analysis. The bottom row demonstrates that Linear Discriminant Analysis can only learn linear boundaries, while Quadratic Discriminant Analysis can learn quadratic boundaries and is therefore more flexible.</p> <div class="topic"> <p class="topic-title first">Examples:</p> <p><a class="reference internal" href="../../auto_examples/classification/plot_lda_qda/#sphx-glr-auto-examples-classification-plot-lda-qda-py"><span class="std std-ref">Linear and Quadratic Discriminant Analysis with confidence ellipsoid</span></a>: Comparison of LDA and QDA on synthetic data.</p> </div>  <h2 id="dimensionality-reduction-using-linear-discriminant-analysis">1.2.1. Dimensionality reduction using Linear Discriminant Analysis</h2> <p><a class="reference internal" href="../generated/sklearn.discriminant_analysis.lineardiscriminantanalysis/#sklearn.discriminant_analysis.LinearDiscriminantAnalysis" title="sklearn.discriminant_analysis.LinearDiscriminantAnalysis"><code>discriminant_analysis.LinearDiscriminantAnalysis</code></a> can be used to perform supervised dimensionality reduction, by projecting the input data to a linear subspace consisting of the directions which maximize the separation between classes (in a precise sense discussed in the mathematics section below). The dimension of the output is necessarily less than the number of classes, so this is a in general a rather strong dimensionality reduction, and only makes senses in a multiclass setting.</p> <p>This is implemented in <a class="reference internal" href="../generated/sklearn.discriminant_analysis.lineardiscriminantanalysis/#sklearn.discriminant_analysis.LinearDiscriminantAnalysis.transform" title="sklearn.discriminant_analysis.LinearDiscriminantAnalysis.transform"><code>discriminant_analysis.LinearDiscriminantAnalysis.transform</code></a>. The desired dimensionality can be set using the <code>n_components</code> constructor parameter. This parameter has no influence on <a class="reference internal" href="../generated/sklearn.discriminant_analysis.lineardiscriminantanalysis/#sklearn.discriminant_analysis.LinearDiscriminantAnalysis.fit" title="sklearn.discriminant_analysis.LinearDiscriminantAnalysis.fit"><code>discriminant_analysis.LinearDiscriminantAnalysis.fit</code></a> or <a class="reference internal" href="../generated/sklearn.discriminant_analysis.lineardiscriminantanalysis/#sklearn.discriminant_analysis.LinearDiscriminantAnalysis.predict" title="sklearn.discriminant_analysis.LinearDiscriminantAnalysis.predict"><code>discriminant_analysis.LinearDiscriminantAnalysis.predict</code></a>.</p> <div class="topic"> <p class="topic-title first">Examples:</p> <p><a class="reference internal" href="../../auto_examples/decomposition/plot_pca_vs_lda/#sphx-glr-auto-examples-decomposition-plot-pca-vs-lda-py"><span class="std std-ref">Comparison of LDA and PCA 2D projection of Iris dataset</span></a>: Comparison of LDA and PCA for dimensionality reduction of the Iris dataset</p> </div>   <h2 id="mathematical-formulation-of-the-lda-and-qda-classifiers">1.2.2. Mathematical formulation of the LDA and QDA classifiers</h2> <p>Both LDA and QDA can be derived from simple probabilistic models which model the class conditional distribution of the data <img class="math" src="http://scikit-learn.org/stable/_images/math/9645b6a35da0e999282a01e7eb9c0b987b968155.png" alt="P(X|y=k)"> for each class <img class="math" src="http://scikit-learn.org/stable/_images/math/0b7c1e16a3a8a849bb8ffdcdbf86f65fd1f30438.png" alt="k">. Predictions can then be obtained by using Bayes’ rule:</p> <div class="math"> <p><img src="http://scikit-learn.org/stable/_images/math/478e4d9a1681e569927f5731a20e736e1eb9094b.png" alt="P(y=k | X) = \frac{P(X | y=k) P(y=k)}{P(X)} = \frac{P(X | y=k) P(y = k)}{ \sum_{l} P(X | y=l) \cdot P(y=l)}"></p> </div>
<p>and we select the class <img class="math" src="http://scikit-learn.org/stable/_images/math/0b7c1e16a3a8a849bb8ffdcdbf86f65fd1f30438.png" alt="k"> which maximizes this conditional probability.</p> <p>More specifically, for linear and quadratic discriminant analysis, <img class="math" src="http://scikit-learn.org/stable/_images/math/0caa57fa1ad5e123266bda48e45b51c80b7e559f.png" alt="P(X|y)"> is modelled as a multivariate Gaussian distribution with density:</p> <div class="math"> <p><img src="http://scikit-learn.org/stable/_images/math/dfe98a40b3ba6bc216cf7b693e6b8c5fdcd5874c.png" alt="p(X | y=k) = \frac{1}{(2\pi)^n |\Sigma_k|^{1/2}}\exp\left(-\frac{1}{2} (X-\mu_k)^t \Sigma_k^{-1} (X-\mu_k)\right)"></p> </div>
<p>To use this model as a classifier, we just need to estimate from the training data the class priors <img class="math" src="http://scikit-learn.org/stable/_images/math/799322e39f8da8e10882b790cad5c8edddb4d464.png" alt="P(y=k)"> (by the proportion of instances of class <img class="math" src="http://scikit-learn.org/stable/_images/math/0b7c1e16a3a8a849bb8ffdcdbf86f65fd1f30438.png" alt="k">), the class means <img class="math" src="http://scikit-learn.org/stable/_images/math/4c5e9e20a0019ba3e027121ffda9c4ef86e87042.png" alt="\mu_k"> (by the empirical sample class means) and the covariance matrices (either by the empirical sample class covariance matrices, or by a regularized estimator: see the section on shrinkage below).</p> <p>In the case of LDA, the Gaussians for each class are assumed to share the same covariance matrix: <img class="math" src="http://scikit-learn.org/stable/_images/math/69eb9440328a5f3e652acb6d66640a2f4d0351de.png" alt="\Sigma_k = \Sigma"> for all <img class="math" src="http://scikit-learn.org/stable/_images/math/0b7c1e16a3a8a849bb8ffdcdbf86f65fd1f30438.png" alt="k">. This leads to linear decision surfaces between, as can be seen by comparing the log-probability ratios <img class="math" src="http://scikit-learn.org/stable/_images/math/d143022379c1410e998d48d6d437df2bd4ed0726.png" alt="\log[P(y=k | X) / P(y=l | X)]">:</p> <div class="math"> <p><img src="http://scikit-learn.org/stable/_images/math/1d7534f3a07f0bdb9fc348b5f529ced01fe250bc.png" alt="\log\left(\frac{P(y=k|X)}{P(y=l | X)}\right) = 0 \Leftrightarrow (\mu_k-\mu_l)\Sigma^{-1} X = \frac{1}{2} (\mu_k^t \Sigma^{-1} \mu_k - \mu_l^t \Sigma^{-1} \mu_l)"></p> </div>
<p>In the case of QDA, there are no assumptions on the covariance matrices <img class="math" src="http://scikit-learn.org/stable/_images/math/9315088e89f6fb2f1229e64228d9462e76a2028b.png" alt="\Sigma_k"> of the Gaussians, leading to quadratic decision surfaces. See <a class="footnote-reference" href="#id4" id="id1">[3]</a> for more details.</p> <div class="admonition note"> <p class="first admonition-title">Note</p> <p><strong>Relation with Gaussian Naive Bayes</strong></p> <p class="last">If in the QDA model one assumes that the covariance matrices are diagonal, then the inputs are assumed to be conditionally independent in each class, and the resulting classifier is equivalent to the Gaussian Naive Bayes classifier <a class="reference internal" href="../generated/sklearn.naive_bayes.gaussiannb/#sklearn.naive_bayes.GaussianNB" title="sklearn.naive_bayes.GaussianNB"><code>naive_bayes.GaussianNB</code></a>.</p> </div>   <h2 id="mathematical-formulation-of-lda-dimensionality-reduction">1.2.3. Mathematical formulation of LDA dimensionality reduction</h2> <p>To understand the use of LDA in dimensionality reduction, it is useful to start with a geometric reformulation of the LDA classification rule explained above. We write <img class="math" src="http://scikit-learn.org/stable/_images/math/684381a21cd73ebbf43b63a087d3f7410ee99ce8.png" alt="K"> for the total number of target classes. Since in LDA we assume that all classes have the same estimated covariance <img class="math" src="http://scikit-learn.org/stable/_images/math/7b887ca4d449002abecf59a472644a272dfcb605.png" alt="\Sigma">, we can rescale the data so that this covariance is the identity:</p> <div class="math"> <p><img src="http://scikit-learn.org/stable/_images/math/dde3ad7ccadedbd18f8a2b372c5ab577f0e8bde7.png" alt="X^* = D^{-1/2}U^t X\text{ with }\Sigma = UDU^t"></p> </div>
<p>Then one can show that to classify a data point after scaling is equivalent to finding the estimated class mean <img class="math" src="http://scikit-learn.org/stable/_images/math/7e3b8a648d99cac62d0a9d7b9cefd453f4d08e3c.png" alt="\mu^*_k"> which is closest to the data point in the Euclidean distance. But this can be done just as well after projecting on the <img class="math" src="http://scikit-learn.org/stable/_images/math/d1fb41a058b1d3126362bd00674f573b11212607.png" alt="K-1"> affine subspace <img class="math" src="http://scikit-learn.org/stable/_images/math/6adac8763a5c7475cb646dcebafc7c21479b43ca.png" alt="H_K"> generated by all the <img class="math" src="http://scikit-learn.org/stable/_images/math/7e3b8a648d99cac62d0a9d7b9cefd453f4d08e3c.png" alt="\mu^*_k"> for all classes. This shows that, implicit in the LDA classifier, there is a dimensionality reduction by linear projection onto a <img class="math" src="http://scikit-learn.org/stable/_images/math/d1fb41a058b1d3126362bd00674f573b11212607.png" alt="K-1"> dimensional space.</p> <p>We can reduce the dimension even more, to a chosen <img class="math" src="http://scikit-learn.org/stable/_images/math/ae2b750f71e1fc0daaa3de9a85d42794d7cd1326.png" alt="L">, by projecting onto the linear subspace <img class="math" src="http://scikit-learn.org/stable/_images/math/744d535c7a0c99b544613ffe8f5cf6d4bef39068.png" alt="H_L"> which maximize the variance of the <img class="math" src="http://scikit-learn.org/stable/_images/math/7e3b8a648d99cac62d0a9d7b9cefd453f4d08e3c.png" alt="\mu^*_k"> after projection (in effect, we are doing a form of PCA for the transformed class means <img class="math" src="http://scikit-learn.org/stable/_images/math/7e3b8a648d99cac62d0a9d7b9cefd453f4d08e3c.png" alt="\mu^*_k">). This <img class="math" src="http://scikit-learn.org/stable/_images/math/ae2b750f71e1fc0daaa3de9a85d42794d7cd1326.png" alt="L"> corresponds to the <code>n_components</code> parameter used in the <a class="reference internal" href="../generated/sklearn.discriminant_analysis.lineardiscriminantanalysis/#sklearn.discriminant_analysis.LinearDiscriminantAnalysis.transform" title="sklearn.discriminant_analysis.LinearDiscriminantAnalysis.transform"><code>discriminant_analysis.LinearDiscriminantAnalysis.transform</code></a> method. See <a class="footnote-reference" href="#id4" id="id2">[3]</a> for more details.</p>   <h2 id="shrinkage">1.2.4. Shrinkage</h2> <p>Shrinkage is a tool to improve estimation of covariance matrices in situations where the number of training samples is small compared to the number of features. In this scenario, the empirical sample covariance is a poor estimator. Shrinkage LDA can be used by setting the <code>shrinkage</code> parameter of the <a class="reference internal" href="../generated/sklearn.discriminant_analysis.lineardiscriminantanalysis/#sklearn.discriminant_analysis.LinearDiscriminantAnalysis" title="sklearn.discriminant_analysis.LinearDiscriminantAnalysis"><code>discriminant_analysis.LinearDiscriminantAnalysis</code></a> class to ‘auto’. This automatically determines the optimal shrinkage parameter in an analytic way following the lemma introduced by Ledoit and Wolf <a class="footnote-reference" href="#id5" id="id3">[4]</a>. Note that currently shrinkage only works when setting the <code>solver</code> parameter to ‘lsqr’ or ‘eigen’.</p> <p>The <code>shrinkage</code> parameter can also be manually set between 0 and 1. In particular, a value of 0 corresponds to no shrinkage (which means the empirical covariance matrix will be used) and a value of 1 corresponds to complete shrinkage (which means that the diagonal matrix of variances will be used as an estimate for the covariance matrix). Setting this parameter to a value between these two extrema will estimate a shrunk version of the covariance matrix.</p> <p class="centered"> <strong><a class="reference external" href="../../auto_examples/classification/plot_lda/"><img alt="shrinkage" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_lda_0011.png" style="width: 600.0px; height: 450.0px;"></a></strong></p>  <h2 id="estimation-algorithms">1.2.5. Estimation algorithms</h2> <p>The default solver is ‘svd’. It can perform both classification and transform, and it does not rely on the calculation of the covariance matrix. This can be an advantage in situations where the number of features is large. However, the ‘svd’ solver cannot be used with shrinkage.</p> <p>The ‘lsqr’ solver is an efficient algorithm that only works for classification. It supports shrinkage.</p> <p>The ‘eigen’ solver is based on the optimization of the between class scatter to within class scatter ratio. It can be used for both classification and transform, and it supports shrinkage. However, the ‘eigen’ solver needs to compute the covariance matrix, so it might not be suitable for situations with a high number of features.</p> <div class="topic"> <p class="topic-title first">Examples:</p> <p><a class="reference internal" href="../../auto_examples/classification/plot_lda/#sphx-glr-auto-examples-classification-plot-lda-py"><span class="std std-ref">Normal and Shrinkage Linear Discriminant Analysis for classification</span></a>: Comparison of LDA classifiers with and without shrinkage.</p> </div> <div class="topic"> <p class="topic-title first">References:</p> <table class="docutils footnote" frame="void" id="id4" rules="none">   <tr>
<td class="label">[3]</td>
<td>
<em>(<a class="fn-backref" href="#id1">1</a>, <a class="fn-backref" href="#id2">2</a>)</em> “The Elements of Statistical Learning”, Hastie T., Tibshirani R., Friedman J., Section 4.3, p.106-119, 2008.</td>
</tr>  </table> <table class="docutils footnote" frame="void" id="id5" rules="none">   <tr>
<td class="label"><a class="fn-backref" href="#id3">[4]</a></td>
<td>Ledoit O, Wolf M. Honey, I Shrunk the Sample Covariance Matrix. The Journal of Portfolio Management 30(4), 110-119, 2004.</td>
</tr>  </table> </div>
<div class="_attribution">
  <p class="_attribution-p">
    © 2007–2016 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="http://scikit-learn.org/stable/modules/lda_qda.html" class="_attribution-link" target="_blank">http://scikit-learn.org/stable/modules/lda_qda.html</a>
  </p>
</div>

			</div>
		</div>
	</section>

	</div>
</body>
</html>
