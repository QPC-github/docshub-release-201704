
<!DOCTYPE HTML>

<html lang="en">

<head>
  <meta charset="utf-8">
  <title>2.4. Biclustering - Scikit-learn - W3cubDocs</title>
  
  <meta name="description" content="Biclustering can be performed with the module sklearn.cluster.bicluster. Biclustering algorithms simultaneously cluster rows and columns of a data &hellip;">
  <meta name="keywords" content="biclustering, -, scikit-learn, scikit_learn">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="http://docs.w3cub.com/scikit_learn/modules/biclustering/">
  <link href="/favicon.png" rel="icon">
  <link type="text/css" rel="stylesheet" href="/assets/application-50364fff564ce3b6327021805f3f00e2957b441cf27f576a7dd4ff63bbc47047.css">
  <script type="text/javascript" src="/assets/application-db64bfd54ceb42be11af7995804cf4902548419ceb79d509b0b7d62c22d98e6f.js"></script>
  <script src="/json/scikit_learn.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body>
	<div class="_app">
	<header class="_header">
  
  <form class="_search">
    <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
    <a class="_search-clear"></a>
    <div class="_search-tag"></div>
  </form>
  
  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/scikit_learn/" class="_nav-link" title="" style="margin-left:0;">scikit-learn</a></span>
  
  <nav class="_nav">
    <a href="/app/" class="_nav-link ">App</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		<div class="_list">
			
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<div class="_page _sphinx">
				
<h1 id="id1">2.4. Biclustering</h1> <p id="biclustering">Biclustering can be performed with the module <a class="reference internal" href="../classes/#module-sklearn.cluster.bicluster" title="sklearn.cluster.bicluster"><code>sklearn.cluster.bicluster</code></a>. Biclustering algorithms simultaneously cluster rows and columns of a data matrix. These clusters of rows and columns are known as biclusters. Each determines a submatrix of the original data matrix with some desired properties.</p> <p>For instance, given a matrix of shape <code>(10, 10)</code>, one possible bicluster with three rows and two columns induces a submatrix of shape <code>(3, 2)</code>:</p> <pre data-language="python">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; data = np.arange(100).reshape(10, 10)
&gt;&gt;&gt; rows = np.array([0, 2, 3])[:, np.newaxis]
&gt;&gt;&gt; columns = np.array([1, 2])
&gt;&gt;&gt; data[rows, columns]
array([[ 1,  2],
       [21, 22],
       [31, 32]])
</pre> <p>For visualization purposes, given a bicluster, the rows and columns of the data matrix may be rearranged to make the bicluster contiguous.</p> <p>Algorithms differ in how they define biclusters. Some of the common types include:</p> <ul class="simple"> <li>constant values, constant rows, or constant columns</li> <li>unusually high or low values</li> <li>submatrices with low variance</li> <li>correlated rows or columns</li> </ul> <p>Algorithms also differ in how rows and columns may be assigned to biclusters, which leads to different bicluster structures. Block diagonal or checkerboard structures occur when rows and columns are divided into partitions.</p> <p>If each row and each column belongs to exactly one bicluster, then rearranging the rows and columns of the data matrix reveals the biclusters on the diagonal. Here is an example of this structure where biclusters have higher average values than the other rows and columns:</p> <div class="figure align-center" id="id5"> <a class="reference external image-reference" href="../../auto_examples/bicluster/images/sphx_glr_plot_spectral_coclustering_003.png/"><img alt="../_images/sphx_glr_plot_spectral_coclustering_0031.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_spectral_coclustering_0031.png" style="width: 300.0px; height: 300.0px;"></a> <p class="caption"><span class="caption-text">An example of biclusters formed by partitioning rows and columns.</span></p> </div> <p>In the checkerboard case, each row belongs to all column clusters, and each column belongs to all row clusters. Here is an example of this structure where the variance of the values within each bicluster is small:</p> <div class="figure align-center" id="id6"> <a class="reference external image-reference" href="../../auto_examples/bicluster/images/sphx_glr_plot_spectral_biclustering_003.png/"><img alt="../_images/sphx_glr_plot_spectral_biclustering_0031.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_spectral_biclustering_0031.png" style="width: 300.0px; height: 300.0px;"></a> <p class="caption"><span class="caption-text">An example of checkerboard biclusters.</span></p> </div> <p>After fitting a model, row and column cluster membership can be found in the <code>rows_</code> and <code>columns_</code> attributes. <code>rows_[i]</code> is a binary vector with nonzero entries corresponding to rows that belong to bicluster <code>i</code>. Similarly, <code>columns_[i]</code> indicates which columns belong to bicluster <code>i</code>.</p> <p>Some models also have <code>row_labels_</code> and <code>column_labels_</code> attributes. These models partition the rows and columns, such as in the block diagonal and checkerboard bicluster structures.</p> <div class="admonition note"> <p class="first admonition-title">Note</p> <p class="last">Biclustering has many other names in different fields including co-clustering, two-mode clustering, two-way clustering, block clustering, coupled two-way clustering, etc. The names of some algorithms, such as the Spectral Co-Clustering algorithm, reflect these alternate names.</p> </div>  <h2 id="spectral-coclustering">2.4.1. Spectral Co-Clustering</h2> <p id="spectral-co-clustering">The <a class="reference internal" href="../generated/sklearn.cluster.bicluster.spectralcoclustering/#sklearn.cluster.bicluster.SpectralCoclustering" title="sklearn.cluster.bicluster.SpectralCoclustering"><code>SpectralCoclustering</code></a> algorithm finds biclusters with values higher than those in the corresponding other rows and columns. Each row and each column belongs to exactly one bicluster, so rearranging the rows and columns to make partitions contiguous reveals these high values along the diagonal:</p> <div class="admonition note"> <p class="first admonition-title">Note</p> <p class="last">The algorithm treats the input data matrix as a bipartite graph: the rows and columns of the matrix correspond to the two sets of vertices, and each entry corresponds to an edge between a row and a column. The algorithm approximates the normalized cut of this graph to find heavy subgraphs.</p> </div>  <h3 id="mathematical-formulation">2.4.1.1. Mathematical formulation</h3> <p>An approximate solution to the optimal normalized cut may be found via the generalized eigenvalue decomposition of the Laplacian of the graph. Usually this would mean working directly with the Laplacian matrix. If the original data matrix <img class="math" src="http://scikit-learn.org/stable/_images/math/0043fe6507e9b1d112e07a2801e24927e267dd50.png" alt="A"> has shape <img class="math" src="http://scikit-learn.org/stable/_images/math/a4aa98eb66dde627d2ff991463b1548b747fc221.png" alt="m
\times n">, the Laplacian matrix for the corresponding bipartite graph has shape <img class="math" src="http://scikit-learn.org/stable/_images/math/db0c11cdac309143cbe7ed42ce3601c670a38ab4.png" alt="(m + n) \times (m + n)">. However, in this case it is possible to work directly with <img class="math" src="http://scikit-learn.org/stable/_images/math/0043fe6507e9b1d112e07a2801e24927e267dd50.png" alt="A">, which is smaller and more efficient.</p> <p>The input matrix <img class="math" src="http://scikit-learn.org/stable/_images/math/0043fe6507e9b1d112e07a2801e24927e267dd50.png" alt="A"> is preprocessed as follows:</p> <div class="math"> <p><img src="http://scikit-learn.org/stable/_images/math/a7c8a7222f104393fe9778e796d8c429b0c22c32.png" alt="A_n = R^{-1/2} A C^{-1/2}"></p> </div>
<p>Where <img class="math" src="http://scikit-learn.org/stable/_images/math/a00254b18ffa992f0ef19f6e6e095b83c8f85e94.png" alt="R"> is the diagonal matrix with entry <img class="math" src="http://scikit-learn.org/stable/_images/math/df0deb143e5ac127f00bd248ee8001ecae572adc.png" alt="i"> equal to <img class="math" src="http://scikit-learn.org/stable/_images/math/0cff13082b1c310f63732c327e38e0a5bab69dc6.png" alt="\sum_{j} A_{ij}"> and <img class="math" src="http://scikit-learn.org/stable/_images/math/afce44aa7c55836ca9345404c22fc7b599d2ed84.png" alt="C"> is the diagonal matrix with entry <img class="math" src="http://scikit-learn.org/stable/_images/math/6b21e0b0899a0d2879d3b8019087fa630bab4ea2.png" alt="j"> equal to <img class="math" src="http://scikit-learn.org/stable/_images/math/2a9f6f13d20d5cbad5cd1562d900ff3dad9617b5.png" alt="\sum_{i} A_{ij}">.</p> <p>The singular value decomposition, <img class="math" src="http://scikit-learn.org/stable/_images/math/9307ab783e3fbb06db6a9c92d9d8c72e273cdaf8.png" alt="A_n = U \Sigma V^\top">, provides the partitions of the rows and columns of <img class="math" src="http://scikit-learn.org/stable/_images/math/0043fe6507e9b1d112e07a2801e24927e267dd50.png" alt="A">. A subset of the left singular vectors gives the row partitions, and a subset of the right singular vectors gives the column partitions.</p> <p>The <img class="math" src="http://scikit-learn.org/stable/_images/math/84deb2b5c91775b5b9760f47a4284d4b40093461.png" alt="\ell = \lceil \log_2 k \rceil"> singular vectors, starting from the second, provide the desired partitioning information. They are used to form the matrix <img class="math" src="http://scikit-learn.org/stable/_images/math/bcb2457ac9d8995a4f34d57cadac7ecbbe58f3bd.png" alt="Z">:</p> <div class="math"> <p><img src="http://scikit-learn.org/stable/_images/math/136c762c8efedf19486f6e7e8acf2e4572bc3398.png" alt="Z = \begin{bmatrix} R^{-1/2} U \\\\
                    C^{-1/2} V
      \end{bmatrix}"></p> </div>
<p>where the columns of <img class="math" src="http://scikit-learn.org/stable/_images/math/169c982b5e649cd1bd6445df5583affe427ea269.png" alt="U"> are <img class="math" src="http://scikit-learn.org/stable/_images/math/406c5eea588bda09a3d9dd45af5183772c72d876.png" alt="u_2, \dots, u_{\ell +
1}">, and similarly for <img class="math" src="http://scikit-learn.org/stable/_images/math/fae0e7a73748991e5540d874416000583f64f58e.png" alt="V">.</p> <p>Then the rows of <img class="math" src="http://scikit-learn.org/stable/_images/math/bcb2457ac9d8995a4f34d57cadac7ecbbe58f3bd.png" alt="Z"> are clustered using <a class="reference internal" href="../clustering/#k-means"><span class="std std-ref">k-means</span></a>. The first <code>n_rows</code> labels provide the row partitioning, and the remaining <code>n_columns</code> labels provide the column partitioning.</p> <div class="topic"> <p class="topic-title first">Examples:</p> <ul class="simple"> <li>
<a class="reference internal" href="../../auto_examples/bicluster/plot_spectral_coclustering/#sphx-glr-auto-examples-bicluster-plot-spectral-coclustering-py"><span class="std std-ref">A demo of the Spectral Co-Clustering algorithm</span></a>: A simple example showing how to generate a data matrix with biclusters and apply this method to it.</li> <li>
<a class="reference internal" href="../../auto_examples/bicluster/bicluster_newsgroups/#sphx-glr-auto-examples-bicluster-bicluster-newsgroups-py"><span class="std std-ref">Biclustering documents with the Spectral Co-clustering algorithm</span></a>: An example of finding biclusters in the twenty newsgroup dataset.</li> </ul> </div> <div class="topic"> <p class="topic-title first">References:</p> <ul class="simple"> <li>Dhillon, Inderjit S, 2001. <a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.140.3011" target="_blank">Co-clustering documents and words using bipartite spectral graph partitioning</a>.</li> </ul> </div>    <h2 id="id2">2.4.2. Spectral Biclustering</h2> <p id="spectral-biclustering">The <a class="reference internal" href="../generated/sklearn.cluster.bicluster.spectralbiclustering/#sklearn.cluster.bicluster.SpectralBiclustering" title="sklearn.cluster.bicluster.SpectralBiclustering"><code>SpectralBiclustering</code></a> algorithm assumes that the input data matrix has a hidden checkerboard structure. The rows and columns of a matrix with this structure may be partitioned so that the entries of any bicluster in the Cartesian product of row clusters and column clusters are approximately constant. For instance, if there are two row partitions and three column partitions, each row will belong to three biclusters, and each column will belong to two biclusters.</p> <p>The algorithm partitions the rows and columns of a matrix so that a corresponding blockwise-constant checkerboard matrix provides a good approximation to the original matrix.</p>  <h3 id="id3">2.4.2.1. Mathematical formulation</h3> <p>The input matrix <img class="math" src="http://scikit-learn.org/stable/_images/math/0043fe6507e9b1d112e07a2801e24927e267dd50.png" alt="A"> is first normalized to make the checkerboard pattern more obvious. There are three possible methods:</p> <ol class="arabic simple"> <li>
<em>Independent row and column normalization</em>, as in Spectral Co-Clustering. This method makes the rows sum to a constant and the columns sum to a different constant.</li> <li>
<strong>Bistochastization</strong>: repeated row and column normalization until convergence. This method makes both rows and columns sum to the same constant.</li> <li>
<strong>Log normalization</strong>: the log of the data matrix is computed: <img class="math" src="http://scikit-learn.org/stable/_images/math/f2c2a8ea3b216bb8693ecb2007fdf40bbec05eb0.png" alt="L =
\log A">. Then the column mean <img class="math" src="http://scikit-learn.org/stable/_images/math/13ad8d09b9addbf06f32031a30e00d5446149ed6.png" alt="\overline{L_{i \cdot}}">, row mean <img class="math" src="http://scikit-learn.org/stable/_images/math/d22f8be276504dd5ab9f54042b3bf37fa66971e7.png" alt="\overline{L_{\cdot j}}">, and overall mean <img class="math" src="http://scikit-learn.org/stable/_images/math/25937f902e4dba82f6d70067474c5e69bbeb3711.png" alt="\overline{L_{\cdot
\cdot}}"> of <img class="math" src="http://scikit-learn.org/stable/_images/math/ae2b750f71e1fc0daaa3de9a85d42794d7cd1326.png" alt="L"> are computed. The final matrix is computed according to the formula</li> </ol> <div class="math"> <p><img src="http://scikit-learn.org/stable/_images/math/1a59f73d14f3c7dbf1f1de48de775800f90d88e5.png" alt="K_{ij} = L_{ij} - \overline{L_{i \cdot}} - \overline{L_{\cdot
j}} + \overline{L_{\cdot \cdot}}"></p> </div>
<p>After normalizing, the first few singular vectors are computed, just as in the Spectral Co-Clustering algorithm.</p> <p>If log normalization was used, all the singular vectors are meaningful. However, if independent normalization or bistochastization were used, the first singular vectors, <img class="math" src="http://scikit-learn.org/stable/_images/math/cf40a917043ed52a5b1c5bc60daf0128a1b47f64.png" alt="u_1"> and <img class="math" src="http://scikit-learn.org/stable/_images/math/8fecd40ca8eb7e61734850b21956deba20be3066.png" alt="v_1">. are discarded. From now on, the “first” singular vectors refers to <img class="math" src="http://scikit-learn.org/stable/_images/math/abc59fbd07c450fde332de0de33e768b132143d3.png" alt="u_2 \dots u_{p+1}"> and <img class="math" src="http://scikit-learn.org/stable/_images/math/fb6d1f67e52083c030936b2f244850d5193d9b26.png" alt="v_2 \dots v_{p+1}"> except in the case of log normalization.</p> <p>Given these singular vectors, they are ranked according to which can be best approximated by a piecewise-constant vector. The approximations for each vector are found using one-dimensional k-means and scored using the Euclidean distance. Some subset of the best left and right singular vector are selected. Next, the data is projected to this best subset of singular vectors and clustered.</p> <p>For instance, if <img class="math" src="http://scikit-learn.org/stable/_images/math/27d463da4622be5b3ef1d4176ced7d7a323c6425.png" alt="p"> singular vectors were calculated, the <img class="math" src="http://scikit-learn.org/stable/_images/math/620a3ce6403ec82f1347af9985bc03f7a9382f4a.png" alt="q"> best are found as described, where <img class="math" src="http://scikit-learn.org/stable/_images/math/1bc98d57cbfe3d7dc1419afe1527ea0771ce74bd.png" alt="q&lt;p">. Let <img class="math" src="http://scikit-learn.org/stable/_images/math/169c982b5e649cd1bd6445df5583affe427ea269.png" alt="U"> be the matrix with columns the <img class="math" src="http://scikit-learn.org/stable/_images/math/620a3ce6403ec82f1347af9985bc03f7a9382f4a.png" alt="q"> best left singular vectors, and similarly <img class="math" src="http://scikit-learn.org/stable/_images/math/fae0e7a73748991e5540d874416000583f64f58e.png" alt="V"> for the right. To partition the rows, the rows of <img class="math" src="http://scikit-learn.org/stable/_images/math/0043fe6507e9b1d112e07a2801e24927e267dd50.png" alt="A"> are projected to a <img class="math" src="http://scikit-learn.org/stable/_images/math/620a3ce6403ec82f1347af9985bc03f7a9382f4a.png" alt="q"> dimensional space: <img class="math" src="http://scikit-learn.org/stable/_images/math/b6f0db892a2f7ed877b3a83674ee1932a16340cb.png" alt="A * V">. Treating the <img class="math" src="http://scikit-learn.org/stable/_images/math/edba97b4c0d864d26e92ea7ea73767fa38eef3f7.png" alt="m"> rows of this <img class="math" src="http://scikit-learn.org/stable/_images/math/72ed3a9ec9bd6e7c124620e1aac526277847d074.png" alt="m \times q"> matrix as samples and clustering using k-means yields the row labels. Similarly, projecting the columns to <img class="math" src="http://scikit-learn.org/stable/_images/math/a3091a5335fd5b9fc73ede24de6b277d853161c5.png" alt="A^{\top} * U"> and clustering this <img class="math" src="http://scikit-learn.org/stable/_images/math/5e9d7e95b083ffb7b8cb72b0a16199c4ef63cef3.png" alt="n \times q"> matrix yields the column labels.</p> <div class="topic"> <p class="topic-title first">Examples:</p> <ul class="simple"> <li>
<a class="reference internal" href="../../auto_examples/bicluster/plot_spectral_biclustering/#sphx-glr-auto-examples-bicluster-plot-spectral-biclustering-py"><span class="std std-ref">A demo of the Spectral Biclustering algorithm</span></a>: a simple example showing how to generate a checkerboard matrix and bicluster it.</li> </ul> </div> <div class="topic"> <p class="topic-title first">References:</p> <ul class="simple"> <li>Kluger, Yuval, et. al., 2003. <a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.135.1608" target="_blank">Spectral biclustering of microarray data: coclustering genes and conditions</a>.</li> </ul> </div>    <h2 id="id4">2.4.3. Biclustering evaluation</h2> <p id="biclustering-evaluation">There are two ways of evaluating a biclustering result: internal and external. Internal measures, such as cluster stability, rely only on the data and the result themselves. Currently there are no internal bicluster measures in scikit-learn. External measures refer to an external source of information, such as the true solution. When working with real data the true solution is usually unknown, but biclustering artificial data may be useful for evaluating algorithms precisely because the true solution is known.</p> <p>To compare a set of found biclusters to the set of true biclusters, two similarity measures are needed: a similarity measure for individual biclusters, and a way to combine these individual similarities into an overall score.</p> <p>To compare individual biclusters, several measures have been used. For now, only the Jaccard index is implemented:</p> <div class="math"> <p><img src="http://scikit-learn.org/stable/_images/math/380ca2d20a08e36aecb173790e00d05b274603ce.png" alt="J(A, B) = \frac{|A \cap B|}{|A| + |B| - |A \cap B|}"></p> </div>
<p>where <img class="math" src="http://scikit-learn.org/stable/_images/math/0043fe6507e9b1d112e07a2801e24927e267dd50.png" alt="A"> and <img class="math" src="http://scikit-learn.org/stable/_images/math/9805f44feec6f81d376d09e88b8236635edbb3c8.png" alt="B"> are biclusters, <img class="math" src="http://scikit-learn.org/stable/_images/math/2224fea2d639f17ad5a333cea91e3e2e61f6d852.png" alt="|A \cap B|"> is the number of elements in their intersection. The Jaccard index achieves its minimum of 0 when the biclusters to not overlap at all and its maximum of 1 when they are identical.</p> <p>Several methods have been developed to compare two sets of biclusters. For now, only <a class="reference internal" href="../generated/sklearn.metrics.consensus_score/#sklearn.metrics.consensus_score" title="sklearn.metrics.consensus_score"><code>consensus_score</code></a> (Hochreiter et. al., 2010) is available:</p> <ol class="arabic simple"> <li>Compute bicluster similarities for pairs of biclusters, one in each set, using the Jaccard index or a similar measure.</li> <li>Assign biclusters from one set to another in a one-to-one fashion to maximize the sum of their similarities. This step is performed using the Hungarian algorithm.</li> <li>The final sum of similarities is divided by the size of the larger set.</li> </ol> <p>The minimum consensus score, 0, occurs when all pairs of biclusters are totally dissimilar. The maximum score, 1, occurs when both sets are identical.</p> <div class="topic"> <p class="topic-title first">References:</p> <ul class="simple"> <li>Hochreiter, Bodenhofer, et. al., 2010. <a class="reference external" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2881408/" target="_blank">FABIA: factor analysis for bicluster acquisition</a>.</li> </ul> </div>
<div class="_attribution">
  <p class="_attribution-p">
    © 2007–2016 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="http://scikit-learn.org/stable/modules/biclustering.html" class="_attribution-link" target="_blank">http://scikit-learn.org/stable/modules/biclustering.html</a>
  </p>
</div>

			</div>
		</div>
	</section>

	</div>
</body>
</html>
