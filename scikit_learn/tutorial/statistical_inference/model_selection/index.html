
<!DOCTYPE HTML>

<html lang="en">

<head>
  <meta charset="utf-8">
  <title>Tutorial&#58; Model Selection - Scikit-learn - W3cubDocs</title>
  
  <meta name="description" content=" As we have seen, every estimator exposes a score method that can judge the quality of the fit (or the prediction) on new data. Bigger is better. ">
  <meta name="keywords" content="model, selection, choosing, estimators, and, their, parameters, tutorial, -, scikit-learn, scikit_learn">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="http://docs.w3cub.com/scikit_learn/tutorial/statistical_inference/model_selection/">
  <link href="/favicon.png" rel="icon">
  <link type="text/css" rel="stylesheet" href="/assets/application-50364fff564ce3b6327021805f3f00e2957b441cf27f576a7dd4ff63bbc47047.css">
  <script type="text/javascript" src="/assets/application-db64bfd54ceb42be11af7995804cf4902548419ceb79d509b0b7d62c22d98e6f.js"></script>
  <script src="/json/scikit_learn.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body>
	<div class="_app">
	<header class="_header">
  
  <form class="_search">
    <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
    <a class="_search-clear"></a>
    <div class="_search-tag"></div>
  </form>
  
  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/scikit_learn/" class="_nav-link" title="" style="margin-left:0;">scikit-learn</a></span>
  
  <nav class="_nav">
    <a href="/app/" class="_nav-link ">App</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		<div class="_list">
			
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<div class="_page _sphinx">
				
<h1 id="model-selection-tut">Model selection: choosing estimators and their parameters</h1>  <h2 id="model-selection-choosing-estimators-and-their-parameters">Score, and cross-validated scores</h2> <p>As we have seen, every estimator exposes a <code>score</code> method that can judge the quality of the fit (or the prediction) on new data. <strong>Bigger is better</strong>.</p> <pre data-language="python">&gt;&gt;&gt; from sklearn import datasets, svm
&gt;&gt;&gt; digits = datasets.load_digits()
&gt;&gt;&gt; X_digits = digits.data
&gt;&gt;&gt; y_digits = digits.target
&gt;&gt;&gt; svc = svm.SVC(C=1, kernel='linear')
&gt;&gt;&gt; svc.fit(X_digits[:-100], y_digits[:-100]).score(X_digits[-100:], y_digits[-100:])
0.97999999999999998
</pre> <p>To get a better measure of prediction accuracy (which we can use as a proxy for goodness of fit of the model), we can successively split the data in <em>folds</em> that we use for training and testing:</p> <pre data-language="python">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; X_folds = np.array_split(X_digits, 3)
&gt;&gt;&gt; y_folds = np.array_split(y_digits, 3)
&gt;&gt;&gt; scores = list()
&gt;&gt;&gt; for k in range(3):
...     # We use 'list' to copy, in order to 'pop' later on
...     X_train = list(X_folds)
...     X_test  = X_train.pop(k)
...     X_train = np.concatenate(X_train)
...     y_train = list(y_folds)
...     y_test  = y_train.pop(k)
...     y_train = np.concatenate(y_train)
...     scores.append(svc.fit(X_train, y_train).score(X_test, y_test))
&gt;&gt;&gt; print(scores)
[0.93489148580968284, 0.95659432387312182, 0.93989983305509184]
</pre> <p>This is called a <a class="reference internal" href="../../../modules/generated/sklearn.model_selection.kfold/#sklearn.model_selection.KFold" title="sklearn.model_selection.KFold"><code>KFold</code></a> cross-validation.</p>   <h2 id="cv-generators-tut">Cross-validation generators</h2> <p id="cross-validation-generators">Scikit-learn has a collection of classes which can be used to generate lists of train/test indices for popular cross-validation strategies.</p> <p>They expose a <code>split</code> method which accepts the input dataset to be split and yields the train/test set indices for each iteration of the chosen cross-validation strategy.</p> <p>This example shows an example usage of the <code>split</code> method.</p> <pre data-language="python">&gt;&gt;&gt; from sklearn.model_selection import KFold, cross_val_score
&gt;&gt;&gt; X = ["a", "a", "b", "c", "c", "c"]
&gt;&gt;&gt; k_fold = KFold(n_splits=3)
&gt;&gt;&gt; for train_indices, test_indices in k_fold.split(X):
...      print('Train: %s | test: %s' % (train_indices, test_indices))
Train: [2 3 4 5] | test: [0 1]
Train: [0 1 4 5] | test: [2 3]
Train: [0 1 2 3] | test: [4 5]
</pre> <p>The cross-validation can then be performed easily:</p> <pre data-language="python">&gt;&gt;&gt; kfold = KFold(n_splits=3)
&gt;&gt;&gt; [svc.fit(X_digits[train], y_digits[train]).score(X_digits[test], y_digits[test])
...          for train, test in k_fold.split(X_digits)]
[0.93489148580968284, 0.95659432387312182, 0.93989983305509184]
</pre> <p>The cross-validation score can be directly calculated using the <a class="reference internal" href="../../../modules/generated/sklearn.model_selection.cross_val_score/#sklearn.model_selection.cross_val_score" title="sklearn.model_selection.cross_val_score"><code>cross_val_score</code></a> helper. Given an estimator, the cross-validation object and the input dataset, the <a class="reference internal" href="../../../modules/generated/sklearn.model_selection.cross_val_score/#sklearn.model_selection.cross_val_score" title="sklearn.model_selection.cross_val_score"><code>cross_val_score</code></a> splits the data repeatedly into a training and a testing set, trains the estimator using the training set and computes the scores based on the testing set for each iteration of cross-validation.</p> <p>By default the estimatorâ€™s <code>score</code> method is used to compute the individual scores.</p> <p>Refer the <a class="reference internal" href="../../../modules/metrics/#metrics"><span class="std std-ref">metrics module</span></a> to learn more on the available scoring methods.</p> <pre data-language="python">&gt;&gt;&gt; cross_val_score(svc, X_digits, y_digits, cv=k_fold, n_jobs=-1)
array([ 0.93489149,  0.95659432,  0.93989983])
</pre> <p><code>n_jobs=-1</code> means that the computation will be dispatched on all the CPUs of the computer.</p> <p>Alternatively, the <code>scoring</code> argument can be provided to specify an alternative scoring method.</p>  <pre data-language="python">&gt;&gt;&gt; cross_val_score(svc, X_digits, y_digits, cv=k_fold,
...                 scoring='precision_macro')
array([ 0.93969761,  0.95911415,  0.94041254])
</pre> <p><strong>Cross-validation generators</strong></p>  <table class="docutils">   <tr class="row-odd">
<td>
<a class="reference internal" href="../../../modules/generated/sklearn.model_selection.kfold/#sklearn.model_selection.KFold" title="sklearn.model_selection.KFold"><code>KFold</code></a> <strong>(n_splits, shuffle, random_state)</strong>
</td> <td>
<a class="reference internal" href="../../../modules/generated/sklearn.model_selection.stratifiedkfold/#sklearn.model_selection.StratifiedKFold" title="sklearn.model_selection.StratifiedKFold"><code>StratifiedKFold</code></a> <strong>(n_iter, test_size, train_size, random_state)</strong>
</td> <td>
<a class="reference internal" href="../../../modules/generated/sklearn.model_selection.groupkfold/#sklearn.model_selection.GroupKFold" title="sklearn.model_selection.GroupKFold"><code>GroupKFold</code></a> <strong>(n_splits, shuffle, random_state)</strong>
</td> </tr> <tr class="row-even">
<td>Splits it into K folds, trains on K-1 and then tests on the left-out.</td> <td>Same as K-Fold but preserves the class distribution within each fold.</td> <td>Ensures that the same group is not in both testing and training sets.</td> </tr>  </table> <table class="docutils">   <tr class="row-odd">
<td>
<a class="reference internal" href="../../../modules/generated/sklearn.model_selection.shufflesplit/#sklearn.model_selection.ShuffleSplit" title="sklearn.model_selection.ShuffleSplit"><code>ShuffleSplit</code></a> <strong>(n_iter, test_size, train_size, random_state)</strong>
</td> <td><a class="reference internal" href="../../../modules/generated/sklearn.model_selection.stratifiedshufflesplit/#sklearn.model_selection.StratifiedShuffleSplit" title="sklearn.model_selection.StratifiedShuffleSplit"><code>StratifiedShuffleSplit</code></a></td> <td><a class="reference internal" href="../../../modules/generated/sklearn.model_selection.groupshufflesplit/#sklearn.model_selection.GroupShuffleSplit" title="sklearn.model_selection.GroupShuffleSplit"><code>GroupShuffleSplit</code></a></td> </tr> <tr class="row-even">
<td>Generates train/test indices based on random permutation.</td> <td>Same as shuffle split but preserves the class distribution within each iteration.</td> <td>Ensures that the same group is not in both testing and training sets.</td> </tr>  </table> <table class="docutils">   <tr class="row-odd">
<td>
<a class="reference internal" href="../../../modules/generated/sklearn.model_selection.leaveonegroupout/#sklearn.model_selection.LeaveOneGroupOut" title="sklearn.model_selection.LeaveOneGroupOut"><code>LeaveOneGroupOut</code></a> <strong>()</strong>
</td> <td>
<a class="reference internal" href="../../../modules/generated/sklearn.model_selection.leavepgroupsout/#sklearn.model_selection.LeavePGroupsOut" title="sklearn.model_selection.LeavePGroupsOut"><code>LeavePGroupsOut</code></a> <strong>(p)</strong>
</td> <td>
<a class="reference internal" href="../../../modules/generated/sklearn.model_selection.leaveoneout/#sklearn.model_selection.LeaveOneOut" title="sklearn.model_selection.LeaveOneOut"><code>LeaveOneOut</code></a> <strong>()</strong>
</td> </tr> <tr class="row-even">
<td>Takes a group array to group observations.</td> <td>Leave P groups out.</td> <td>Leave one observation out.</td> </tr>  </table> <table class="docutils">   <tr class="row-odd">
<td>
<a class="reference internal" href="../../../modules/generated/sklearn.model_selection.leavepout/#sklearn.model_selection.LeavePOut" title="sklearn.model_selection.LeavePOut"><code>LeavePOut</code></a> <strong>(p)</strong>
</td> <td><a class="reference internal" href="../../../modules/generated/sklearn.model_selection.predefinedsplit/#sklearn.model_selection.PredefinedSplit" title="sklearn.model_selection.PredefinedSplit"><code>PredefinedSplit</code></a></td> </tr> <tr class="row-even">
<td>Leave P observations out.</td> <td>Generates train/test indices based on predefined splits.</td> </tr>  </table> <div class="green topic"> <p class="topic-title first"><strong>Exercise</strong></p> <a class="reference external image-reference" href="../../../auto_examples/exercises/plot_cv_digits/"><img alt="../../_images/sphx_glr_plot_cv_digits_001.png" class="align-right" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_cv_digits_001.png" style="width: 360.0px; height: 270.0px;"></a> <p>On the digits dataset, plot the cross-validation score of a <a class="reference internal" href="../../../modules/generated/sklearn.svm.svc/#sklearn.svm.SVC" title="sklearn.svm.SVC"><code>SVC</code></a> estimator with an linear kernel as a function of parameter <code>C</code> (use a logarithmic grid of points, from 1 to 10).</p> <pre data-language="python">
import numpy as np
from sklearn.model_selection import cross_val_score
from sklearn import datasets, svm

digits = datasets.load_digits()
X = digits.data
y = digits.target

svc = svm.SVC(kernel='linear')
C_s = np.logspace(-10, 0, 10)
</pre> <p><strong>Solution:</strong> <a class="reference internal" href="../../../auto_examples/exercises/plot_cv_digits/#sphx-glr-auto-examples-exercises-plot-cv-digits-py"><span class="std std-ref">Cross-validation on Digits Dataset Exercise</span></a></p> </div>   <h2 id="grid-search-and-cross-validated-estimators">Grid-search and cross-validated estimators</h2>  <h3 id="grid-search">Grid-search</h3> <p>scikit-learn provides an object that, given data, computes the score during the fit of an estimator on a parameter grid and chooses the parameters to maximize the cross-validation score. This object takes an estimator during the construction and exposes an estimator API:</p> <pre data-language="python">&gt;&gt;&gt; from sklearn.model_selection import GridSearchCV, cross_val_score
&gt;&gt;&gt; Cs = np.logspace(-6, -1, 10)
&gt;&gt;&gt; clf = GridSearchCV(estimator=svc, param_grid=dict(C=Cs),
...                    n_jobs=-1)
&gt;&gt;&gt; clf.fit(X_digits[:1000], y_digits[:1000])        
GridSearchCV(cv=None,...
&gt;&gt;&gt; clf.best_score_                                  
0.925...
&gt;&gt;&gt; clf.best_estimator_.C                            
0.0077...

&gt;&gt;&gt; # Prediction performance on test set is not as good as on train set
&gt;&gt;&gt; clf.score(X_digits[1000:], y_digits[1000:])      
0.943...
</pre> <p>By default, the <a class="reference internal" href="../../../modules/generated/sklearn.model_selection.gridsearchcv/#sklearn.model_selection.GridSearchCV" title="sklearn.model_selection.GridSearchCV"><code>GridSearchCV</code></a> uses a 3-fold cross-validation. However, if it detects that a classifier is passed, rather than a regressor, it uses a stratified 3-fold.</p> <div class="topic"> <p class="topic-title first">Nested cross-validation</p> <pre data-language="python">&gt;&gt;&gt; cross_val_score(clf, X_digits, y_digits)
...                                               
array([ 0.938...,  0.963...,  0.944...])
</pre> <p>Two cross-validation loops are performed in parallel: one by the <a class="reference internal" href="../../../modules/generated/sklearn.model_selection.gridsearchcv/#sklearn.model_selection.GridSearchCV" title="sklearn.model_selection.GridSearchCV"><code>GridSearchCV</code></a> estimator to set <code>gamma</code> and the other one by <code>cross_val_score</code> to measure the prediction performance of the estimator. The resulting scores are unbiased estimates of the prediction score on new data.</p> </div> <div class="admonition warning"> <p class="first admonition-title">Warning</p> <p class="last">You cannot nest objects with parallel computing (<code>n_jobs</code> different than 1).</p> </div>   <h3 id="cv-estimators-tut">Cross-validated estimators</h3> <p id="cross-validated-estimators">Cross-validation to set a parameter can be done more efficiently on an algorithm-by-algorithm basis. This is why, for certain estimators, scikit-learn exposes <a class="reference internal" href="../../../modules/cross_validation/#cross-validation"><span class="std std-ref">Cross-validation: evaluating estimator performance</span></a> estimators that set their parameter automatically by cross-validation:</p> <pre data-language="python">&gt;&gt;&gt; from sklearn import linear_model, datasets
&gt;&gt;&gt; lasso = linear_model.LassoCV()
&gt;&gt;&gt; diabetes = datasets.load_diabetes()
&gt;&gt;&gt; X_diabetes = diabetes.data
&gt;&gt;&gt; y_diabetes = diabetes.target
&gt;&gt;&gt; lasso.fit(X_diabetes, y_diabetes)
LassoCV(alphas=None, copy_X=True, cv=None, eps=0.001, fit_intercept=True,
    max_iter=1000, n_alphas=100, n_jobs=1, normalize=False, positive=False,
    precompute='auto', random_state=None, selection='cyclic', tol=0.0001,
    verbose=False)
&gt;&gt;&gt; # The estimator chose automatically its lambda:
&gt;&gt;&gt; lasso.alpha_ 
0.01229...
</pre> <p>These estimators are called similarly to their counterparts, with â€˜CVâ€™ appended to their name.</p> <div class="green topic"> <p class="topic-title first"><strong>Exercise</strong></p> <p>On the diabetes dataset, find the optimal regularization parameter alpha.</p> <p><strong>Bonus</strong>: How much can you trust the selection of alpha?</p> <pre data-language="python">
from sklearn import datasets
from sklearn.linear_model import LassoCV
from sklearn.linear_model import Lasso
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score

diabetes = datasets.load_diabetes()
</pre> <p><strong>Solution:</strong> <a class="reference internal" href="../../../auto_examples/exercises/plot_cv_diabetes/#sphx-glr-auto-examples-exercises-plot-cv-diabetes-py"><span class="std std-ref">Cross-validation on diabetes Dataset Exercise</span></a></p> </div>
<div class="_attribution">
  <p class="_attribution-p">
    Â© 2007â€“2016 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="http://scikit-learn.org/stable/tutorial/statistical_inference/model_selection.html" class="_attribution-link" target="_blank">http://scikit-learn.org/stable/tutorial/statistical_inference/model_selection.html</a>
  </p>
</div>

			</div>
		</div>
	</section>

	</div>
</body>
</html>
