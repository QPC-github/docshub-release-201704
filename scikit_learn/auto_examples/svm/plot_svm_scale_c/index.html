
<!DOCTYPE HTML>

<html lang="en">

<head>
  <meta charset="utf-8">
  <title>Example&#58; Scaling the Regularization Parameter for SVCs - Scikit-learn - W3cubDocs</title>
  
  <meta name="description" content="The following example illustrates the effect of scaling the regularization parameter when using Support Vector Machines for classification. For SVC &hellip;">
  <meta name="keywords" content="scaling, regularization, parameter, for, svcs, example, -, scikit-learn, scikit_learn">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="http://docs.w3cub.com/scikit_learn/auto_examples/svm/plot_svm_scale_c/">
  <link href="/favicon.png" rel="icon">
  <link type="text/css" rel="stylesheet" href="/assets/application-50364fff564ce3b6327021805f3f00e2957b441cf27f576a7dd4ff63bbc47047.css">
  <script type="text/javascript" src="/assets/application-db64bfd54ceb42be11af7995804cf4902548419ceb79d509b0b7d62c22d98e6f.js"></script>
  <script src="/json/scikit_learn.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body>
	<div class="_app">
	<header class="_header">
  
  <form class="_search">
    <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
    <a class="_search-clear"></a>
    <div class="_search-tag"></div>
  </form>
  
  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/scikit_learn/" class="_nav-link" title="" style="margin-left:0;">scikit-learn</a></span>
  
  <nav class="_nav">
    <a href="/app/" class="_nav-link ">App</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		<div class="_list">
			
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<div class="_page _sphinx">
				
<h1 id="sphx-glr-auto-examples-svm-plot-svm-scale-c-py">Scaling the regularization parameter for SVCs</h1> <p id="scaling-the-regularization-parameter-for-svcs">The following example illustrates the effect of scaling the regularization parameter when using <a class="reference internal" href="../../../modules/svm/#svm"><span class="std std-ref">Support Vector Machines</span></a> for <a class="reference internal" href="../../../modules/svm/#svm-classification"><span class="std std-ref">classification</span></a>. For SVC classification, we are interested in a risk minimization for the equation:</p> <div class="math"> <p><img src="http://scikit-learn.org/stable/_images/math/8f8b07a48d22d808a36357e982790f1a34486fed.png" alt="C \sum_{i=1, n} \mathcal{L} (f(x_i), y_i) + \Omega (w)"></p> </div>
<p>where</p>  <ul class="simple"> <li>
<img class="math" src="http://scikit-learn.org/stable/_images/math/afce44aa7c55836ca9345404c22fc7b599d2ed84.png" alt="C"> is used to set the amount of regularization</li> <li>
<img class="math" src="http://scikit-learn.org/stable/_images/math/fbafa98177a4e80f5b20c54216c5fa7ed736f5f0.png" alt="\mathcal{L}"> is a <code>loss</code> function of our samples and our model parameters.</li> <li>
<img class="math" src="http://scikit-learn.org/stable/_images/math/cd30e0b91d2b53728ae09ccac9d857b79b3adfcb.png" alt="\Omega"> is a <code>penalty</code> function of our model parameters</li> </ul>  <p>If we consider the loss function to be the individual error per sample, then the data-fit term, or the sum of the error for each sample, will increase as we add more samples. The penalization term, however, will not increase.</p> <p>When using, for example, <a class="reference internal" href="../../../modules/cross_validation/#cross-validation"><span class="std std-ref">cross validation</span></a>, to set the amount of regularization with <code>C</code>, there will be a different amount of samples between the main problem and the smaller problems within the folds of the cross validation.</p> <p>Since our loss function is dependent on the amount of samples, the latter will influence the selected value of <code>C</code>. The question that arises is <code>How do we optimally adjust C to account for the different amount of training samples?</code></p> <p>The figures below are used to illustrate the effect of scaling our <code>C</code> to compensate for the change in the number of samples, in the case of using an <code>l1</code> penalty, as well as the <code>l2</code> penalty.</p>  <h2 id="l1-penalty-case">l1-penalty case</h2> <p>In the <code>l1</code> case, theory says that prediction consistency (i.e. that under given hypothesis, the estimator learned predicts as well as a model knowing the true distribution) is not possible because of the bias of the <code>l1</code>. It does say, however, that model consistency, in terms of finding the right set of non-zero parameters as well as their signs, can be achieved by scaling <code>C1</code>.</p>   <h2 id="l2-penalty-case">l2-penalty case</h2> <p>The theory says that in order to achieve prediction consistency, the penalty parameter should be kept constant as the number of samples grow.</p>   <h2 id="simulations">Simulations</h2> <p>The two figures below plot the values of <code>C</code> on the <code>x-axis</code> and the corresponding cross-validation scores on the <code>y-axis</code>, for several different fractions of a generated data-set.</p> <p>In the <code>l1</code> penalty case, the cross-validation-error correlates best with the test-error, when scaling our <code>C</code> with the number of samples, <code>n</code>, which can be seen in the first figure.</p> <p>For the <code>l2</code> penalty case, the best result comes from the case where <code>C</code> is not scaled.</p> <div class="topic"> <p class="topic-title first">Note:</p> <p>Two separate datasets are used for the two different plots. The reason behind this is the <code>l1</code> case works better on sparse data, while <code>l2</code> is better suited to the non-sparse case.</p> </div> <ul class="sphx-glr-horizontal"> <li>
<a class="first reference internal image-reference" href="http://scikit-learn.org/stable/_images/sphx_glr_plot_svm_scale_c_000.png" target="_blank"><img alt="../../_images/sphx_glr_plot_svm_scale_c_000.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_svm_scale_c_000.png" style="width: 423.0px; height: 470.0px;"></a> </li> <li>
<a class="first reference internal image-reference" href="http://scikit-learn.org/stable/_images/sphx_glr_plot_svm_scale_c_001.png" target="_blank"><img alt="../../_images/sphx_glr_plot_svm_scale_c_001.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_svm_scale_c_001.png" style="width: 423.0px; height: 470.0px;"></a> </li> </ul> <pre data-language="python">print(__doc__)


# Author: Andreas Mueller &lt;amueller@ais.uni-bonn.de&gt;
#         Jaques Grobler &lt;jaques.grobler@inria.fr&gt;
# License: BSD 3 clause


import numpy as np
import matplotlib.pyplot as plt

from sklearn.svm import LinearSVC
from sklearn.model_selection import ShuffleSplit
from sklearn.model_selection import GridSearchCV
from sklearn.utils import check_random_state
from sklearn import datasets

rnd = check_random_state(1)

# set up dataset
n_samples = 100
n_features = 300

# l1 data (only 5 informative features)
X_1, y_1 = datasets.make_classification(n_samples=n_samples,
                                        n_features=n_features, n_informative=5,
                                        random_state=1)

# l2 data: non sparse, but less features
y_2 = np.sign(.5 - rnd.rand(n_samples))
X_2 = rnd.randn(n_samples, n_features / 5) + y_2[:, np.newaxis]
X_2 += 5 * rnd.randn(n_samples, n_features / 5)

clf_sets = [(LinearSVC(penalty='l1', loss='squared_hinge', dual=False,
                       tol=1e-3),
             np.logspace(-2.3, -1.3, 10), X_1, y_1),
            (LinearSVC(penalty='l2', loss='squared_hinge', dual=True,
                       tol=1e-4),
             np.logspace(-4.5, -2, 10), X_2, y_2)]

colors = ['navy', 'cyan', 'darkorange']
lw = 2

for fignum, (clf, cs, X, y) in enumerate(clf_sets):
    # set up the plot for each regressor
    plt.figure(fignum, figsize=(9, 10))

    for k, train_size in enumerate(np.linspace(0.3, 0.7, 3)[::-1]):
        param_grid = dict(C=cs)
        # To get nice curve, we need a large number of iterations to
        # reduce the variance
        grid = GridSearchCV(clf, refit=False, param_grid=param_grid,
                            cv=ShuffleSplit(train_size=train_size,
                                            n_splits=250, random_state=1))
        grid.fit(X, y)
        scores = grid.cv_results_['mean_test_score']

        scales = [(1, 'No scaling'),
                  ((n_samples * train_size), '1/n_samples'),
                  ]

        for subplotnum, (scaler, name) in enumerate(scales):
            plt.subplot(2, 1, subplotnum + 1)
            plt.xlabel('C')
            plt.ylabel('CV Score')
            grid_cs = cs * float(scaler)  # scale the C's
            plt.semilogx(grid_cs, scores, label="fraction %.2f" %
                         train_size, color=colors[k], lw=lw)
            plt.title('scaling=%s, penalty=%s, loss=%s' %
                      (name, clf.penalty, clf.loss))

    plt.legend(loc="best")
plt.show()
</pre> <p><strong>Total running time of the script:</strong> (1 minutes 1.682 seconds)</p> <div class="sphx-glr-download container"> <strong>Download Python source code:</strong> <a class="reference download internal" href="http://scikit-learn.org/stable/_downloads/plot_svm_scale_c.py" download="" target="_blank"><code>plot_svm_scale_c.py</code></a>
</div> <div class="sphx-glr-download container"> <strong>Download IPython notebook:</strong> <a class="reference download internal" href="http://scikit-learn.org/stable/_downloads/plot_svm_scale_c.ipynb" download="" target="_blank"><code>plot_svm_scale_c.ipynb</code></a>
</div>
<div class="_attribution">
  <p class="_attribution-p">
    © 2007–2016 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="http://scikit-learn.org/stable/auto_examples/svm/plot_svm_scale_c.html" class="_attribution-link" target="_blank">http://scikit-learn.org/stable/auto_examples/svm/plot_svm_scale_c.html</a>
  </p>
</div>

			</div>
		</div>
	</section>

	</div>
</body>
</html>
