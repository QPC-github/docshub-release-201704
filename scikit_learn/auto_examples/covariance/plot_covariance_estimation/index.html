
<!DOCTYPE HTML>

<html lang="en">

<head>
  <meta charset="utf-8">
  <title>Example&#58; Shrinkage Covariance Estimation - Scikit-learn - W3cubDocs</title>
  
  <meta name="description" content="When working with covariance estimation, the usual approach is to use a maximum likelihood estimator, such as the sklearn.covariance. &hellip;">
  <meta name="keywords" content="shrinkage, covariance, estimation, ledoitwolf, oas, and, max-likelihood, example, -, scikit-learn, scikit_learn">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="http://docs.w3cub.com/scikit_learn/auto_examples/covariance/plot_covariance_estimation/">
  <link href="/favicon.png" rel="icon">
  <link type="text/css" rel="stylesheet" href="/assets/application-50364fff564ce3b6327021805f3f00e2957b441cf27f576a7dd4ff63bbc47047.css">
  <script type="text/javascript" src="/assets/application-db64bfd54ceb42be11af7995804cf4902548419ceb79d509b0b7d62c22d98e6f.js"></script>
  <script src="/json/scikit_learn.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body>
	<div class="_app">
	<header class="_header">
  
  <form class="_search">
    <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
    <a class="_search-clear"></a>
    <div class="_search-tag"></div>
  </form>
  
  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/scikit_learn/" class="_nav-link" title="" style="margin-left:0;">scikit-learn</a></span>
  
  <nav class="_nav">
    <a href="/app/" class="_nav-link ">App</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		<div class="_list">
			
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<div class="_page _sphinx">
				
<h1 id="sphx-glr-auto-examples-covariance-plot-covariance-estimation-py">Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood</h1> <p id="shrinkage-covariance-estimation-ledoitwolf-vs-oas-and-max-likelihood">When working with covariance estimation, the usual approach is to use a maximum likelihood estimator, such as the <a class="reference internal" href="../../../modules/generated/sklearn.covariance.empiricalcovariance/#sklearn.covariance.EmpiricalCovariance" title="sklearn.covariance.EmpiricalCovariance"><code>sklearn.covariance.EmpiricalCovariance</code></a>. It is unbiased, i.e. it converges to the true (population) covariance when given many observations. However, it can also be beneficial to regularize it, in order to reduce its variance; this, in turn, introduces some bias. This example illustrates the simple regularization used in <a class="reference internal" href="../../../modules/covariance/#shrunk-covariance"><span class="std std-ref">Shrunk Covariance</span></a> estimators. In particular, it focuses on how to set the amount of regularization, i.e. how to choose the bias-variance trade-off.</p> <p>Here we compare 3 approaches:</p> <ul class="simple"> <li>Setting the parameter by cross-validating the likelihood on three folds according to a grid of potential shrinkage parameters.</li> <li>A close formula proposed by Ledoit and Wolf to compute the asymptotically optimal regularization parameter (minimizing a MSE criterion), yielding the <a class="reference internal" href="../../../modules/generated/sklearn.covariance.ledoitwolf/#sklearn.covariance.LedoitWolf" title="sklearn.covariance.LedoitWolf"><code>sklearn.covariance.LedoitWolf</code></a> covariance estimate.</li> <li>An improvement of the Ledoit-Wolf shrinkage, the <a class="reference internal" href="../../../modules/generated/sklearn.covariance.oas/#sklearn.covariance.OAS" title="sklearn.covariance.OAS"><code>sklearn.covariance.OAS</code></a>, proposed by Chen et al. Its convergence is significantly better under the assumption that the data are Gaussian, in particular for small samples.</li> </ul> <p>To quantify estimation error, we plot the likelihood of unseen data for different values of the shrinkage parameter. We also show the choices by cross-validation, or with the LedoitWolf and OAS estimates.</p> <p>Note that the maximum likelihood estimate corresponds to no shrinkage, and thus performs poorly. The Ledoit-Wolf estimate performs really well, as it is close to the optimal and is computational not costly. In this example, the OAS estimate is a bit further away. Interestingly, both approaches outperform cross-validation, which is significantly most computationally costly.</p> <pre data-language="python">print(__doc__)

import numpy as np
import matplotlib.pyplot as plt
from scipy import linalg

from sklearn.covariance import LedoitWolf, OAS, ShrunkCovariance, \
    log_likelihood, empirical_covariance
from sklearn.model_selection import GridSearchCV
</pre> <p>Generate sample data</p> <pre data-language="python">n_features, n_samples = 40, 20
np.random.seed(42)
base_X_train = np.random.normal(size=(n_samples, n_features))
base_X_test = np.random.normal(size=(n_samples, n_features))

# Color samples
coloring_matrix = np.random.normal(size=(n_features, n_features))
X_train = np.dot(base_X_train, coloring_matrix)
X_test = np.dot(base_X_test, coloring_matrix)
</pre> <p>Compute the likelihood on test data</p> <pre data-language="python"># spanning a range of possible shrinkage coefficient values
shrinkages = np.logspace(-2, 0, 30)
negative_logliks = [-ShrunkCovariance(shrinkage=s).fit(X_train).score(X_test)
                    for s in shrinkages]

# under the ground-truth model, which we would not have access to in real
# settings
real_cov = np.dot(coloring_matrix.T, coloring_matrix)
emp_cov = empirical_covariance(X_train)
loglik_real = -log_likelihood(emp_cov, linalg.inv(real_cov))
</pre> <p>Compare different approaches to setting the parameter</p> <pre data-language="python"># GridSearch for an optimal shrinkage coefficient
tuned_parameters = [{'shrinkage': shrinkages}]
cv = GridSearchCV(ShrunkCovariance(), tuned_parameters)
cv.fit(X_train)

# Ledoit-Wolf optimal shrinkage coefficient estimate
lw = LedoitWolf()
loglik_lw = lw.fit(X_train).score(X_test)

# OAS coefficient estimate
oa = OAS()
loglik_oa = oa.fit(X_train).score(X_test)
</pre> <p>Plot results</p> <pre data-language="python">fig = plt.figure()
plt.title("Regularized covariance: likelihood and shrinkage coefficient")
plt.xlabel('Regularizaton parameter: shrinkage coefficient')
plt.ylabel('Error: negative log-likelihood on test data')
# range shrinkage curve
plt.loglog(shrinkages, negative_logliks, label="Negative log-likelihood")

plt.plot(plt.xlim(), 2 * [loglik_real], '--r',
         label="Real covariance likelihood")

# adjust view
lik_max = np.amax(negative_logliks)
lik_min = np.amin(negative_logliks)
ymin = lik_min - 6. * np.log((plt.ylim()[1] - plt.ylim()[0]))
ymax = lik_max + 10. * np.log(lik_max - lik_min)
xmin = shrinkages[0]
xmax = shrinkages[-1]
# LW likelihood
plt.vlines(lw.shrinkage_, ymin, -loglik_lw, color='magenta',
           linewidth=3, label='Ledoit-Wolf estimate')
# OAS likelihood
plt.vlines(oa.shrinkage_, ymin, -loglik_oa, color='purple',
           linewidth=3, label='OAS estimate')
# best CV estimator likelihood
plt.vlines(cv.best_estimator_.shrinkage, ymin,
           -cv.best_estimator_.score(X_test), color='cyan',
           linewidth=3, label='Cross-validation best estimate')

plt.ylim(ymin, ymax)
plt.xlim(xmin, xmax)
plt.legend()

plt.show()
</pre> <img alt="../../_images/sphx_glr_plot_covariance_estimation_001.png" class="align-center" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_covariance_estimation_001.png"> <p><strong>Total running time of the script:</strong> (0 minutes 0.436 seconds)</p> <div class="sphx-glr-download container"> <strong>Download Python source code:</strong> <a class="reference download internal" href="http://scikit-learn.org/stable/_downloads/plot_covariance_estimation.py" download="" target="_blank"><code>plot_covariance_estimation.py</code></a>
</div> <div class="sphx-glr-download container"> <strong>Download IPython notebook:</strong> <a class="reference download internal" href="http://scikit-learn.org/stable/_downloads/plot_covariance_estimation.ipynb" download="" target="_blank"><code>plot_covariance_estimation.ipynb</code></a>
</div>
<div class="_attribution">
  <p class="_attribution-p">
    © 2007–2016 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="http://scikit-learn.org/stable/auto_examples/covariance/plot_covariance_estimation.html" class="_attribution-link" target="_blank">http://scikit-learn.org/stable/auto_examples/covariance/plot_covariance_estimation.html</a>
  </p>
</div>

			</div>
		</div>
	</section>

	</div>
</body>
</html>
