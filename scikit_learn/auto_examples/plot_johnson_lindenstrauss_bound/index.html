
<!DOCTYPE HTML>

<html lang="en">

<head>
  <meta charset="utf-8">
  <title>Example&#58; the Johnson-Lindenstrauss Bound for Embedding With Random Projections - Scikit-learn - W3cubDocs</title>
  
  <meta name="description" content="The Johnson-Lindenstrauss lemma states that any high dimensional dataset can be randomly projected into a lower dimensional Euclidean space while &hellip;">
  <meta name="keywords" content="johnson-lindenstrauss, bound, for, embedding, with, random, projections, example, -, scikit-learn, scikit_learn">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="http://docs.w3cub.com/scikit_learn/auto_examples/plot_johnson_lindenstrauss_bound/">
  <link href="/favicon.png" rel="icon">
  <link type="text/css" rel="stylesheet" href="/assets/application-50364fff564ce3b6327021805f3f00e2957b441cf27f576a7dd4ff63bbc47047.css">
  <script type="text/javascript" src="/assets/application-db64bfd54ceb42be11af7995804cf4902548419ceb79d509b0b7d62c22d98e6f.js"></script>
  <script src="/json/scikit_learn.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body>
	<div class="_app">
	<header class="_header">
  
  <form class="_search">
    <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
    <a class="_search-clear"></a>
    <div class="_search-tag"></div>
  </form>
  
  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/scikit_learn/" class="_nav-link" title="" style="margin-left:0;">scikit-learn</a></span>
  
  <nav class="_nav">
    <a href="/app/" class="_nav-link ">App</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		<div class="_list">
			
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<div class="_page _sphinx">
				
<h1 id="sphx-glr-auto-examples-plot-johnson-lindenstrauss-bound-py">The Johnson-Lindenstrauss bound for embedding with random projections</h1> <p id="the-johnson-lindenstrauss-bound-for-embedding-with-random-projections">The <a class="reference external" href="https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma" target="_blank">Johnson-Lindenstrauss lemma</a> states that any high dimensional dataset can be randomly projected into a lower dimensional Euclidean space while controlling the distortion in the pairwise distances.</p>  <h2 id="theoretical-bounds">Theoretical bounds</h2> <p>The distortion introduced by a random projection <code>p</code> is asserted by the fact that <code>p</code> is defining an eps-embedding with good probability as defined by:</p> <div class="math"> <p><img src="http://scikit-learn.org/stable/_images/math/f959343b770e80df34786eebb08e80e60d69d91f.png" alt="(1 - eps) \|u - v\|^2 &lt; \|p(u) - p(v)\|^2 &lt; (1 + eps) \|u - v\|^2"></p> </div>
<p>Where u and v are any rows taken from a dataset of shape [n_samples, n_features] and p is a projection by a random Gaussian N(0, 1) matrix with shape [n_components, n_features] (or a sparse Achlioptas matrix).</p> <p>The minimum number of components to guarantees the eps-embedding is given by:</p> <div class="math"> <p><img src="http://scikit-learn.org/stable/_images/math/065d63cc8b891d9269d412a42f0691aeaec0f49f.png" alt="n\_components &gt;= 4 log(n\_samples) / (eps^2 / 2 - eps^3 / 3)"></p> </div>
<p>The first plot shows that with an increasing number of samples <code>n_samples</code>, the minimal number of dimensions <code>n_components</code> increased logarithmically in order to guarantee an <code>eps</code>-embedding.</p> <p>The second plot shows that an increase of the admissible distortion <code>eps</code> allows to reduce drastically the minimal number of dimensions <code>n_components</code> for a given number of samples <code>n_samples</code></p>   <h2 id="empirical-validation">Empirical validation</h2> <p>We validate the above bounds on the digits dataset or on the 20 newsgroups text document (TF-IDF word frequencies) dataset:</p> <ul class="simple"> <li>for the digits dataset, some 8x8 gray level pixels data for 500 handwritten digits pictures are randomly projected to spaces for various larger number of dimensions <code>n_components</code>.</li> <li>for the 20 newsgroups dataset some 500 documents with 100k features in total are projected using a sparse random matrix to smaller euclidean spaces with various values for the target number of dimensions <code>n_components</code>.</li> </ul> <p>The default dataset is the digits dataset. To run the example on the twenty newsgroups dataset, pass the â€“twenty-newsgroups command line argument to this script.</p> <p>For each value of <code>n_components</code>, we plot:</p> <ul class="simple"> <li>2D distribution of sample pairs with pairwise distances in original and projected spaces as x and y axis respectively.</li> <li>1D histogram of the ratio of those distances (projected / original).</li> </ul> <p>We can see that for low values of <code>n_components</code> the distribution is wide with many distorted pairs and a skewed distribution (due to the hard limit of zero ratio on the left as distances are always positives) while for larger values of n_components the distortion is controlled and the distances are well preserved by the random projection.</p>   <h2 id="remarks">Remarks</h2> <p>According to the JL lemma, projecting 500 samples without too much distortion will require at least several thousands dimensions, irrespective of the number of features of the original dataset.</p> <p>Hence using random projections on the digits dataset which only has 64 features in the input space does not make sense: it does not allow for dimensionality reduction in this case.</p> <p>On the twenty newsgroups on the other hand the dimensionality can be decreased from 56436 down to 10000 while reasonably preserving pairwise distances.</p> <ul class="sphx-glr-horizontal"> <li>
<a class="first reference internal image-reference" href="http://scikit-learn.org/stable/_images/sphx_glr_plot_johnson_lindenstrauss_bound_001.png" target="_blank"><img alt="../_images/sphx_glr_plot_johnson_lindenstrauss_bound_001.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_johnson_lindenstrauss_bound_001.png" style="width: 376.0px; height: 282.0px;"></a> </li> <li>
<a class="first reference internal image-reference" href="http://scikit-learn.org/stable/_images/sphx_glr_plot_johnson_lindenstrauss_bound_002.png" target="_blank"><img alt="../_images/sphx_glr_plot_johnson_lindenstrauss_bound_002.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_johnson_lindenstrauss_bound_002.png" style="width: 376.0px; height: 282.0px;"></a> </li> <li>
<a class="first reference internal image-reference" href="http://scikit-learn.org/stable/_images/sphx_glr_plot_johnson_lindenstrauss_bound_003.png" target="_blank"><img alt="../_images/sphx_glr_plot_johnson_lindenstrauss_bound_003.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_johnson_lindenstrauss_bound_003.png" style="width: 376.0px; height: 282.0px;"></a> </li> <li>
<a class="first reference internal image-reference" href="http://scikit-learn.org/stable/_images/sphx_glr_plot_johnson_lindenstrauss_bound_004.png" target="_blank"><img alt="../_images/sphx_glr_plot_johnson_lindenstrauss_bound_004.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_johnson_lindenstrauss_bound_004.png" style="width: 376.0px; height: 282.0px;"></a> </li> <li>
<a class="first reference internal image-reference" href="http://scikit-learn.org/stable/_images/sphx_glr_plot_johnson_lindenstrauss_bound_005.png" target="_blank"><img alt="../_images/sphx_glr_plot_johnson_lindenstrauss_bound_005.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_johnson_lindenstrauss_bound_005.png" style="width: 376.0px; height: 282.0px;"></a> </li> <li>
<a class="first reference internal image-reference" href="http://scikit-learn.org/stable/_images/sphx_glr_plot_johnson_lindenstrauss_bound_006.png" target="_blank"><img alt="../_images/sphx_glr_plot_johnson_lindenstrauss_bound_006.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_johnson_lindenstrauss_bound_006.png" style="width: 376.0px; height: 282.0px;"></a> </li> <li>
<a class="first reference internal image-reference" href="http://scikit-learn.org/stable/_images/sphx_glr_plot_johnson_lindenstrauss_bound_007.png" target="_blank"><img alt="../_images/sphx_glr_plot_johnson_lindenstrauss_bound_007.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_johnson_lindenstrauss_bound_007.png" style="width: 376.0px; height: 282.0px;"></a> </li> <li>
<a class="first reference internal image-reference" href="http://scikit-learn.org/stable/_images/sphx_glr_plot_johnson_lindenstrauss_bound_008.png" target="_blank"><img alt="../_images/sphx_glr_plot_johnson_lindenstrauss_bound_008.png" src="http://scikit-learn.org/stable/_images/sphx_glr_plot_johnson_lindenstrauss_bound_008.png" style="width: 376.0px; height: 282.0px;"></a> </li> </ul> <p class="sphx-glr-script-out">Out:</p> <pre data-language="python">  Embedding 500 samples with dim 64 using various random projections
Projected 500 samples from 64 to 300 in 0.022s
Random matrix with size: 0.030MB
Mean distances rate: 0.99 (0.07)
Projected 500 samples from 64 to 1000 in 0.044s
Random matrix with size: 0.098MB
Mean distances rate: 1.01 (0.05)
Projected 500 samples from 64 to 10000 in 0.726s
Random matrix with size: 0.961MB
Mean distances rate: 1.00 (0.01)
</pre>  <pre data-language="python">print(__doc__)

import sys
from time import time
import numpy as np
import matplotlib.pyplot as plt
from sklearn.random_projection import johnson_lindenstrauss_min_dim
from sklearn.random_projection import SparseRandomProjection
from sklearn.datasets import fetch_20newsgroups_vectorized
from sklearn.datasets import load_digits
from sklearn.metrics.pairwise import euclidean_distances

# Part 1: plot the theoretical dependency between n_components_min and
# n_samples

# range of admissible distortions
eps_range = np.linspace(0.1, 0.99, 5)
colors = plt.cm.Blues(np.linspace(0.3, 1.0, len(eps_range)))

# range of number of samples (observation) to embed
n_samples_range = np.logspace(1, 9, 9)

plt.figure()
for eps, color in zip(eps_range, colors):
    min_n_components = johnson_lindenstrauss_min_dim(n_samples_range, eps=eps)
    plt.loglog(n_samples_range, min_n_components, color=color)

plt.legend(["eps = %0.1f" % eps for eps in eps_range], loc="lower right")
plt.xlabel("Number of observations to eps-embed")
plt.ylabel("Minimum number of dimensions")
plt.title("Johnson-Lindenstrauss bounds:\nn_samples vs n_components")

# range of admissible distortions
eps_range = np.linspace(0.01, 0.99, 100)

# range of number of samples (observation) to embed
n_samples_range = np.logspace(2, 6, 5)
colors = plt.cm.Blues(np.linspace(0.3, 1.0, len(n_samples_range)))

plt.figure()
for n_samples, color in zip(n_samples_range, colors):
    min_n_components = johnson_lindenstrauss_min_dim(n_samples, eps=eps_range)
    plt.semilogy(eps_range, min_n_components, color=color)

plt.legend(["n_samples = %d" % n for n in n_samples_range], loc="upper right")
plt.xlabel("Distortion eps")
plt.ylabel("Minimum number of dimensions")
plt.title("Johnson-Lindenstrauss bounds:\nn_components vs eps")

# Part 2: perform sparse random projection of some digits images which are
# quite low dimensional and dense or documents of the 20 newsgroups dataset
# which is both high dimensional and sparse

if '--twenty-newsgroups' in sys.argv:
    # Need an internet connection hence not enabled by default
    data = fetch_20newsgroups_vectorized().data[:500]
else:
    data = load_digits().data[:500]

n_samples, n_features = data.shape
print("Embedding %d samples with dim %d using various random projections"
      % (n_samples, n_features))

n_components_range = np.array([300, 1000, 10000])
dists = euclidean_distances(data, squared=True).ravel()

# select only non-identical samples pairs
nonzero = dists != 0
dists = dists[nonzero]

for n_components in n_components_range:
    t0 = time()
    rp = SparseRandomProjection(n_components=n_components)
    projected_data = rp.fit_transform(data)
    print("Projected %d samples from %d to %d in %0.3fs"
          % (n_samples, n_features, n_components, time() - t0))
    if hasattr(rp, 'components_'):
        n_bytes = rp.components_.data.nbytes
        n_bytes += rp.components_.indices.nbytes
        print("Random matrix with size: %0.3fMB" % (n_bytes / 1e6))

    projected_dists = euclidean_distances(
        projected_data, squared=True).ravel()[nonzero]

    plt.figure()
    plt.hexbin(dists, projected_dists, gridsize=100, cmap=plt.cm.PuBu)
    plt.xlabel("Pairwise squared distances in original space")
    plt.ylabel("Pairwise squared distances in projected space")
    plt.title("Pairwise distances distribution for n_components=%d" %
              n_components)
    cb = plt.colorbar()
    cb.set_label('Sample pairs counts')

    rates = projected_dists / dists
    print("Mean distances rate: %0.2f (%0.2f)"
          % (np.mean(rates), np.std(rates)))

    plt.figure()
    plt.hist(rates, bins=50, normed=True, range=(0., 2.))
    plt.xlabel("Squared distances rate: projected / original")
    plt.ylabel("Distribution of samples pairs")
    plt.title("Histogram of pairwise distance rates for n_components=%d" %
              n_components)

    # TODO: compute the expected value of eps and add them to the previous plot
    # as vertical lines / region

plt.show()
</pre> <p><strong>Total running time of the script:</strong> (0 minutes 3.466 seconds)</p> <div class="sphx-glr-download container"> <strong>Download Python source code:</strong> <a class="reference download internal" href="http://scikit-learn.org/stable/_downloads/plot_johnson_lindenstrauss_bound.py" download="" target="_blank"><code>plot_johnson_lindenstrauss_bound.py</code></a>
</div> <div class="sphx-glr-download container"> <strong>Download IPython notebook:</strong> <a class="reference download internal" href="http://scikit-learn.org/stable/_downloads/plot_johnson_lindenstrauss_bound.ipynb" download="" target="_blank"><code>plot_johnson_lindenstrauss_bound.ipynb</code></a>
</div>
<div class="_attribution">
  <p class="_attribution-p">
    Â© 2007â€“2016 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="http://scikit-learn.org/stable/auto_examples/plot_johnson_lindenstrauss_bound.html" class="_attribution-link" target="_blank">http://scikit-learn.org/stable/auto_examples/plot_johnson_lindenstrauss_bound.html</a>
  </p>
</div>

			</div>
		</div>
	</section>

	</div>
</body>
</html>
