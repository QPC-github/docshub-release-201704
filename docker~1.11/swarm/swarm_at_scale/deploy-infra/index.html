
<!DOCTYPE HTML>

<html lang="en">

<head>
  <meta charset="utf-8">
  <title>Deploy Application Infrastructure (Swarm) - Docker 1.11 - W3cubDocs</title>
  
  <meta name="description" content="In this step, you create several Docker hosts to run your application stack on. Before you continue, make sure you have taken the time to learn the &hellip;">
  <meta name="keywords" content="deploy, your, infrastructure, application, swarm, -, docker, docker~1.11">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="http://docs.w3cub.com/docker~1.11/swarm/swarm_at_scale/deploy-infra/">
  <link href="/favicon.png" rel="icon">
  <link type="text/css" rel="stylesheet" href="/assets/application-50364fff564ce3b6327021805f3f00e2957b441cf27f576a7dd4ff63bbc47047.css">
  <script type="text/javascript" src="/assets/application-db64bfd54ceb42be11af7995804cf4902548419ceb79d509b0b7d62c22d98e6f.js"></script>
  <script src="/json/docker~1.11.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body>
	<div class="_app">
	<header class="_header">
  
  <form class="_search">
    <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
    <a class="_search-clear"></a>
    <div class="_search-tag"></div>
  </form>
  
  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/docker~1.11/" class="_nav-link" title="" style="margin-left:0;">Docker 1.11</a></span>
  
  <nav class="_nav">
    <a href="/app/" class="_nav-link ">App</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		<div class="_list">
			
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<div class="_page _docker">
				
<h1 id="deploy-your-infrastructure">Deploy your infrastructure</h1> <p>In this step, you create several Docker hosts to run your application stack on. Before you continue, make sure you have taken the time to <a href="../about/">learn the application architecture</a>.</p> <h2 id="about-these-instructions">About these instructions</h2> <p>This example assumes you are running on a Mac or Windows system and enabling Docker Engine <code>docker</code> commands by provisioning local VirtualBox virtual machines thru Docker Machine. For this evaluation installation, you’ll need 6 (six) VirtualBox VMs.</p> <p>While this example uses Docker Machine, this is only one example of an infrastructure you can use. You can create the environment design on whatever infrastructure you wish. For example, you could place the application on another public cloud platform such as Azure or DigitalOcean, on premises in your data center, or even in in a test environment on your laptop.</p> <p>Finally, these instructions use some common <code>bash</code> command substituion techniques to resolve some values, for example:</p> <pre>$ eval $(docker-machine env keystore)
</pre> <p>In a Windows environment, these substituation fail. If you are running in Windows, replace the substitution <code>$(docker-machine env keystore)</code> with the actual value.</p> <h2 id="task-1-create-the-keystore-server">Task 1. Create the keystore server</h2> <p>To enable a Docker container network and Swarm discovery, you must supply deploy a key-value store. As a discovery backend, the keystore maintains an up-to-date list of cluster members and shares that list with the Swarm manager. The Swarm manager uses this list to assign tasks to the nodes.</p> <p>An overlay network requires a key-value store. The key-value store holds information about the network state which includes discovery, networks, endpoints, IP addresses, and more.</p> <p>Several different backends are supported. This example uses <a href="https://www.consul.io/" target="_blank">Consul</a> container.</p> <ol> <li>
<p>Create a “machine” named <code>keystore</code>.</p> <pre>$ docker-machine create -d virtualbox --virtualbox-memory "2000" \
--engine-opt="label=com.function=consul"  keystore
</pre> <p>You can set options for the Engine daemon with the <code>--engine-opt</code> flag. You’ll use it to label this Engine instance.</p>
</li> <li>
<p>Set your local shell to the <code>keystore</code> Docker host.</p> <pre>$ eval $(docker-machine env keystore)
</pre>
</li> <li>
<p>Run <a href="https://hub.docker.com/r/progrium/consul/" target="_blank">the <code>consul</code> container</a>.</p> <pre>$ docker run --restart=unless-stopped -d -p 8500:8500 -h consul progrium/consul -server -bootstrap
</pre> <p>The <code>-p</code> flag publishes port 8500 on the container which is where the Consul server listens. The server also has several other ports exposed which you can see by running <code>docker ps</code>.</p> <pre>$ docker ps
CONTAINER ID        IMAGE               ...       PORTS                                                                            NAMES
372ffcbc96ed        progrium/consul     ...       53/tcp, 53/udp, 8300-8302/tcp, 8400/tcp, 8301-8302/udp, 0.0.0.0:8500-&gt;8500/tcp   dreamy_ptolemy
</pre>
</li> <li>
<p>Use a <code>curl</code> command test the server by listing the nodes.</p> <pre>$ curl $(docker-machine ip keystore):8500/v1/catalog/nodes
[{"Node":"consul","Address":"172.17.0.2"}]
</pre>
</li> </ol> <h2 id="task-2-create-the-swarm-manager">Task 2. Create the Swarm manager</h2> <p>In this step, you create the Swarm manager and connect it to the <code>keystore</code> instance. The Swarm manager container is the heart of your Swarm cluster. It is responsible for receiving all Docker commands sent to the cluster, and for scheduling resources against the cluster. In a real-world production deployment, you should configure additional replica Swarm managers as secondaries for high availability (HA).</p> <p>You’ll use the <code>--eng-opt</code> flag to set the <code>cluster-store</code> and <code>cluster-advertise</code> options to refer to the <code>keystore</code> server. These options support the container network you’ll create later.</p> <ol> <li>
<p>Create the <code>manager</code> host.</p> <pre>$ docker-machine create -d virtualbox --virtualbox-memory "2000" \
--engine-opt="label=com.function=manager" \
--engine-opt="cluster-store=consul://$(docker-machine ip keystore):8500" \
--engine-opt="cluster-advertise=eth1:2376" manager
</pre> <p>You also give the daemon a <code>manager</code> label.</p>
</li> <li>
<p>Set your local shell to the <code>manager</code> Docker host.</p> <pre>$ eval $(docker-machine env manager)
</pre>
</li> <li>
<p>Start the Swarm manager process.</p> <pre>$ docker run --restart=unless-stopped -d -p 3376:2375 \
-v /var/lib/boot2docker:/certs:ro \
swarm manage --tlsverify \
--tlscacert=/certs/ca.pem \
--tlscert=/certs/server.pem \
--tlskey=/certs/server-key.pem \
consul://$(docker-machine ip keystore):8500
</pre> <p>This command uses the TLS certificates created for the <code>boot2docker.iso</code> or the manager. This is key for the manager when it connects to other machines in the cluster.</p>
</li> <li>
<p>Test your work by using displaying the Docker daemon logs from the host.</p> <pre>$ docker-machine ssh manager
&lt;-- output snipped --&gt;
docker@manager:~$ tail /var/lib/boot2docker/docker.log
time="2016-04-06T23:11:56.481947896Z" level=debug msg="Calling GET /v1.15/version"
time="2016-04-06T23:11:56.481984742Z" level=debug msg="GET /v1.15/version"
time="2016-04-06T23:12:13.070231761Z" level=debug msg="Watch triggered with 1 nodes" discovery=consul
time="2016-04-06T23:12:33.069387215Z" level=debug msg="Watch triggered with 1 nodes" discovery=consul
time="2016-04-06T23:12:53.069471308Z" level=debug msg="Watch triggered with 1 nodes" discovery=consul
time="2016-04-06T23:13:13.069512320Z" level=debug msg="Watch triggered with 1 nodes" discovery=consul
time="2016-04-06T23:13:33.070021418Z" level=debug msg="Watch triggered with 1 nodes" discovery=consul
time="2016-04-06T23:13:53.069395005Z" level=debug msg="Watch triggered with 1 nodes" discovery=consul
time="2016-04-06T23:14:13.071417551Z" level=debug msg="Watch triggered with 1 nodes" discovery=consul
time="2016-04-06T23:14:33.069843647Z" level=debug msg="Watch triggered with 1 nodes" discovery=consul
</pre> <p>The output indicates that the <code>consul</code> and the <code>manager</code> are communicating correctly.</p>
</li> <li>
<p>Exit the Docker host.</p> <pre>docker@manager:~$ exit
</pre>
</li> </ol> <h2 id="task-3-add-the-load-balancer">Task 3. Add the load balancer</h2> <p>The application uses <a href="https://github.com/ehazlett/interlock" target="_blank">Interlock</a> and Nginx as a loadbalancer. Before you build the load balancer host, you’ll create the configuration you’ll use for Nginx.</p> <ol> <li>On your local host, create a <code>config</code> directory.</li> <li>
<p>Change directories to the <code>config</code> directory.</p> <pre>$ cd config
</pre>
</li> <li>
<p>Get the IP address of the Swarm manager host.</p> <p>For example:</p> <pre>$ docker-machine ip manager
192.168.99.101
</pre>
</li> <li>
<p>Use your favorite editor to create a <code>config.toml</code> file and add this content to the file:</p> <pre>ListenAddr = ":8080"
DockerURL = "tcp://SWARM_MANAGER_IP:3376"
TLSCACert = "/var/lib/boot2docker/ca.pem"
TLSCert = "/var/lib/boot2docker/server.pem"
TLSKey = "/var/lib/boot2docker/server-key.pem"

[[Extensions]]
Name = "nginx"
ConfigPath = "/etc/conf/nginx.conf"
PidPath = "/etc/conf/nginx.pid"
MaxConn = 1024
Port = 80
</pre>
</li> <li>
<p>In the configuration, replace the <code>SWARM_MANAGER_IP</code> with the <code>manager</code> IP you got in Step 4.</p> <p>You use this value because the load balancer listens on the manager’s event stream.</p>
</li> <li><p>Save and close the <code>config.toml</code> file.</p></li> <li>
<p>Create a machine for the load balancer.</p> <pre>$ docker-machine create -d virtualbox --virtualbox-memory "2000" \
--engine-opt="label=com.function=interlock" loadbalancer
</pre>
</li> <li>
<p>Switch the environment to the <code>loadbalancer</code>.</p> <pre>$ eval $(docker-machine env loadbalancer)
</pre>
</li> <li>
<p>Start an <code>interlock</code> container running.</p> <pre>$ docker run \
    -P \
    -d \
    -ti \
    -v nginx:/etc/conf \
    -v /var/lib/boot2docker:/var/lib/boot2docker:ro \
    -v /var/run/docker.sock:/var/run/docker.sock \
    -v $(pwd)/config.toml:/etc/config.toml \
    --name interlock \
    ehazlett/interlock:1.0.1 \
    -D run -c /etc/config.toml
</pre> <p>This command relies on the <code>config.toml</code> file being in the current directory. After running the command, confirm the image is runing:</p> <pre>$ docker ps
CONTAINER ID        IMAGE                      COMMAND                  CREATED             STATUS              PORTS                     NAMES
d846b801a978        ehazlett/interlock:1.0.1   "/bin/interlock -D ru"   2 minutes ago       Up 2 minutes        0.0.0.0:32770-&gt;8080/tcp   interlock
</pre> <p>If you don’t see the image runing, use <code>docker ps -a</code> to list all images to make sure the system attempted to start the image. Then, get the logs to see why the container failed to start.</p> <pre>$ docker logs interlock
INFO[0000] interlock 1.0.1 (000291d)
DEBU[0000] loading config from: /etc/config.toml
FATA[0000] read /etc/config.toml: is a directory
</pre> <p>This error usually means you weren’t starting the <code>docker run</code> from the same <code>config</code> directory where the <code>config.toml</code> file is. If you run the command and get a Conflict error such as:</p> <pre>docker: Error response from daemon: Conflict. The name "/interlock" is already in use by container d846b801a978c76979d46a839bb05c26d2ab949ff9f4f740b06b5e2564bae958. You have to remove (or rename) that container to be able to reuse that name.
</pre> <p>Remove the interlock container with the <code>docker rm interlock</code> and try again.</p>
</li> <li>
<p>Start an <code>nginx</code> container on the load balancer.</p> <pre>$ docker run -ti -d \
  -p 80:80 \
  --label interlock.ext.name=nginx \
  --link=interlock:interlock \
  -v nginx:/etc/conf \
  --name nginx \
  nginx nginx -g "daemon off;" -c /etc/conf/nginx.conf
</pre>
</li> </ol> <h2 id="task-4-create-the-other-swarm-nodes">Task 4. Create the other Swarm nodes</h2> <p>A host in a Swarm cluster is called a <em>node</em>. You’ve already created the manager node. Here, the task is to create each virtual host for each node. There are three commands required:</p> <ul> <li>create the host with Docker Machine</li> <li>point the local environment to the new host</li> <li>join the host to the Swarm cluster</li> </ul> <p>If you were building this in a non-Mac/Windows environment, you’d only need to run the <code>join</code> command to add a node to the Swarm cluster and register it with the Consul discovery service. When you create a node, you also give it a label, for example:</p> <pre>--engine-opt="label=com.function=frontend01"
</pre> <p>You’ll use these labels later when starting application containers. In the commands below, notice the label you are applying to each node.</p> <ol> <li>
<p>Create the <code>frontend01</code> host and add it to the Swarm cluster.</p> <pre>$ docker-machine create -d virtualbox --virtualbox-memory "2000" \
--engine-opt="label=com.function=frontend01" \
--engine-opt="cluster-store=consul://$(docker-machine ip keystore):8500" \
--engine-opt="cluster-advertise=eth1:2376" frontend01
$ eval $(docker-machine env frontend01)
$ docker run -d swarm join --addr=$(docker-machine ip frontend01):2376 consul://$(docker-machine ip keystore):8500
</pre>
</li> <li>
<p>Create the <code>frontend02</code> VM.</p> <pre>$ docker-machine create -d virtualbox --virtualbox-memory "2000" \
--engine-opt="label=com.function=frontend02" \
--engine-opt="cluster-store=consul://$(docker-machine ip keystore):8500" \
--engine-opt="cluster-advertise=eth1:2376" frontend02
$ eval $(docker-machine env frontend02)
$ docker run -d swarm join --addr=$(docker-machine ip frontend02):2376 consul://$(docker-machine ip keystore):8500
</pre>
</li> <li>
<p>Create the <code>worker01</code> VM.</p> <pre>$ docker-machine create -d virtualbox --virtualbox-memory "2000" \
--engine-opt="label=com.function=worker01" \
--engine-opt="cluster-store=consul://$(docker-machine ip keystore):8500" \
--engine-opt="cluster-advertise=eth1:2376" worker01
$ eval $(docker-machine env worker01)
$ docker run -d swarm join --addr=$(docker-machine ip worker01):2376 consul://$(docker-machine ip keystore):8500
</pre>
</li> <li>
<p>Create the <code>dbstore</code> VM.</p> <pre>$ docker-machine create -d virtualbox --virtualbox-memory "2000" \
--engine-opt="label=com.function=dbstore" \
--engine-opt="cluster-store=consul://$(docker-machine ip keystore):8500" \
--engine-opt="cluster-advertise=eth1:2376" dbstore
$ eval $(docker-machine env dbstore)
$ docker run -d swarm join --addr=$(docker-machine ip dbstore):2376 consul://$(docker-machine ip keystore):8500
</pre>
</li> <li>
<p>Check your work.</p> <p>At this point, you have deployed on the infrastructure you need to run the application. Test this now by listing the running machines:</p> <pre>$ docker-machine ls
NAME           ACTIVE   DRIVER       STATE     URL                         SWARM   DOCKER    ERRORS
dbstore        -        virtualbox   Running   tcp://192.168.99.111:2376           v1.10.3
frontend01     -        virtualbox   Running   tcp://192.168.99.108:2376           v1.10.3
frontend02     -        virtualbox   Running   tcp://192.168.99.109:2376           v1.10.3
keystore       -        virtualbox   Running   tcp://192.168.99.100:2376           v1.10.3
loadbalancer   -        virtualbox   Running   tcp://192.168.99.107:2376           v1.10.3
manager        -        virtualbox   Running   tcp://192.168.99.101:2376           v1.10.3
worker01       *        virtualbox   Running   tcp://192.168.99.110:2376           v1.10.3
</pre>
</li> <li>
<p>Make sure the Swarm manager sees all your nodes.</p> <pre>$ docker -H $(docker-machine ip manager):3376 info
Containers: 4
 Running: 4
 Paused: 0
 Stopped: 0
Images: 3
Server Version: swarm/1.1.3
Role: primary
Strategy: spread
Filters: health, port, dependency, affinity, constraint
Nodes: 4
 dbstore: 192.168.99.111:2376
  └ Status: Healthy
  └ Containers: 1
  └ Reserved CPUs: 0 / 1
  └ Reserved Memory: 0 B / 2.004 GiB
  └ Labels: com.function=dbstore, executiondriver=native-0.2, kernelversion=4.1.19-boot2docker, operatingsystem=Boot2Docker 1.10.3 (TCL 6.4.1); master : 625117e - Thu Mar 10 22:09:02 UTC 2016, provider=virtualbox, storagedriver=aufs
  └ Error: (none)
  └ UpdatedAt: 2016-04-07T18:25:37Z
 frontend01: 192.168.99.108:2376
  └ Status: Healthy
  └ Containers: 1
  └ Reserved CPUs: 0 / 1
  └ Reserved Memory: 0 B / 2.004 GiB
  └ Labels: com.function=frontend01, executiondriver=native-0.2, kernelversion=4.1.19-boot2docker, operatingsystem=Boot2Docker 1.10.3 (TCL 6.4.1); master : 625117e - Thu Mar 10 22:09:02 UTC 2016, provider=virtualbox, storagedriver=aufs
  └ Error: (none)
  └ UpdatedAt: 2016-04-07T18:26:10Z
 frontend02: 192.168.99.109:2376
  └ Status: Healthy
  └ Containers: 1
  └ Reserved CPUs: 0 / 1
  └ Reserved Memory: 0 B / 2.004 GiB
  └ Labels: com.function=frontend02, executiondriver=native-0.2, kernelversion=4.1.19-boot2docker, operatingsystem=Boot2Docker 1.10.3 (TCL 6.4.1); master : 625117e - Thu Mar 10 22:09:02 UTC 2016, provider=virtualbox, storagedriver=aufs
  └ Error: (none)
  └ UpdatedAt: 2016-04-07T18:25:43Z
 worker01: 192.168.99.110:2376
  └ Status: Healthy
  └ Containers: 1
  └ Reserved CPUs: 0 / 1
  └ Reserved Memory: 0 B / 2.004 GiB
  └ Labels: com.function=worker01, executiondriver=native-0.2, kernelversion=4.1.19-boot2docker, operatingsystem=Boot2Docker 1.10.3 (TCL 6.4.1); master : 625117e - Thu Mar 10 22:09:02 UTC 2016, provider=virtualbox, storagedriver=aufs
  └ Error: (none)
  └ UpdatedAt: 2016-04-07T18:25:56Z
Plugins:
 Volume:
 Network:
Kernel Version: 4.1.19-boot2docker
Operating System: linux
Architecture: amd64
CPUs: 4
Total Memory: 8.017 GiB
Name: bb13b7cf80e8
</pre> <p>The command is acting on the Swarm port, so it returns information about the entire cluster. You have a manager and no nodes.</p>
</li> </ol> <h2 id="next-step">Next Step</h2> <p>Your key-store, load balancer, and Swarm cluster infrastructure is up. You are ready to <a href="../deploy-app/">build and run the voting application</a> on it.</p>
<div class="_attribution">
  <p class="_attribution-p">
    © 2013–2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>
    <a href="https://docs.docker.com/v1.11/swarm/swarm_at_scale/deploy-infra/" class="_attribution-link" target="_blank">https://docs.docker.com/v1.11/swarm/swarm_at_scale/deploy-infra/</a>
  </p>
</div>

			</div>
		</div>
	</section>

	</div>
</body>
</html>
