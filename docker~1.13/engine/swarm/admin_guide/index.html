
<!DOCTYPE HTML>

<html lang="en">

<head>
  <meta charset="utf-8">
  <title>Swarm Administration Guide (Engine) - Docker 1.13 - W3cubDocs</title>
  
  <meta name="description" content="When you run a swarm of Docker Engines, manager nodes are the key components for managing the swarm and storing the swarm state. It is important to &hellip;">
  <meta name="keywords" content="administer, and, maintain, swarm, docker, engines, administration, guide, engine, -, docker~1.13">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="http://docs.w3cub.com/docker~1.13/engine/swarm/admin_guide/">
  <link href="/favicon.png" rel="icon">
  <link type="text/css" rel="stylesheet" href="/assets/application-50364fff564ce3b6327021805f3f00e2957b441cf27f576a7dd4ff63bbc47047.css">
  <script type="text/javascript" src="/assets/application-db64bfd54ceb42be11af7995804cf4902548419ceb79d509b0b7d62c22d98e6f.js"></script>
  <script src="/json/docker~1.13.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body>
	<div class="_app">
	<header class="_header">
  
  <form class="_search">
    <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
    <a class="_search-clear"></a>
    <div class="_search-tag"></div>
  </form>
  
  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/docker~1.13/" class="_nav-link" title="" style="margin-left:0;">Docker 1.13</a></span>
  
  <nav class="_nav">
    <a href="/app/" class="_nav-link ">App</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		<div class="_list">
			
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<div class="_page _docker">
				
<h1>Administer and maintain a swarm of Docker Engines</h1>  <p>When you run a swarm of Docker Engines, <strong>manager nodes</strong> are the key components for managing the swarm and storing the swarm state. It is important to understand some key features of manager nodes in order to properly deploy and maintain the swarm.</p> <p>This article covers the following swarm administration tasks:</p> <ul> <li><a href="#use-a-static-ip-for-manager-node-advertise-address">Using a static IP for manager node advertise address</a></li> <li><a href="#add-manager-nodes-for-fault-tolerance">Adding manager nodes for fault tolerance</a></li> <li><a href="#distribute-manager-nodes">Distributing manager nodes</a></li> <li><a href="#run-manager-only-nodes">Running manager-only nodes</a></li> <li><a href="#back-up-the-swarm-state">Backing up the swarm state</a></li> <li><a href="#monitor-swarm-health">Monitoring the swarm health</a></li> <li><a href="#troubleshoot-a-manager-node">Troubleshooting a manager node</a></li> <li><a href="#force-remove-a-node">Forcefully removing a node</a></li> <li><a href="#recover-from-disaster">Recovering from disaster</a></li> <li><a href="#forcing-the-swarm-to-rebalance">Forcing the swarm to rebalance</a></li> </ul> <p>Refer to <a href="../how-swarm-mode-works/nodes/">How nodes work</a> for a brief overview of Docker Swarm mode and the difference between manager and worker nodes.</p> <h2 id="operating-manager-nodes-in-a-swarm">Operating manager nodes in a swarm</h2> <p>Swarm manager nodes use the <a href="../raft/">Raft Consensus Algorithm</a> to manage the swarm state. You only need to understand some general concepts of Raft in order to manage a swarm.</p> <p>There is no limit on the number of manager nodes. The decision about how many manager nodes to implement is a trade-off between performance and fault-tolerance. Adding manager nodes to a swarm makes the swarm more fault-tolerant. However, additional manager nodes reduce write performance because more nodes must acknowledge proposals to update the swarm state. This means more network round-trip traffic.</p> <p>Raft requires a majority of managers, also called the quorum, to agree on proposed updates to the swarm, such as node additions or removals. Membership operations are subject to the same constraints as state replication.</p> <h3 id="maintaining-the-quorum-of-managers">Maintaining the quorum of managers</h3> <p>If the swarm loses the quorum of managers, the swarm cannot perform management tasks. If your swarm has multiple managers, always have more than two. In order to maintain quorum, a majority of managers must be available. An odd number of managers is recommended, because the next even number does not make the quorum easier to keep. For instance, whether you have 3 or 4 managers, you can still only lose 1 manager and maintain the quorum. If you have 5 or 6 managers, you can still only lose two.</p> <p>Even if a swarm loses the quorum of managers, swarm tasks on existing worker nodes continue to run. However, swarm nodes cannot be added, updated, or removed, and new or existing tasks cannot be started, stopped, moved, or updated.</p> <p>See <a href="#recovering-from-losing-the-quorum">Recovering from losing the quorum</a> for troubleshooting steps if you do lose the quorum of managers.</p> <h2 id="use-a-static-ip-for-manager-node-advertise-address">Use a static IP for manager node advertise address</h2> <p>When initiating a swarm, you have to specify the <code class="highlighter-rouge">--advertise-addr</code> flag to advertise your address to other manager nodes in the swarm. For more information, see <a href="../swarm-mode/#configure-the-advertise-address">Run Docker Engine in swarm mode</a>. Because manager nodes are meant to be a stable component of the infrastructure, you should use a <em>fixed IP address</em> for the advertise address to prevent the swarm from becoming unstable on machine reboot.</p> <p>If the whole swarm restarts and every manager node subsequently gets a new IP address, there is no way for any node to contact an existing manager. Therefore the swarm is hung while nodes to contact one another at their old IP addresses.</p> <p>Dynamic IP addresses are OK for worker nodes.</p> <h2 id="add-manager-nodes-for-fault-tolerance">Add manager nodes for fault tolerance</h2> <p>You should maintain an odd number of managers in the swarm to support manager node failures. Having an odd number of managers ensures that during a network partition, there is a higher chance that the quorum remains available to process requests if the network is partitioned into two sets. Keeping the quorum is not guaranteed if you encounter more than two network partitions.</p> <table> <thead> <tr> <th style="text-align: center">Swarm Size</th> <th style="text-align: center">Majority</th> <th style="text-align: center">Fault Tolerance</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">1</td> <td style="text-align: center">1</td> <td style="text-align: center">0</td> </tr> <tr> <td style="text-align: center">2</td> <td style="text-align: center">2</td> <td style="text-align: center">0</td> </tr> <tr> <td style="text-align: center"><strong>3</strong></td> <td style="text-align: center">2</td> <td style="text-align: center"><strong>1</strong></td> </tr> <tr> <td style="text-align: center">4</td> <td style="text-align: center">3</td> <td style="text-align: center">1</td> </tr> <tr> <td style="text-align: center"><strong>5</strong></td> <td style="text-align: center">3</td> <td style="text-align: center"><strong>2</strong></td> </tr> <tr> <td style="text-align: center">6</td> <td style="text-align: center">4</td> <td style="text-align: center">2</td> </tr> <tr> <td style="text-align: center"><strong>7</strong></td> <td style="text-align: center">4</td> <td style="text-align: center"><strong>3</strong></td> </tr> <tr> <td style="text-align: center">8</td> <td style="text-align: center">5</td> <td style="text-align: center">3</td> </tr> <tr> <td style="text-align: center"><strong>9</strong></td> <td style="text-align: center">5</td> <td style="text-align: center"><strong>4</strong></td> </tr> </tbody> </table> <p>For example, in a swarm with <em>5 nodes</em>, if you lose <em>3 nodes</em>, you don’t have a quorum. Therefore you can’t add or remove nodes until you recover one of the unavailable manager nodes or recover the swarm with disaster recovery commands. See <a href="#recover-from-disaster">Recover from disaster</a>.</p> <p>While it is possible to scale a swarm down to a single manager node, it is impossible to demote the last manager node. This ensures you maintain access to the swarm and that the swarm can still process requests. Scaling down to a single manager is an unsafe operation and is not recommended. If the last node leaves the swarm unexpetedly during the demote operation, the swarm will become unavailable until you reboot the node or restart with <code class="highlighter-rouge">--force-new-cluster</code>.</p> <p>You manage swarm membership with the <code class="highlighter-rouge">docker swarm</code> and <code class="highlighter-rouge">docker node</code> subsystems. Refer to <a href="../join-nodes/">Add nodes to a swarm</a> for more information on how to add worker nodes and promote a worker node to be a manager.</p> <h2 id="distribute-manager-nodes">Distribute manager nodes</h2> <p>In addition to maintaining an odd number of manager nodes, pay attention to datacenter topology when placing managers. For optimal fault-tolerance, distribute manager nodes across a minimum of 3 availability-zones to support failures of an entire set of machines or common maintenance scenarios. If you suffer a failure in any of those zones, the swarm should maintain the quorum of manager nodes available to process requests and rebalance workloads.</p> <table> <thead> <tr> <th style="text-align: center">Swarm manager nodes</th> <th style="text-align: center">Repartition (on 3 Availability zones)</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">3</td> <td style="text-align: center">1-1-1</td> </tr> <tr> <td style="text-align: center">5</td> <td style="text-align: center">2-2-1</td> </tr> <tr> <td style="text-align: center">7</td> <td style="text-align: center">3-2-2</td> </tr> <tr> <td style="text-align: center">9</td> <td style="text-align: center">3-3-3</td> </tr> </tbody> </table> <h2 id="run-manager-only-nodes">Run manager-only nodes</h2> <p>By default manager nodes also act as a worker nodes. This means the scheduler can assign tasks to a manager node. For small and non-critical swarms assigning tasks to managers is relatively low-risk as long as you schedule services using <strong>resource constraints</strong> for <em>cpu</em> and <em>memory</em>.</p> <p>However, because manager nodes use the Raft consensus algorithm to replicate data in a consistent way, they are sensitive to resource starvation. You should isolate managers in your swarm from processes that might block swarm operations like swarm heartbeat or leader elections.</p> <p>To avoid interference with manager node operation, you can drain manager nodes to make them unavailable as worker nodes:</p> <pre class="highlight" data-language="bash">docker node update --availability drain &lt;NODE&gt;
</pre>  <p>When you drain a node, the scheduler reassigns any tasks running on the node to other available worker nodes in the swarm. It also prevents the scheduler from assigning tasks to the node.</p> <h2 id="back-up-the-swarm-state">Back up the swarm state</h2> <p>Docker manager nodes store the swarm state and manager logs in the following directory:</p> <pre class="highlight" data-language="bash">/var/lib/docker/swarm/raft
</pre>  <p>Back up the <code class="highlighter-rouge">raft</code> data directory often so that you can use it in case of <a href="#recover-from-disaster">disaster recovery</a>. Then you can take the <code class="highlighter-rouge">raft</code> directory of one of the manager nodes to restore to a new swarm.</p> <h2 id="monitor-swarm-health">Monitor swarm health</h2> <p>You can monitor the health of manager nodes by querying the docker <code class="highlighter-rouge">nodes</code> API in JSON format through the <code class="highlighter-rouge">/nodes</code> HTTP endpoint. Refer to the <a href="https://docs.docker.com/engine/reference/api/docker_remote_api_v1.24/#36-nodes" target="_blank">nodes API documentation</a> for more information.</p> <p>From the command line, run <code class="highlighter-rouge">docker node inspect &lt;id-node&gt;</code> to query the nodes. For instance, to query the reachability of the node as a manager:</p> <pre class="highlight" data-language="bash">
docker node inspect manager1 --format "{{ .ManagerStatus.Reachability }}"
reachable

</pre>  <p>To query the status of the node as a worker that accept tasks:</p> <pre class="highlight" data-language="bash">
docker node inspect manager1 --format "{{ .Status.State }}"
ready

</pre>  <p>From those commands, we can see that <code class="highlighter-rouge">manager1</code> is both at the status <code class="highlighter-rouge">reachable</code> as a manager and <code class="highlighter-rouge">ready</code> as a worker.</p> <p>An <code class="highlighter-rouge">unreachable</code> health status means that this particular manager node is unreachable from other manager nodes. In this case you need to take action to restore the unreachable manager:</p> <ul> <li>Restart the daemon and see if the manager comes back as reachable.</li> <li>Reboot the machine.</li> <li>If neither restarting or rebooting work, you should add another manager node or promote a worker to be a manager node. You also need to cleanly remove the failed node entry from the manager set with <code class="highlighter-rouge">docker node demote &lt;NODE&gt;</code> and <code class="highlighter-rouge">docker node rm &lt;id-node&gt;</code>.</li> </ul> <p>Alternatively you can also get an overview of the swarm health from a manager node with <code class="highlighter-rouge">docker node ls</code>:</p> <pre class="highlight" data-language="bash">
docker node ls
ID                           HOSTNAME  MEMBERSHIP  STATUS  AVAILABILITY  MANAGER STATUS
1mhtdwhvsgr3c26xxbnzdc3yp    node05    Accepted    Ready   Active
516pacagkqp2xc3fk9t1dhjor    node02    Accepted    Ready   Active        Reachable
9ifojw8of78kkusuc4a6c23fx *  node01    Accepted    Ready   Active        Leader
ax11wdpwrrb6db3mfjydscgk7    node04    Accepted    Ready   Active
bb1nrq2cswhtbg4mrsqnlx1ck    node03    Accepted    Ready   Active        Reachable
di9wxgz8dtuh9d2hn089ecqkf    node06    Accepted    Ready   Active
</pre>  <h2 id="troubleshoot-a-manager-node">Troubleshoot a manager node</h2> <p>You should never restart a manager node by copying the <code class="highlighter-rouge">raft</code> directory from another node. The data directory is unique to a node ID. A node can only use a node ID once to join the swarm. The node ID space should be globally unique.</p> <p>To cleanly re-join a manager node to a cluster:</p> <ol> <li>To demote the node to a worker, run <code class="highlighter-rouge">docker node demote &lt;NODE&gt;</code>.</li> <li>To remove the node from the swarm, run <code class="highlighter-rouge">docker node rm &lt;NODE&gt;</code>.</li> <li>Re-join the node to the swarm with a fresh state using <code class="highlighter-rouge">docker swarm join</code>.</li> </ol> <p>For more information on joining a manager node to a swarm, refer to <a href="../join-nodes/">Join nodes to a swarm</a>.</p> <h2 id="force-remove-a-node">Force remove a node</h2> <p>In most cases, you should shut down a node before removing it from a swarm with the <code class="highlighter-rouge">docker node rm</code> command. If a node becomes unreachable, unresponsive, or compromised you can forcefully remove the node without shutting it down by passing the <code class="highlighter-rouge">--force</code> flag. For instance, if <code class="highlighter-rouge">node9</code> becomes compromised:</p>  <pre class="highlight" data-language="">$ docker node rm node9

Error response from daemon: rpc error: code = 9 desc = node node9 is not down and can't be removed

$ docker node rm --force node9

Node node9 removed from swarm
</pre>  <p>Before you forcefully remove a manager node, you must first demote it to the worker role. Make sure that you always have an odd number of manager nodes if you demote or remove a manager</p> <h2 id="recover-from-disaster">Recover from disaster</h2> <p>Swarm is resilient to failures and the swarm can recover from any number of temporary node failures (machine reboots or crash with restart) or other transient errors. However, a swarm cannot automatically recover if it loses a quorum. Tasks on existing worker nodes will continue to run, but administrative tasks are not possible, including scaling or updating services and joining or removing nodes from the swarm. The best way to recover is to bring the missing manager nodes back online. If that is not possible, continue reading for some options for recovering your swarm.</p> <p>In a swarm of <code class="highlighter-rouge">N</code> managers, a quorum (a majority) of manager nodes must always be available. For example, in a swarm with 5 managers, a minimum of 3 must be operational and in communication with each other. In other words, the swarm can tolerate up to <code class="highlighter-rouge">(N-1)/2</code> permanent failures beyond which requests involving swarm management cannot be processed. These types of failures include data corruption or hardware failures.</p> <h3 id="recovering-from-losing-the-quorum">Recovering from losing the quorum</h3> <p>If you lose the quorum of managers, you cannot administer the swarm. If you have lost the quorum and you attempt to perform any management operation on the swarm, an error occurs:</p> <pre data-language="">Error response from daemon: rpc error: code = 4 desc = context deadline exceeded
</pre> <p>The best way to recover from losing the quorum is to bring the failed nodes back online. If you can’t do that, the only way to recover from this state is to use the <code class="highlighter-rouge">--force-new-cluster</code> action from a manager node. This removes all managers except the manager the command was run from. The quorum is achieved because there is now only one manager. Promote nodes to be managers until you have the desired number of managers.</p> <pre class="highlight" data-language="bash"># From the node to recover
docker swarm init --force-new-cluster --advertise-addr node01:2377
</pre>  <p>When you run the <code class="highlighter-rouge">docker swarm init</code> command with the <code class="highlighter-rouge">--force-new-cluster</code> flag, the Docker Engine where you run the command becomes the manager node of a single-node swarm which is capable of managing and running services. The manager has all the previous information about services and tasks, worker nodes are still part of the swarm, and services are still running. You will need to add or re-add manager nodes to achieve your previous task distribution and ensure that you have enough managers to maintain high availability and prevent losing the quorum.</p> <h2 id="forcing-the-swarm-to-rebalance">Forcing the swarm to rebalance</h2> <p>Generally, you do not need to force the swarm to rebalance its tasks. When you add a new node to a swarm, or a node reconnects to the swarm after a period of unavailability, the swarm does not automatically give a workload to the idle node. This is a design decision. If the swarm periodically shifted tasks to different nodes for the sake of balance, the clients using those tasks would be disrupted. The goal is to avoid disrupting running services for the sake of balance across the swarm. When new tasks start, or when a node with running tasks becomes unavailable, those tasks are given to less busy nodes. The goal is eventual balance, with minimal disruption to the end user.</p> <p>In Docker 1.13 and higher, you can use the <code class="highlighter-rouge">--force</code> or <code class="highlighter-rouge">-f</code> flag with the <code class="highlighter-rouge">docker service update</code> command to force the service to redistribute its tasks across the available worker nodes. This will cause the service tasks to restart. Client applications may be disrupted. If you have configured it, your service will use a <a href="../swarm-tutorial/#rolling-update">rolling update</a>.</p> <p>If you use an earlier version and you want to achieve an even balance of load across workers and don’t mind disrupting running tasks, you can force your swarm to re-balance by temporarily scaling the service upward. Use <code class="highlighter-rouge">docker service inspect --pretty &lt;servicename&gt;</code> to see the configured scale of a service. When you use <code class="highlighter-rouge">docker service scale</code>, the nodes with the lowest number of tasks are targeted to receive the new workloads. There may be multiple under-loaded nodes in your swarm. You may need to scale the service up by modest increments a few times to achieve the balance you want across all the nodes.</p> <p>When the load is balanced to your satisfaction, you can scale the service back down to the original scale. You can use <code class="highlighter-rouge">docker service ps</code> to assess the current balance of your service across nodes.</p> <p>See also <a href="../../reference/commandline/service_scale/"><code class="highlighter-rouge">docker service scale</code></a> and <a href="../../reference/commandline/service_ps/"><code class="highlighter-rouge">docker service ps</code></a>.</p>
<div class="_attribution">
  <p class="_attribution-p">
    © 2013–2016 Docker, Inc.<br>Licensed under the Apache License, Version 2.0.<br>Docker and the Docker logo are trademarks or registered trademarks of Docker, Inc. in the United States and/or other countries.<br>Docker, Inc. and other parties may also have trademark rights in other terms used herein.<br>
    <a href="https://docs.docker.com/engine/swarm/admin_guide/" class="_attribution-link" target="_blank">https://docs.docker.com/engine/swarm/admin_guide/</a>
  </p>
</div>

			</div>
		</div>
	</section>

	</div>
</body>
</html>
