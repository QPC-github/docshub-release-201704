
<!DOCTYPE HTML>

<html lang="en">

<head>
  <meta charset="utf-8">
  <title>TensorFlow Guide Documentation - W3cubDocs</title>
  
  <meta name="description" content=" TensorFlow Guide documentation ">
  <meta name="keywords" content="getting, started, with, tensorflow, tensors, guide, documentation, tensorflow~guide">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="http://docs.w3cub.com/tensorflow~guide/">
  <link href="/favicon.png" rel="icon">
  <link type="text/css" rel="stylesheet" href="/assets/application-50364fff564ce3b6327021805f3f00e2957b441cf27f576a7dd4ff63bbc47047.css">
  <script type="text/javascript" src="/assets/application-db64bfd54ceb42be11af7995804cf4902548419ceb79d509b0b7d62c22d98e6f.js"></script>
  <script src="/json/tensorflow~guide.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body>
	<div class="_app">
	<header class="_header">
  
  <form class="_search">
    <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
    <a class="_search-clear"></a>
    <div class="_search-tag"></div>
  </form>
  
  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/tensorflow~guide/" class="_nav-link" title="" style="margin-left:0;">TensorFlow Guide</a></span>
  
  <nav class="_nav">
    <a href="/app/" class="_nav-link ">App</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		<div class="_list">
			
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<div class="_page _tensorflow">
				
<h1 itemprop="name" class="devsite-page-title"> Getting Started With TensorFlow </h1>     <p>This guide gets you started programming in TensorFlow. Before using this guide, <a href="https://www.tensorflow.org/install/index" target="_blank">install TensorFlow</a>. To get the most out of this guide, you should know the following:</p> <ul> <li>How to program in Python.</li> <li>At least a little bit about arrays.</li> <li>Ideally, something about machine learning. However, if you know little or nothing about machine learning, then this is still the first guide you should read.</li> </ul> <p>TensorFlow provides multiple APIs. The lowest level API--TensorFlow Core-- provides you with complete programming control. We recommend TensorFlow Core for machine learning researchers and others who require fine levels of control over their models. The higher level APIs are built on top of TensorFlow Core. These higher level APIs are typically easier to learn and use than TensorFlow Core. In addition, the higher level APIs make repetitive tasks easier and more consistent between different users. A high-level API like tf.contrib.learn helps you manage data sets, estimators, training and inference. Note that a few of the high-level TensorFlow APIs--those whose method names contain <code>contrib</code>-- are still in development. It is possible that some <code>contrib</code> methods will change or become obsolete in subsequent TensorFlow releases.</p> <p>This guide begins with a tutorial on TensorFlow Core. Later, we demonstrate how to implement the same model in tf.contrib.learn. Knowing TensorFlow Core principles will give you a great mental model of how things are working internally when you use the more compact higher level API.</p> <h1 class="page-title" id="tensors">Tensors</h1> <p>The central unit of data in TensorFlow is the <strong>tensor</strong>. A tensor consists of a set of primitive values shaped into an array of any number of dimensions. A tensor's <strong>rank</strong> is its number of dimensions. Here are some examples of tensors:</p> <pre class="prettyprint lang-python" data-language="python">3 # a rank 0 tensor; this is a scalar with shape []
[1. ,2., 3.] # a rank 1 tensor; this is a vector with shape [3]
[[1., 2., 3.], [4., 5., 6.]] # a rank 2 tensor; a matrix with shape [2, 3]
[[[1., 2., 3.]], [[7., 8., 9.]]] # a rank 3 tensor with shape [2, 1, 3]
</pre> <h2 id="tensorflow_core_tutorial">TensorFlow Core tutorial</h2> <h3 id="importing_tensorflow">Importing TensorFlow</h3> <p>The canonical import statement for TensorFlow programs is as follows:</p> <pre class="prettyprint lang-python" data-language="python">import tensorflow as tf

</pre> <p>This gives Python access to all of TensorFlow's classes, methods, and symbols. Most of the documentation assumes you have already done this.</p> <h3 id="the_computational_graph">The Computational Graph</h3> <p>You might think of TensorFlow Core programs as consisting of two discrete sections:</p> <ol> <li>Building the computational graph.</li> <li>Running the computational graph.</li> </ol> <p>A <strong>computational graph</strong> is a series of TensorFlow operations arranged into a graph of nodes. Let's build a simple computational graph. Each node takes zero or more tensors as inputs and produces a tensor as an output. One type of node is a constant. Like all TensorFlow constants, it takes no inputs, and it outputs a value it stores internally. We can create two floating point Tensors <code>node1</code> and <code>node2</code> as follows:</p> <pre class="prettyprint lang-python" data-language="python">node1 = tf.constant(3.0, tf.float32)
node2 = tf.constant(4.0) # also tf.float32 implicitly
print(node1, node2)
</pre> <p>The final print statement produces</p> <pre class="prettyprint" data-language="cpp">Tensor("Const:0", shape=(), dtype=float32) Tensor("Const_1:0", shape=(), dtype=float32)
</pre> <p>Notice that printing the nodes does not output the values <code>3.0</code> and <code>4.0</code> as you might expect. Instead, they are nodes that, when evaluated, would produce 3.0 and 4.0, respectively. To actually evaluate the nodes, we must run the computational graph within a <strong>session</strong>. A session encapsulates the control and state of the TensorFlow runtime.</p> <p>The following code creates a <code>Session</code> object and then invokes its <code>run</code> method to run enough of the computational graph to evaluate <code>node1</code> and <code>node2</code>. By running the computational graph in a session as follows:</p> <pre class="prettyprint lang-python" data-language="python">sess = tf.Session()
print(sess.run([node1, node2]))
</pre> <p>we see the expected values of 3.0 and 4.0:</p> <pre class="prettyprint" data-language="cpp">[3.0, 4.0]
</pre> <p>We can build more complicated computations by combining <code>Tensor</code> nodes with operations (Operations are also nodes.). For example, we can add our two constant nodes and produce a new graph as follows:</p> <pre class="prettyprint lang-python" data-language="python">node3 = tf.add(node1, node2)
print("node3: ", node3)
print("sess.run(node3): ",sess.run(node3))
</pre> <p>The last two print statements produce</p> <pre class="prettyprint" data-language="cpp">node3:  Tensor("Add_2:0", shape=(), dtype=float32)
sess.run(node3):  7.0
</pre> <p>TensorFlow provides a utility called TensorBoard that can display a picture of the computational graph. Here is a screenshot showing how TensorBoard visualizes the graph:</p> <p><img alt="TensorBoard screenshot" src="https://www.tensorflow.org/images/getting_started_add.png"></p> <p>As it stands, this graph is not especially interesting because it always produces a constant result. A graph can be paramaterized to accept external inputs, known as <strong>placeholders</strong>. A <strong>placeholder</strong> is a promise to provide a value later.</p> <pre class="prettyprint lang-python" data-language="python">a = tf.placeholder(tf.float32)
b = tf.placeholder(tf.float32)
adder_node = a + b  # + provides a shortcut for tf.add(a, b)
</pre> <p>The preceding three lines are a bit like a function or a lambda in which we define two input parameters (a and b) and then an operation on them. We can evaluate this graph with multiple inputs by using the feed_dict parameter to specify Tensors that provide concrete values to these placeholders:</p> <pre class="prettyprint lang-python" data-language="python">print(sess.run(adder_node, {a: 3, b:4.5}))
print(sess.run(adder_node, {a: [1,3], b: [2, 4]}))
</pre> <p>resulting in the output</p> <pre class="prettyprint" data-language="cpp">7.5
[ 3.  7.]
</pre> <p>In TensorBoard, the graph looks like this:</p> <p><img alt="TensorBoard screenshot" src="https://www.tensorflow.org/images/getting_started_adder.png"></p> <p>We can make the computational graph more complex by adding another operation. For example,</p> <pre class="prettyprint lang-python" data-language="python">add_and_triple = adder_node * 3.
print(sess.run(add_and_triple, {a: 3, b:4.5}))
</pre> <p>produces the output</p> <pre class="prettyprint" data-language="cpp">22.5
</pre> <p>The preceding computational graph would look as follows in TensorBoard:</p> <p><img alt="TensorBoard screenshot" src="https://www.tensorflow.org/images/getting_started_triple.png"></p> <p>In machine learning we will typically want a model that can take arbitrary inputs, such as the one above. To make the model trainable, we need to be able to modify the graph to get new outputs with the same input. <strong>Variables</strong> allow us to add trainable parameters to a graph. They are constructed with a type and initial value:</p> <pre class="prettyprint lang-python" data-language="python">W = tf.Variable([.3], tf.float32)
b = tf.Variable([-.3], tf.float32)
x = tf.placeholder(tf.float32)
linear_model = W * x + b
</pre> <p>Constants are initialized when you call <code>tf.constant</code>, and their value can never change. By contrast, variables are not initialized when you call <code>tf.Variable</code>. To initialize all the variables in a TensorFlow program, you must explicitly call a special operation as follows:</p> <pre class="prettyprint lang-python" data-language="python">init = tf.global_variables_initializer()
sess.run(init)
</pre> <p>It is important to realize <code>init</code> is a handle to the TensorFlow sub-graph that initializes all the global variables. Until we call <code>sess.run</code>, the variables are uninitialized.</p> <p>Since <code>x</code> is a placeholder, we can evaluate <code>linear_model</code> for several values of <code>x</code> simultaneously as follows:</p> <pre class="prettyprint lang-python" data-language="python">print(sess.run(linear_model, {x:[1,2,3,4]}))
</pre> <p>to produce the output</p> <pre class="prettyprint" data-language="cpp">[ 0.          0.30000001  0.60000002  0.90000004]
</pre> <p>We've created a model, but we don't know how good it is yet. To evaluate the model on training data, we need a <code>y</code> placeholder to provide the desired values, and we need to write a loss function.</p> <p>A loss function measures how far apart the current model is from the provided data. We'll use a standard loss model for linear regression, which sums the squares of the deltas between the current model and the provided data. <code>linear_model - y</code> creates a vector where each element is the corresponding example's error delta. We call <code>tf.square</code> to square that error. Then, we sum all the squared errors to create a single scalar that abstracts the error of all examples using <code>tf.reduce_sum</code>:</p> <pre class="prettyprint lang-python" data-language="python">y = tf.placeholder(tf.float32)
squared_deltas = tf.square(linear_model - y)
loss = tf.reduce_sum(squared_deltas)
print(sess.run(loss, {x:[1,2,3,4], y:[0,-1,-2,-3]}))
</pre> <p>producing the loss value</p> <pre class="prettyprint" data-language="cpp">23.66
</pre> <p>We could improve this manually by reassigning the values of <code>W</code> and <code>b</code> to the perfect values of -1 and 1. A variable is initialized to the value provided to <code>tf.Variable</code> but can be changed using operations like <code>tf.assign</code>. For example, <code>W=-1</code> and <code>b=1</code> are the optimal parameters for our model. We can change <code>W</code> and <code>b</code> accordingly:</p> <pre class="prettyprint lang-python" data-language="python">fixW = tf.assign(W, [-1.])
fixb = tf.assign(b, [1.])
sess.run([fixW, fixb])
print(sess.run(loss, {x:[1,2,3,4], y:[0,-1,-2,-3]}))
</pre> <p>The final print shows the loss now is zero.</p> <pre class="prettyprint" data-language="cpp">0.0
</pre> <p>We guessed the "perfect" values of <code>W</code> and <code>b</code>, but the whole point of machine learning is to find the correct model parameters automatically. We will show how to accomplish this in the next section.</p> <h2 id="tftrain_api">tf.train API</h2> <p>A complete discussion of machine learning is out of the scope of this tutorial. However, TensorFlow provides <strong>optimizers</strong> that slowly change each variable in order to minimize the loss function. The simplest optimizer is <strong>gradient descent</strong>. It modifies each variable according to the magnitude of the derivative of loss with respect to that variable. In general, computing symbolic derivatives manually is tedious and error-prone. Consequently, TensorFlow can automatically produce derivatives given only a description of the model using the function <code>tf.gradients</code>. For simplicity, optimizers typically do this for you. For example,</p> <p>For example:</p> <pre class="prettyprint lang-python" data-language="python">optimizer = tf.train.GradientDescentOptimizer(0.01)
train = optimizer.minimize(loss)
</pre> <pre class="prettyprint lang-python" data-language="python">sess.run(init) # reset values to incorrect defaults.
for i in range(1000):
  sess.run(train, {x:[1,2,3,4], y:[0,-1,-2,-3]})

print(sess.run([W, b]))
</pre> <p>which results in the final model parameters</p> <pre class="prettyprint" data-language="cpp">[array([-0.9999969], dtype=float32), array([ 0.99999082],
 dtype=float32)]
</pre> <p>Now we have done actual machine learning! Although doing this simple linear regression doesn't require much TensorFlow core code, more complicated models and methods to feed data into your model necessitate more code. Thus TensorFlow provides higher level abstractions for common patterns, structures, and functionality. We will learn how to use some of these abstractions in the next section.</p> <h3 id="complete_program">Complete program</h3> <p>The completed trainable linear regression model is shown here:</p> <pre class="prettyprint lang-python" data-language="python">import numpy as np
import tensorflow as tf

# Model parameters
W = tf.Variable([.3], tf.float32)
b = tf.Variable([-.3], tf.float32)
# Model input and output
x = tf.placeholder(tf.float32)
linear_model = W * x + b
y = tf.placeholder(tf.float32)
# loss
loss = tf.reduce_sum(tf.square(linear_model - y)) # sum of the squares
# optimizer
optimizer = tf.train.GradientDescentOptimizer(0.01)
train = optimizer.minimize(loss)
# training data
x_train = [1,2,3,4]
y_train = [0,-1,-2,-3]
# training loop
init = tf.global_variables_initializer()
sess = tf.Session()
sess.run(init) # reset values to wrong
for i in range(1000):
  sess.run(train, {x:x_train, y:y_train})

# evaluate training accuracy
curr_W, curr_b, curr_loss  = sess.run([W, b, loss], {x:x_train, y:y_train})
print("W: %s b: %s loss: %s"%(curr_W, curr_b, curr_loss))
</pre> <p>When run, it produces</p> <pre class="prettyprint" data-language="cpp">W: [-0.9999969] b: [ 0.99999082] loss: 5.69997e-11
</pre> <p>This more complicated program can still be visualized in TensorBoard <img alt="TensorBoard final model visualization" src="https://www.tensorflow.org/images/getting_started_final.png"></p> <h2 id="tfcontriblearn"><code>tf.contrib.learn</code></h2> <p><code>tf.contrib.learn</code> is a high-level TensorFlow library that simplifies the mechanics of machine learning, including the following:</p> <ul> <li>running training loops</li> <li>running evaluation loops</li> <li>managing data sets</li> <li>managing feeding</li> </ul> <p>tf.contrib.learn defines many common models.</p> <h3 id="basic_usage">Basic usage</h3> <p>Notice how much simpler the linear regression program becomes with <code>tf.contrib.learn</code>:</p> <pre class="prettyprint lang-python" data-language="python">import tensorflow as tf
# NumPy is often used to load, manipulate and preprocess data.
import numpy as np

# Declare list of features. We only have one real-valued feature. There are many
# other types of columns that are more complicated and useful.
features = [tf.contrib.layers.real_valued_column("x", dimension=1)]

# An estimator is the front end to invoke training (fitting) and evaluation
# (inference). There are many predefined types like linear regression,
# logistic regression, linear classification, logistic classification, and
# many neural network classifiers and regressors. The following code
# provides an estimator that does linear regression.
estimator = tf.contrib.learn.LinearRegressor(feature_columns=features)

# TensorFlow provides many helper methods to read and set up data sets.
# Here we use `numpy_input_fn`. We have to tell the function how many batches
# of data (num_epochs) we want and how big each batch should be.
x = np.array([1., 2., 3., 4.])
y = np.array([0., -1., -2., -3.])
input_fn = tf.contrib.learn.io.numpy_input_fn({"x":x}, y, batch_size=4,
                                              num_epochs=1000)

# We can invoke 1000 training steps by invoking the `fit` method and passing the
# training data set.
estimator.fit(input_fn=input_fn, steps=1000)

# Here we evaluate how well our model did. In a real example, we would want
# to use a separate validation and testing data set to avoid overfitting.
estimator.evaluate(input_fn=input_fn)
</pre> <p>When run, it produces</p> <pre class="prettyprint" data-language="cpp">{'global_step': 1000, 'loss': 1.9650059e-11}
</pre> <h3 id="a_custom_model">A custom model</h3> <p><code>tf.contrib.learn</code> does not lock you into its predefined models. Suppose we wanted to create a custom model that is not built into TensorFlow. We can still retain the high level abstraction of data set, feeding, training, etc. of <code>tf.contrib.learn</code>. For illustration, we will show how to implement our own equivalent model to <code>LinearRegressor</code> using our knowledge of the lower level TensorFlow API.</p> <p>To define a custom model that works with <code>tf.contrib.learn</code>, we need to use <code>tf.contrib.learn.Estimator</code>. <code>tf.contrib.learn.LinearRegressor</code> is actually a sub-class of <code>tf.contrib.learn.Estimator</code>. Instead of sub-classing <code>Estimator</code>, we simply provide <code>Estimator</code> a function <code>model_fn</code> that tells <code>tf.contrib.learn</code> how it can evaluate predictions, training steps, and loss. The code is as follows:</p> <pre class="prettyprint lang-python" data-language="python">import numpy as np
import tensorflow as tf
# Declare list of features, we only have one real-valued feature
def model(features, labels, mode):
  # Build a linear model and predict values
  W = tf.get_variable("W", [1], dtype=tf.float64)
  b = tf.get_variable("b", [1], dtype=tf.float64)
  y = W*features['x'] + b
  # Loss sub-graph
  loss = tf.reduce_sum(tf.square(y - labels))
  # Training sub-graph
  global_step = tf.train.get_global_step()
  optimizer = tf.train.GradientDescentOptimizer(0.01)
  train = tf.group(optimizer.minimize(loss),
                   tf.assign_add(global_step, 1))
  # ModelFnOps connects subgraphs we built to the
  # appropriate functionality.
  return tf.contrib.learn.ModelFnOps(
      mode=mode, predictions=y,
      loss= loss,
      train_op=train)

estimator = tf.contrib.learn.Estimator(model_fn=model)
# define our data set
x=np.array([1., 2., 3., 4.])
y=np.array([0., -1., -2., -3.])
input_fn = tf.contrib.learn.io.numpy_input_fn({"x": x}, y, 4, num_epochs=1000)

# train
estimator.fit(input_fn=input_fn, steps=1000)
# evaluate our model
print(estimator.evaluate(input_fn=input_fn, steps=10))
</pre> <p>When run, it produces</p> <pre class="prettyprint lang-python" data-language="python">{'loss': 5.9819476e-11, 'global_step': 1000}
</pre> <p>Notice how the contents of the custom <code>model()</code> function are very similar to our manual model training loop from the lower level API.</p> <h2 id="next_steps">Next steps</h2> <p>Now you have a working knowledge of the basics of TensorFlow. We have several more tutorials that you can look at to learn more. If you are a beginner in machine learning see <a href="get_started/mnist/beginners/">MNIST for beginners</a>, otherwise see <a href="get_started/mnist/pros/">Deep MNIST for experts</a>.</p>
<div class="_attribution">
  <p class="_attribution-p">
    © 2017 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/get_started/get_started" class="_attribution-link" target="_blank">https://www.tensorflow.org/get_started/get_started</a>
  </p>
</div>

			</div>
		</div>
	</section>

	</div>
</body>
</html>
