
<!DOCTYPE HTML>

<html lang="en">

<head>
  <meta charset="utf-8">
  <title>contrib.layers.optimize_loss() - TensorFlow Python - W3cubDocs</title>
  
  <meta name="description" content=" See the guide&#58; Layers (contrib) &#62; Optimization ">
  <meta name="keywords" content="tf, contrib, layers, optimize, loss, global, step, learning, rate, optimizer, gradient, noise, scale, none, multipliers, clip, gradients, decay, fn, update, ops, variables, name, summaries, colocate, with, false, -, tensorflow, python, tensorflow~python">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="http://docs.w3cub.com/tensorflow~python/tf/contrib/layers/optimize_loss/">
  <link href="/favicon.png" rel="icon">
  <link type="text/css" rel="stylesheet" href="/assets/application-50364fff564ce3b6327021805f3f00e2957b441cf27f576a7dd4ff63bbc47047.css">
  <script type="text/javascript" src="/assets/application-db64bfd54ceb42be11af7995804cf4902548419ceb79d509b0b7d62c22d98e6f.js"></script>
  <script src="/json/tensorflow~python.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body>
	<div class="_app">
	<header class="_header">
  
  <form class="_search">
    <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
    <a class="_search-clear"></a>
    <div class="_search-tag"></div>
  </form>
  
  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/tensorflow~python/" class="_nav-link" title="" style="margin-left:0;">TensorFlow Python</a></span>
  
  <nav class="_nav">
    <a href="/app/" class="_nav-link ">App</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		<div class="_list">
			
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<div class="_page _tensorflow">
				
<h1 itemprop="name" class="devsite-page-title"> tf.contrib.layers.optimize_loss(loss, global_step, learning_rate, optimizer, gradient_noise_scale=None, gradient_multipliers=None, clip_gradients=None, learning_rate_decay_fn=None, update_ops=None, variables=None, name=None, summaries=None, colocate_gradients_with_ops=False) </h1>    <h3 id="tfcontriblayersoptimize_lossloss_global_step_learning_rate_optimizer_gradient_noise_scalenone_gradient_multipliersnone_clip_gradientsnone_learning_rate_decay_fnnone_update_opsnone_variablesnone_namenone_summariesnone_colocate_gradients_with_opsfalse_1"><code>tf.contrib.layers.optimize_loss(loss, global_step, learning_rate, optimizer, gradient_noise_scale=None, gradient_multipliers=None, clip_gradients=None, learning_rate_decay_fn=None, update_ops=None, variables=None, name=None, summaries=None, colocate_gradients_with_ops=False)</code></h3> <p>See the guide: <a href="https://www.tensorflow.org/api_guides/python/contrib.layers#Optimization" target="_blank">Layers (contrib) &gt; Optimization</a></p> <p>Given loss and parameters for optimizer, returns a training op.</p> <p>Various ways of passing optimizers, include:</p> <ul> <li>string, name of the optimizer like 'SGD', 'Adam', see OPTIMIZER_CLS_NAMES for full list. E.g. <code>optimize_loss(..., optimizer='Adam')</code>.</li> <li>function, takes learning rate <code>Tensor</code> as argument and must return <code>Optimizer</code> instance. E.g. <code>optimize_loss(..., optimizer=lambda lr: tf.train.MomentumOptimizer(lr, momentum=0.5))</code>. Alternatively, if <code>learning_rate</code> is <code>None</code>, the function takes no arguments. E.g. <code>optimize_loss(..., learning_rate=None, optimizer=lambda: tf.train.MomentumOptimizer(0.5, momentum=0.5))</code>.</li> <li>class, subclass of <code>Optimizer</code> that takes only one required argument - learning rate, such as AdamOptimizer, AdagradOptimizer. E.g. <code>optimize_loss(..., optimizer=tf.train.AdagradOptimizer)</code>.</li> <li>object, instance of subclass of <code>Optimizer</code>. E.g., <code>optimizer_loss(..., optimizer=tf.train.AdagradOptimizer(0.5))</code>.</li> </ul> <h4 id="args">Args:</h4> <ul> <li>
<b><code>loss</code></b>: Scalar <code>Tensor</code>.</li> <li>
<b><code>global_step</code></b>: Scalar int <code>Tensor</code>, step counter for each update. If not supplied, it will be fetched from the default graph (see <code>tf.contrib.framework.get_global_step</code> for details). If it's not been created, no step will be incremented with each weight update. <code>learning_rate_decay_fn</code> requires <code>global_step</code>.</li> <li>
<b><code>learning_rate</code></b>: float or <code>Tensor</code>, magnitude of update per each training step. Can be <code>None</code>.</li> <li>
<b><code>optimizer</code></b>: string, class or optimizer instance, used as trainer. string should be name of optimizer, like 'SGD', 'Adam', 'Adagrad'. Full list in OPTIMIZER_CLS_NAMES constant. class should be sub-class of <code>tf.Optimizer</code> that implements <code>compute_gradients</code> and <code>apply_gradients</code> functions. optimizer instance should be instantiation of <code>tf.Optimizer</code> sub-class and have <code>compute_gradients</code> and <code>apply_gradients</code> functions.</li> <li>
<b><code>gradient_noise_scale</code></b>: float or None, adds 0-mean normal noise scaled by this value.</li> <li>
<b><code>gradient_multipliers</code></b>: dict of variables or variable names to floats. If present, gradients for specified variables will be multiplied by given constant.</li> <li>
<b><code>clip_gradients</code></b>: float, callable or <code>None</code>. If float, is provided, a global clipping is applied to prevent the norm of the gradient to exceed this value. Alternatively, a callable can be provided e.g.: adaptive_clipping. This callable takes a <code>list</code> of <code>(gradients, variables)</code> <code>tuple</code>s and returns the same thing with the gradients modified.</li> <li>
<b><code>learning_rate_decay_fn</code></b>: function, takes <code>learning_rate</code> and <code>global_step</code> <code>Tensor</code>s, returns <code>Tensor</code>. Can be used to implement any learning rate decay functions. For example: <code>tf.train.exponential_decay</code>. Ignored if <code>learning_rate</code> is not supplied.</li> <li>
<b><code>update_ops</code></b>: list of update <code>Operation</code>s to execute at each step. If <code>None</code>, uses elements of UPDATE_OPS collection. The order of execution between <code>update_ops</code> and <code>loss</code> is non-deterministic.</li> <li>
<b><code>variables</code></b>: list of variables to optimize or <code>None</code> to use all trainable variables.</li> <li>
<b><code>name</code></b>: The name for this operation is used to scope operations and summaries.</li> <li>
<b><code>summaries</code></b>: List of internal quantities to visualize on tensorboard. If not set only the loss and the learning rate will be reported. The complete list is in OPTIMIZER_SUMMARIES.</li> <li>
<b><code>colocate_gradients_with_ops</code></b>: If True, try colocating gradients with the corresponding op.</li> </ul> <h4 id="returns">Returns:</h4> <p>Training op.</p> <h4 id="raises">Raises:</h4> <ul> <li>
<b><code>ValueError</code></b>: if:<ul> <li>
<code>loss</code> is an invalid type or shape.</li> <li>
<code>global_step</code> is an invalid type or shape.</li> <li>
<code>learning_rate</code> is an invalid type or value.</li> <li>
<code>optimizer</code> is wrong type.</li> <li>
<code>clip_gradients</code> is not float or callable.</li> <li>
<code>learning_rate</code> and <code>learning_rate_decay_fn</code> are supplied, but no <code>global_step</code> is available.</li> </ul> </li> </ul> <p>Defined in <a href="https://www.tensorflow.org/code/tensorflow/contrib/layers/python/layers/optimizers.py" target="_blank"><code>tensorflow/contrib/layers/python/layers/optimizers.py</code></a>.</p>
<div class="_attribution">
  <p class="_attribution-p">
    Â© 2017 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/layers/optimize_loss" class="_attribution-link" target="_blank">https://www.tensorflow.org/api_docs/python/tf/contrib/layers/optimize_loss</a>
  </p>
</div>

			</div>
		</div>
	</section>

	</div>
</body>
</html>
