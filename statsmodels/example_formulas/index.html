
<!DOCTYPE HTML>

<html lang="en">

<head>
  <meta charset="utf-8">
  <title>Manual&#58; Fitting Models Using R-style Formulas - Statsmodels - W3cubDocs</title>
  
  <meta name="description" content="Since version 0.5.0, statsmodels allows users to fit statistical models using R-style formulas. Internally, statsmodels uses the patsy package to &hellip;">
  <meta name="keywords" content="fitting, models, using, r-style, formulas, manual, -, statsmodels">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="http://docs.w3cub.com/statsmodels/example_formulas/">
  <link href="/favicon.png" rel="icon">
  <link type="text/css" rel="stylesheet" href="/assets/application-50364fff564ce3b6327021805f3f00e2957b441cf27f576a7dd4ff63bbc47047.css">
  <script type="text/javascript" src="/assets/application-db64bfd54ceb42be11af7995804cf4902548419ceb79d509b0b7d62c22d98e6f.js"></script>
  <script src="/json/statsmodels.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body>
	<div class="_app">
	<header class="_header">
  
  <form class="_search">
    <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
    <a class="_search-clear"></a>
    <div class="_search-tag"></div>
  </form>
  
  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/statsmodels/" class="_nav-link" title="" style="margin-left:0;">Statsmodels</a></span>
  
  <nav class="_nav">
    <a href="/app/" class="_nav-link ">App</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		<div class="_list">
			
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<div class="_page _sphinx">
				
<h1 id="formula-examples">Fitting models using R-style formulas</h1> <p id="fitting-models-using-r-style-formulas">Since version 0.5.0, <code>statsmodels</code> allows users to fit statistical models using R-style formulas. Internally, <code>statsmodels</code> uses the <a class="reference external" href="http://patsy.readthedocs.org/" target="_blank">patsy</a> package to convert formulas and data to the matrices that are used in model fitting. The formula framework is quite powerful; this tutorial only scratches the surface. A full description of the formula language can be found in the <code>patsy</code> docs:</p> <ul class="simple"> <li><a class="reference external" href="http://patsy.readthedocs.org/" target="_blank">Patsy formula language description</a></li> </ul>  <h2 id="loading-modules-and-functions">Loading modules and functions</h2> <pre data-language="python">import statsmodels.formula.api as smf
import numpy as np
import pandas
</pre> <p>Notice that we called <code>statsmodels.formula.api</code> instead of the usual <code>statsmodels.api</code>. The <code>formula.api</code> hosts many of the same functions found in <code>api</code> (e.g. OLS, GLM), but it also holds lower case counterparts for most of these models. In general, lower case models accept <code>formula</code> and <code>df</code> arguments, whereas upper case ones take <code>endog</code> and <code>exog</code> design matrices. <code>formula</code> accepts a string which describes the model in terms of a <code>patsy</code> formula. <code>df</code> takes a <a class="reference external" href="http://pandas.pydata.org/" target="_blank">pandas</a> data frame.</p> <p><code>dir(smf)</code> will print a list of available models.</p> <p>Formula-compatible models have the following generic call signature: <code>(formula, data, subset=None, *args, **kwargs)</code></p>   <h2 id="ols-regression-using-formulas">OLS regression using formulas</h2> <p>To begin, we fit the linear model described on the <a class="reference external" href="../gettingstarted/">Getting Started</a> page. Download the data, subset columns, and list-wise delete to remove missing observations:</p> <pre data-language="python">df = sm.datasets.get_rdataset("Guerry", "HistData").data
df = df[['Lottery', 'Literacy', 'Wealth', 'Region']].dropna()
df.head()
</pre> <pre data-language="python">   Lottery  Literacy  Wealth Region
0       41        37      73      E
1       38        51      22      N
2       66        13      61      C
3       80        46      76      E
4       79        69      83      E
</pre> <p>Fit the model:</p> <pre data-language="python">mod = smf.ols(formula='Lottery ~ Literacy + Wealth + Region', data=df)
res = mod.fit()
print res.summary()
</pre> <pre data-language="python">                            OLS Regression Results
==============================================================================
Dep. Variable:                Lottery   R-squared:                       0.338
Model:                            OLS   Adj. R-squared:                  0.287
Method:                 Least Squares   F-statistic:                     6.636
Date:                Sun, 13 Jan 2013   Prob (F-statistic):           1.07e-05
Time:                        10:38:36   Log-Likelihood:                -375.30
No. Observations:                  85   AIC:                             764.6
Df Residuals:                      78   BIC:                             781.7
Df Model:                           6
===============================================================================
                  coef    std err          t      P&gt;|t|      [95.0% Conf. Int.]
-------------------------------------------------------------------------------
Intercept      38.6517      9.456      4.087      0.000        19.826    57.478
Region[T.E]   -15.4278      9.727     -1.586      0.117       -34.793     3.938
Region[T.N]   -10.0170      9.260     -1.082      0.283       -28.453     8.419
Region[T.S]    -4.5483      7.279     -0.625      0.534       -19.039     9.943
Region[T.W]   -10.0913      7.196     -1.402      0.165       -24.418     4.235
Literacy       -0.1858      0.210     -0.886      0.378        -0.603     0.232
Wealth          0.4515      0.103      4.390      0.000         0.247     0.656
==============================================================================
Omnibus:                        3.049   Durbin-Watson:                   1.785
Prob(Omnibus):                  0.218   Jarque-Bera (JB):                2.694
Skew:                          -0.340   Prob(JB):                        0.260
Kurtosis:                       2.454   Cond. No.                         371.
==============================================================================
</pre>   <h2 id="categorical-variables">Categorical variables</h2> <p>Looking at the summary printed above, notice that <code>patsy</code> determined that elements of <em>Region</em> were text strings, so it treated <em>Region</em> as a categorical variable. <code>patsy</code>‘s default is also to include an intercept, so we automatically dropped one of the <em>Region</em> categories.</p> <p>If <em>Region</em> had been an integer variable that we wanted to treat explicitly as categorical, we could have done so by using the <code>C()</code> operator:</p> <pre data-language="python">res = smf.ols(formula='Lottery ~ Literacy + Wealth + C(Region)', data=df).fit()
print res.params
</pre> <pre data-language="python">Intercept         38.651655
C(Region)[T.E]   -15.427785
C(Region)[T.N]   -10.016961
C(Region)[T.S]    -4.548257
C(Region)[T.W]   -10.091276
Literacy          -0.185819
Wealth             0.451475
</pre> <p>Examples more advanced features <code>patsy</code>‘s categorical variables function can be found here: <a class="reference external" href="../contrasts/">Patsy: Contrast Coding Systems for categorical variables</a></p>   <h2 id="operators">Operators</h2> <p>We have already seen that “~” separates the left-hand side of the model from the right-hand side, and that “+” adds new columns to the design matrix.</p>  <h3 id="removing-variables">Removing variables</h3> <p>The “-” sign can be used to remove columns/variables. For instance, we can remove the intercept from a model by:</p> <pre data-language="python">res = smf.ols(formula='Lottery ~ Literacy + Wealth + C(Region) -1 ', data=df).fit()
print res.params
</pre> <pre data-language="python">C(Region)[C]    38.651655
C(Region)[E]    23.223870
C(Region)[N]    28.634694
C(Region)[S]    34.103399
C(Region)[W]    28.560379
Literacy        -0.185819
Wealth           0.451475
</pre>   <h3 id="multiplicative-interactions">Multiplicative interactions</h3> <p>”:” adds a new column to the design matrix with the product of the other two columns. “*” will also include the individual columns that were multiplied together:</p> <pre data-language="python">res1 = smf.ols(formula='Lottery ~ Literacy : Wealth - 1', data=df).fit()
res2 = smf.ols(formula='Lottery ~ Literacy * Wealth - 1', data=df).fit()
print res1.params, '\n'
print res2.params
</pre> <pre data-language="python">Literacy:Wealth    0.018176

Literacy           0.427386
Wealth             1.080987
Literacy:Wealth   -0.013609
</pre> <p>Many other things are possible with operators. Please consult the <a class="reference external" href="https://patsy.readthedocs.org/en/latest/formulas.html" target="_blank">patsy docs</a> to learn more.</p>    <h2 id="functions">Functions</h2> <p>You can apply vectorized functions to the variables in your model:</p> <pre data-language="python">res = smf.ols(formula='Lottery ~ np.log(Literacy)', data=df).fit()
print res.params
</pre> <pre data-language="python">Intercept           115.609119
np.log(Literacy)    -20.393959
</pre> <p>Define a custom function:</p> <pre data-language="python">def log_plus_1(x):
    return np.log(x) + 1.
res = smf.ols(formula='Lottery ~ log_plus_1(Literacy)', data=df).fit()
print res.params
</pre> <pre data-language="python">Intercept               136.003079
log_plus_1(Literacy)    -20.393959
</pre>   <h2 id="patsy-namespaces">Namespaces</h2> <p id="namespaces">Notice that all of the above examples use the calling namespace to look for the functions to apply. The namespace used can be controlled via the <code>eval_env</code> keyword. For example, you may want to give a custom namespace using the <a class="reference external" href="http://patsy.readthedocs.org/en/latest/API-reference.html#patsy.EvalEnvironment" title="(in patsy v0.3.0-dev)" target="_blank"><code>patsy.EvalEnvironment</code></a> or you may want to use a “clean” namespace, which we provide by passing <code>eval_func=-1</code>. The default is to use the caller’s namespace. This can have (un)expected consequences, if, for example, someone has a variable names <code>C</code> in the user namespace or in their data structure passed to <code>patsy</code>, and <code>C</code> is used in the formula to handle a categorical variable. See the <a class="reference external" href="http://patsy.readthedocs.org/en/latest/API-reference.html" target="_blank">Patsy API Reference</a> for more information.</p>   <h2 id="using-formulas-with-models-that-do-not-yet-support-them">Using formulas with models that do not (yet) support them</h2> <p>Even if a given <code>statsmodels</code> function does not support formulas, you can still use <code>patsy</code>‘s formula language to produce design matrices. Those matrices can then be fed to the fitting function as <code>endog</code> and <code>exog</code> arguments.</p> <p>To generate <code>numpy</code> arrays:</p> <pre data-language="python">import patsy
f = 'Lottery ~ Literacy * Wealth'
y,X = patsy.dmatrices(f, df, return_type='dataframe')
print y[:5]
print X[:5]
</pre> <pre data-language="python">   Lottery
0       41
1       38
2       66
3       80
4       79
   Intercept  Literacy  Wealth  Literacy:Wealth
0          1        37      73             2701
1          1        51      22             1122
2          1        13      61              793
3          1        46      76             3496
4          1        69      83             5727
</pre> <p>To generate pandas data frames:</p> <pre data-language="python">f = 'Lottery ~ Literacy * Wealth'
y,X = patsy.dmatrices(f, df, return_type='dataframe')
print y[:5]
print X[:5]
</pre> <pre data-language="python">   Lottery
0       41
1       38
2       66
3       80
4       79
   Intercept  Literacy  Wealth  Literacy:Wealth
0          1        37      73             2701
1          1        51      22             1122
2          1        13      61              793
3          1        46      76             3496
4          1        69      83             5727
</pre> <pre data-language="python">print smf.OLS(y, X).fit().summary()
</pre> <pre data-language="python">                            OLS Regression Results
==============================================================================
Dep. Variable:                Lottery   R-squared:                       0.309
Model:                            OLS   Adj. R-squared:                  0.283
Method:                 Least Squares   F-statistic:                     12.06
Date:                Sun, 13 Jan 2013   Prob (F-statistic):           1.32e-06
Time:                        10:38:36   Log-Likelihood:                -377.13
No. Observations:                  85   AIC:                             762.3
Df Residuals:                      81   BIC:                             772.0
Df Model:                           3
===================================================================================
                      coef    std err          t      P&gt;|t|      [95.0% Conf. Int.]
-----------------------------------------------------------------------------------
Intercept          38.6348     15.825      2.441      0.017         7.149    70.121
Literacy           -0.3522      0.334     -1.056      0.294        -1.016     0.312
Wealth              0.4364      0.283      1.544      0.126        -0.126     0.999
Literacy:Wealth    -0.0005      0.006     -0.085      0.933        -0.013     0.012
==============================================================================
Omnibus:                        4.447   Durbin-Watson:                   1.953
Prob(Omnibus):                  0.108   Jarque-Bera (JB):                3.228
Skew:                          -0.332   Prob(JB):                        0.199
Kurtosis:                       2.314   Cond. No.                     1.40e+04
==============================================================================

The condition number is large, 1.4e+04. This might indicate that there are
strong multicollinearity or other numerical problems.
</pre>
<div class="_attribution">
  <p class="_attribution-p">
    © 2009–2012 Statsmodels Developers<br>© 2006–2008 Scipy Developers<br>© 2006 Jonathan E. Taylor<br>Licensed under the 3-clause BSD License.<br>
    <a href="http://statsmodels.sourceforge.net/stable/example_formulas.html" class="_attribution-link" target="_blank">http://statsmodels.sourceforge.net/stable/example_formulas.html</a>
  </p>
</div>

			</div>
		</div>
	</section>

	</div>
</body>
</html>
